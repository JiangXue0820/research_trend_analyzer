# LLM API Keys
# Required: At least one LLM provider must be configured

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4-turbo-preview  # or gpt-3.5-turbo

# Google Gemini API Configuration  
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-pro

# Default LLM Provider Selection
DEFAULT_LLM_PROVIDER=openai  # Options: openai, gemini, anthropic

# Logging Configuration
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=logs/app.log

# File Path Configuration
DATA_DIR=data
OUTPUT_DIR=output
TEMP_DIR=temp

# Network Configuration
REQUEST_TIMEOUT=30  # seconds
MAX_RETRIES=3
RETRY_DELAY=2  # seconds

# Workflow Configuration
MAX_PAPERS_PER_RUN=50  # Maximum papers to process in one workflow run
MAX_CONCURRENT_DOWNLOADS=5  # Concurrent PDF downloads
MAX_SUMMARY_LENGTH=1000  # Maximum characters per summary

# Cache Configuration (optional)
USE_CACHE=true
CACHE_EXPIRY_HOURS=24

# Note: Copy this file to .env and fill in your actual API keys
# The system will use the first available LLM provider in this order:
# 1. OpenAI (if OPENAI_API_KEY is set)
# 2. Gemini (if GEMINI_API_KEY is set) 
# 3. Anthropic (if ANTHROPIC_API_KEY is set)