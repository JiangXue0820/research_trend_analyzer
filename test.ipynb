{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defcf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Jiang\\AppData\\Local\\Temp\\ipykernel_29076\\3422960472.py:15: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import sys\n",
    "# sys.path.append(\"../\")\n",
    "from configs import config\n",
    "from configs.llm_provider import get_llm\n",
    "from research_trend_analyzer.tools.paper_fetch_tools import paper_fetch_toolkit\n",
    "from research_trend_analyzer.tools.paper_summary_tools import paper_analyze_toolkit\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "\n",
    "# 1. Initialize LLM\n",
    "config.LLM_PROVIDER = 'gemini'\n",
    "llm = get_llm(config)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 1. Create the planner (LLM decides on multi-step plan)\n",
    "planner = load_chat_planner(llm)\n",
    "\n",
    "# 2. Create the executor (agent capable of tool execution)\n",
    "executor = load_agent_executor(\n",
    "    llm=llm,\n",
    "    tools=paper_fetch_toolkit+paper_analyze_toolkit,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. Combine into plan-and-execute agent\n",
    "master_agent = PlanAndExecute(\n",
    "    planner=planner,\n",
    "    executor=executor,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# # 2. Create fetch and analyze agents\n",
    "# paper_fetch_agent = initialize_agent(\n",
    "#     tools=paper_fetch_toolkit,\n",
    "#     llm=llm,\n",
    "#     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# paper_analyze_agent = initialize_agent(\n",
    "#     tools=paper_analyze_toolkit,\n",
    "#     llm=llm,\n",
    "#     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_trend_analyzer.configs.log_config import configure_logging\n",
    "configure_logging()  # Make sure logging is set up first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0eb3033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Jiang\\AppData\\Local\\Temp\\ipykernel_29076\\931161638.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  master_agent.run(\"filter privacy-related papers from NIPS 2023, write a summary for each of the paper\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 250\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 14\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmaster_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfilter privacy-related papers from NIPS 2023, write a summary for each of the paper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:603\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    602\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_experimental\\plan_and_execute\\agent_executor.py:43\u001b[39m, in \u001b[36mPlanAndExecute._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     40\u001b[39m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m     41\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     42\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     plan = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mplanner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n\u001b[32m     48\u001b[39m         run_manager.on_text(\u001b[38;5;28mstr\u001b[39m(plan), verbose=\u001b[38;5;28mself\u001b[39m.verbose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_experimental\\plan_and_execute\\planners\\base.py:37\u001b[39m, in \u001b[36mLLMPlanner.plan\u001b[39m\u001b[34m(self, inputs, callbacks, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplan\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: \u001b[38;5;28mdict\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> Plan:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given input, decide what to do.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     llm_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_parser.parse(llm_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:608\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[32m0\u001b[39m], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    614\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but none were provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    616\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    147\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:980\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    973\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    977\u001b[39m     **kwargs: Any,\n\u001b[32m    978\u001b[39m ) -> LLMResult:\n\u001b[32m    979\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:799\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    798\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m         )\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    807\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1045\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1043\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1049\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1441\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1416\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1427\u001b[39m     **kwargs: Any,\n\u001b[32m   1428\u001b[39m ) -> ChatResult:\n\u001b[32m   1429\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1430\u001b[39m         messages,\n\u001b[32m   1431\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1439\u001b[39m         **kwargs,\n\u001b[32m   1440\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:231\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    224\u001b[39m params = (\n\u001b[32m    225\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:222\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:206\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 250\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 14\n}\n]"
     ]
    }
   ],
   "source": [
    "master_agent.run(\"filter privacy-related papers from NIPS 2023, write a summary for each of the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19d970d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m steps=[\u001b[43mStep\u001b[49m(value=\u001b[33m'\u001b[39m\u001b[33mAccess the official NeurIPS 2023 proceedings or paper list.\u001b[39m\u001b[33m'\u001b[39m), Step(value=\u001b[33m'\u001b[39m\u001b[33mSearch the paper titles and abstracts using keywords such as \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprivacy,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdifferential privacy,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfederated learning,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33manonymity,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfidentiality,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprivacy-preserving.\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m), Step(value=\u001b[33m'\u001b[39m\u001b[33mReview the search results to identify papers primarily focused on privacy.\u001b[39m\u001b[33m'\u001b[39m), Step(value=\u001b[33m'\u001b[39m\u001b[33mSelect 10 distinct and relevant privacy-related papers from the identified list.\u001b[39m\u001b[33m'\u001b[39m), Step(value=\u001b[33m'\u001b[39m\u001b[33mPresent the titles and, if possible, authors or links for the 10 selected papers.\u001b[39m\u001b[33m'\u001b[39m), Step(value=\u001b[33m'\u001b[39m\u001b[33mGiven the above steps taken, please respond to the users original question.\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[31mNameError\u001b[39m: name 'Step' is not defined"
     ]
    }
   ],
   "source": [
    "steps=[Step(value='Access the official NeurIPS 2023 proceedings or paper list.'), Step(value='Search the paper titles and abstracts using keywords such as \"privacy,\" \"differential privacy,\" \"federated learning,\" \"anonymity,\" \"confidentiality,\" and \"privacy-preserving.\"'), Step(value='Review the search results to identify papers primarily focused on privacy.'), Step(value='Select 10 distinct and relevant privacy-related papers from the identified list.'), Step(value='Present the titles and, if possible, authors or links for the 10 selected papers.'), Step(value='Given the above steps taken, please respond to the users original question.')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from research_trend_analyzer_light.utils.paper_process import download_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3437948",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = download_pdf(\"https://www.dfki.de/fileadmin/user_upload/import/5224_paper12.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace9731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(resp['data']['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38d0313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('temp/paper.pdf')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb49be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for page in doc:\n",
    "    text += page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2300b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43265"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "175c2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.llm_provider import get_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba381930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e44df39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = get_text_splitter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70f52326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document = [Document(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb3640b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Advances in Deep Parsing of Scholarly Paper\\nContent\\nUlrich Sch¨afer and Bernd Kiefer\\nLanguage Technology Lab\\nGerman Research Center for Artiﬁcial Intelligence (DFKI)\\nCampus D3 1, D-66123 Saarbr¨ucken, Germany\\n{ulrich.schaefer,kiefer}@dfki.de\\nhttp://www.dfki.de/lt\\nAbstract. We report on advances in deep linguistic parsing of the full\\ntextual content of 8200 papers from the ACL Anthology, a collection of\\nelectronically available scientiﬁc papers in the ﬁelds of Computational\\nLinguistics and Language Technology.\\nWe describe how – by incorporating new techniques – we increase both\\nspeed and robustness of deep analysis, speciﬁcally on long sentences\\nwhere deep parsing often failed in former approaches. With the current\\nopen source HPSG (Head-driven phrase structure grammar) for English\\n(ERG), we obtain deep parses for more than 85% of the sentences in the\\n1.5 million sentences corpus, while the former approaches achieved only\\napprox. 65% coverage.\\nThe resulting sentence-wise semantic representations are used in the Sci-\\nentist’s Workbench, a platform demonstrating the use and beneﬁt of\\nnatural language processing (NLP) to support scientists or other knowl-\\nedge workers in fast and better access to digital document content. With\\nthe generated NLP annotations, we are able to implement important,\\nnovel applications such as robust semantic search, citation classiﬁcation,\\nand (in the future) question answering and deﬁnition exploration.\\n1\\nIntroduction\\nScientists in all disciplines are nowadays faced with a ﬂood of new publications\\nevery day. In addition, more and more publications from the past become dig-\\nitally available and thus even increase the amount of data. Therefore, ﬁnding\\nrelevant information and avoiding redundancy and duplication of work have be-\\ncome urgent issues to be addressed by the scientiﬁc community.\\nThe organization and preservation of scientiﬁc knowledge in scientiﬁc pub-\\nlications, vulgo text documents, thwarts these eﬀorts. From a viewpoint of a\\ncomputer scientist, scientiﬁc papers are just ‘unstructured information’.\\nAutomatically precomputed, normalized semantic representations of textual\\nutterances could help to structure the search space and ﬁnd equivalent or related\\npropositions even if they are expressed diﬀerently, e.g. in passive constructions,\\nusing synonyms etc. Domain-relevant semantic similarity can be computed auto-\\nmatically and exploited as additional knowledge source to support robust search.\\n⋆Pre-print. The original publication is available at http://www.springerlink.com.\\n2\\nUlrich Sch¨afer and Bernd Kiefer\\nTo again constrain the so expanded search space, users can ask the system\\nin simply structured subject-predicate-object queries and get all matching, pre-\\ncomputed predicate-argument structures along with the original sentence from\\nthe paper. On the other hand, by storing the structure along with the original\\ntext in a structured full-text search engine such as Apache Lucene, it can be\\nguaranteed that recall cannot fall behind the baseline of a fulltext search engine.\\nThe basis of our scientiﬁc paper corpus is a subset of the ACL Anthology1,\\na collection of conference and workshop papers in the ﬁeld of Computational\\nLinguistics and Language Technology. We concentrate on 8200 papers from the\\nyears 2002 through 2009 from which we extracted the textual content using\\nAbbyy PDF Transformer.\\nExcept for named entity recognition which is partly based on instances and\\nconcepts of a domain ontology, the processing pipeline we describe below is\\nindependent of the science domain.\\nTo make the deep parser robust, it is embedded in a hybrid NLP workﬂow\\nstarting with a tokenizer, a part-of-speech tagger, and a named entity recognizer.\\nThese components help to identify and classify open class words such as person\\nnames, events (e.g. conferences) or locations. The trigram-based tagger helps\\nto guess part-of-speech tags of words unknown to the deep lexicon. For both\\nunknown words and named entities, generic lexicon entries are generated in the\\ndeep parser running the open source broad-coverage grammar ERG [5].\\nIn contrast to shallow parsers, the ERG not only handles detailed syntac-\\ntic analyses of phrases, compounds, coordination, negation and other linguistic\\nphenomena that are important for extracting semantic relations, but also gen-\\nerates a formal semantic representation of the meaning of the input sentence in\\nthe MRS (Minimal Recursion Semantics; [6]) representation format. Ambiguities\\nresulting in multiple readings per input sentence are ranked using a statistical\\nparse ranking model.\\nIn an earlier experiment, we obtained full deep parses for 64.89% of 955,581\\nsentences and 35.11% of the sentences were parsed by a fall-back shallow parser.\\nOnly 0.24% of the sentences could not be parsed at all.\\nIn this chapter, we describe the ﬁne-grained mapping of punctuation and\\nother tokenization details by means of a chart mapping technique [1] ensuring\\nthat this information is now optimally used by the deep grammar for disambigua-\\ntion. We also report on progress that we achieved by applying a chart pruning\\ntechnique [7] that, as already proven on another corpus, helps to considerably\\nincrease parsing speed of the deep parser and the number of successfully parsed\\nsentences. With both techniques applied together, we could not only increase\\nparsing speed considerably, but also the coverage on the ACL Anthology corpus\\nto more than 85%.\\nThis chapter is structured as follows. In section 2, we present the improved\\nparsing approach and results. In Section 3, we describe the semantic search\\napplication based on the improved parsing results. Section 4 discusses related\\nwork, and we ﬁnally conclude and give an outlook to future work in Section 5.\\n1 http://www.aclweb.org/anthology\\nAdvances in Deep Parsing of Scholarly Paper Content\\n3\\n2\\nDeep Parsing of Scholarly Papers\\nThe general idea of the semantics-oriented access to scholarly paper content is\\nto apply NLP analysis to each sentence they contain and distill a structured\\nrepresentation that can be searched for in addition to fulltext. Diﬀerent levels of\\nanalysis such as part-of-speech (PoS) tagging, named entity recognition (NER),\\nchunking, shallow and deep parsing are suitable for diﬀerent tasks.\\nWhile citation sentence classiﬁcation in scholarly papers, a further applica-\\ntion described in [16], is currently based on shallow NLP tasks such as tokeniza-\\ntion, PoS tagging and patterns thereof only, the semantic search application is\\nbased on the full range of hybrid, robustness-oriented NLP. This includes shal-\\nlow preprocessing with statistical taggers up to full deep parsing with generation\\nof sentence semantics representations from which basically predicate-argument\\nstructure is derived. Thus, both applications share the preprocessing, and in the\\nfuture, also citation sentence classiﬁcation could make use of linguistic features\\nextracted by more advanced NLP.\\n2.1\\nThe Corpus\\nThe basis of our scientiﬁc paper corpus is a subset of the ACL Anthology [2],\\na collection of conference and workshop papers in the ﬁeld of Computational\\nLinguistics and Language Technology. We concentrate on 8200 papers from the\\nyears 2002 through 2009 available in a native PDF format, i.e. not optically\\nscanned at limited quality such as many older papers. Except for named entity\\nrecognition which is partly based an a domain ontology, the processing pipeline\\nwe describe below is independent of the science domain. However, we expect\\nimprovements in the future by modeling domain knowledge, e.g. through auto-\\nmatically extracted domain speciﬁc terms and ontology concepts.\\n2.2\\nPDF Extraction\\nThe preprocessing step starts extracting clean text from the digital PDF docu-\\nments. In a ﬁrst version, we used PDFBox2 to gain raw text content from the\\npapers. This works well for most (especially recent) papers. However, it is prob-\\nlematic in general because PDFBox relies on the logical, digital content of the\\npage (layout) description language PDF. Its internal structure is very much de-\\npendent on the tool that was used to generate the PDF, and there are many tools\\nand of varying quality. Thus, decoding text from it does not work 100% correctly,\\nand imposes severe problems up to complete garbage because of non-standard\\ncharacter encodings or no output on about 10% of the corpus.\\nTo overcome these problems and become independent of the PDF encoder\\nthat was used to generate the digital paper, we recently moved to OCR-based\\nPDF extraction with the commercial product Abbyy PDF Transformer3. It also\\n2 http://pdfbox.apache.org\\n3 http://www.abbyy.com\\n4\\nUlrich Sch¨afer and Bernd Kiefer\\nreliably resolves hyphenated words using its own language model as well as text\\n(order) in tables. Moreover, and in contrast to PDFBox, it also works on scanned\\ndocuments, provided that the scan quality is good enough. However, recognition\\nof non-Latin characters such as in mathematical formulae remains a problem.\\nIt can be ignored for the time being because the NLP tools used also do not\\nunderstand mathematics.\\nAfter text extraction, a sentence splitter segments into sentence units in order\\nto provide suitable input for subsequent NLP. For each sentence, we record a\\nunique document ID (in case of our corpus the ACL Anthology paper ID, e.g.\\nC02-1023 for a paper from the COLING-2002 proceedings), the page on which\\nit appeared, and the sentence number relative to the whole document. Amongst\\nothers, this information is important to highlight a search result or citation\\nsentence within the original PDF paper layout.\\n2.3\\nHybrid Parsing\\nTo make the deep parser robust, it is embedded in a hybrid NLP workﬂow\\nimplemented using the hybrid NLP platform Heart of Gold [15]. Heart of Gold\\nis an XML-based middleware architecture for the integration of multilingual\\nshallow and deep natural language processing components, developed under the\\numbrella of the DELPH-IN initiative4.\\nThe employed Heart of Gold conﬁguration instance starts with a tokenizer,\\nthe shallow part-of-speech tagger TnT [3] and the named entity recognizer\\nSProUT [8]. These components help to identify and classify open class words\\nsuch as person names, events (e.g. conferences) or locations.\\nThe (trigram-based) tagger helps to guess part-of-speech tags of words un-\\nknown to the deep lexicon. For both unknown words and named entities, generic\\nlexicon entries are generated in the deep parser. By means of the PET input\\nchart XML format FSC [1], the shallow preprocessing results are combined and\\npassed to the high-speed HPSG [12] parser PET [4] running the open source\\nbroad-coverage grammar ERG [5] (cf. Fig 2).\\n2.4\\nPrecise Preprocessing Integration with Chart Mapping\\nChart mapping [1] is a novel mechanism for the non-monotonic, rule-based ma-\\nnipulation of chart items that are described by feature structures. There are\\ncurrently two chart mapping phases in PET during parsing: (1) Token map-\\nping, where input items as delivered by external preprocessors are adapted to\\nthe expectations of the grammar. This requires that input items are described\\nby feature structures – the token feature structures. (2) Lexical ﬁltering, where\\nlexical items can be ﬁltered by hard constraints after lexical parsing has ﬁnished.\\nToken mapping requires tokens to be described by feature structures. Token\\nfeature structures can be arbitrarily complex. This allows users to pass informa-\\ntion of various preprocessing modules into the parser. To this end, a new format,\\nthe XML-based FSC input format, was developed.\\n4 http://www.delph-in.net/heartofgold/\\nAdvances in Deep Parsing of Scholarly Paper Content\\n5\\nFollowing is an excerpt from the FSC for the sentence “Resnik and Smith\\n(2003) extract bilingual sentences from the Web to create parallel corpora for\\nmachine translation.” (from anthology document N07-1043) generated by Heart\\nof Gold preprocessing from TnT and SProUT output.\\n<fsc version=\"1.0\">\\n<chart id=\"hog://session1284321397757/collection1/TnT\">\\n<lattice init=\"v0\" final=\"v20\">\\n<edge source=\"v0\" target=\"v1\">\\n<fs type=\"token\">\\n<f name=\"+FORM\"><str>Resnik</str></f>\\n<f name=\"+FROM\"><str>0</str></f>\\n<f name=\"+TO\"><str>6</str></f>\\n<f name=\"+TNT\">\\n<fs type=\"tnt\">\\n<f name=\"+TAGS\" org=\"list\"><str>NNP</str></f>\\n<f name=\"+PRBS\" org=\"list\"><str>1.000000</str></f>\\n</fs>\\n</f>\\n</fs>\\n</edge>\\n... <!-- more token edges from TnT -->\\n<edge source=\"v6\" target=\"v7\">\\n<fs type=\"token\">\\n<f name=\"+FORM\"><str>extract</str></f>\\n<f name=\"+FROM\"><str>24</str></f>\\n<f name=\"+TO\"><str>31</str></f>\\n<f name=\"+TNT\">\\n<fs type=\"tnt\">\\n<f name=\"+TAGS\" org=\"list\"><str>VB</str></f>\\n<f name=\"+PRBS\" org=\"list\"><str>1.000000</str></f>\\n</fs>\\n</f>\\n</fs>\\n</edge>\\n... <!-- more token edges from TnT -->\\n<!-- this edge comes from the Named Entity Recognizer -->\\n<edge source=\"v0\" target=\"v6\">\\n<fs type=\"token\">\\n<f name=\"+FORM\"><str>Resnik and Smith (2003)</str></f>\\n<f name=\"+FROM\"><str>0</str></f>\\n<f name=\"+TO\"><str>23</str></f>\\n<f name=\"+TNT\"><fs type=\"null_tnt\"/></f>\\n<f name=\"+CLASS\"><fs type=\"proper_ne\"/></f>\\n<f name=\"+TRAIT\"><fs type=\"generic_trait\"/></f>\\n</fs>\\n</edge>\\n</lattice>\\n</chart>\\n</fsc>\\n6\\nUlrich Sch¨afer and Bernd Kiefer\\nFigure 1 shows how tokenized and PoS-tagged input is combined with pos-\\nsibly concurrent information from a named entity recognizer, in the example\\nSProUT delivering hypothetical information on named entities (here a citation\\nstring) in a single named entity item spanning over multiple words.\\nConcerning punctuation, the deep grammar can e.g. make use of information\\non opening and closing quotation marks. This information is often not explicit\\nin the input text, e.g. when gained through OCR techniques, which make no\\ndistinction between ‘ and ’ or “ and ”. However, a tokenizer can often guess\\n(reconstruct) leftness and rightness correctly. This information, passed to the\\ndeep parser via FSC, helps it to disambiguate.\\nv0\\nv1\\nFORM Resnik\\nFROM\\n0\\nTO\\n6\\nTNT\\nNNP\\nv6\\nFORM Resnik and Smith (2003)\\nFROM\\n0\\nTO\\n23\\nCLASS\\nproper_ne\\nv2\\nFORM and\\nFROM\\n7\\nTO\\n10\\nTNT\\nCC\\nv3\\nFORM Smith\\nFROM\\n11\\nTO\\n16\\nTNT\\nNNP\\nv4\\nFORM\\n(\\nFROM 17\\nTO\\n18\\nTNT\\n(\\nv5\\nFORM 2003\\nFROM\\n18\\nTO\\n22\\nTNT\\nCD\\nFORM\\n)\\nFROM 22\\nTO\\n23\\nTNT\\n)\\nFig. 1. FSC input to PET with combined information from tokenizer, PoS tagger and\\nconcurrent SProUT citation string item for input fragment “Resnik and Smith (2003)\\nextract ...”\\nFurthermore, a new way of generic lexical instantiation has been introduced\\nwith token feature structures and chart mapping. In this new setup, the parser\\ntries to instantiate all generic lexical entries for each word. Upon lexical instan-\\ntiation, the token feature is uniﬁed into a designated path of the lexical entry.\\nOnly if this uniﬁcation succeeds, the lexical item is instantiated. In order to con-\\ntrol the instantiation of generic lexical entries, the token feature structures are\\nappropriately constrained in the generic lexical entry, for instance by requiring\\nthat a generic verbal entry is only applicable for token feature structures where\\nthe highest ranked part-of-speech tag is a verb.\\n2.5\\nIncreased Processing Speed and Coverage through Chart\\nPruning\\nThe use of statistical models for result selection is well established for parsing\\nwith PET and ERG. We use a discriminative maximum entropy model based on\\nWeScience data [9] to compute the best parse results. Recently, [7] described the\\nuse of a generative model to increase eﬃciency by shaping the search space of\\nthe parser towards the more likely constituents and pruning very unlikely ones.\\nThis method not only results in lower parse times, but also in slightly better\\ncoverage, since sentences which could not be parsed due to timeouts now ﬁt into\\nthe given time bounds.\\nThe generative model is in fact a probabilistic context-free grammar (PCFG)\\ncomputed from the same tree banks as the discriminative model. The parser in\\nPET is a straightforward bottom-up chart parser with agenda, which makes it\\nAdvances in Deep Parsing of Scholarly Paper Content\\n7\\ninput\\nsentence\\ninput\\nsentence\\ntokenizer\\ntokenizer\\nPoS tagger\\nPoS tagger\\nnamed entity\\nrecognizer\\nnamed entity\\nrecognizer\\nPET parser\\nPET parser\\nPET XML\\ninput chart\\nPET XML\\ninput chart\\nMRX\\nMRX\\nsemantic tuples\\ndatabase\\nsemantic tuples\\ndatabase\\nsemantic tuples extractor\\nsemantic tuples extractor\\nFig. 2. Heart of Gold workﬂow for hybrid parsing and semantic tuples extraction\\neasy to use a model that has only local dependencies, such as PCFG. What\\nis missing is a heuristics to prune unlikely items in a way that has a small\\ncomputation overhead and will retain most of the items that are needed for the\\nglobally best results.\\n[11] did a very thorough comparison of diﬀerent performance optimization\\nstrategies, and among those also a local pruning strategy which is similar to the\\none used by [7]. It restricts the number of items given both their length and start\\npoint in the chart. This is easy to implement and avoids the use of complicated\\nheuristics to compensate the bias that shorter items become over longer chart\\nitems because of decreasing probability, which leads, without compensation, to\\na breadth-ﬁrst strategy for the whole parse. The number of items per chart cell\\nis restricted to a ﬁxed number to hinder the parser from getting lost in local\\nprobability maxima.\\nThere is an important diﬀerence to the system of [11], namely that their\\nsystem works on a reduced context-free backbone of the grammar and then\\nreconstructs the full results, while PET uses the full HPSG grammar directly,\\n8\\nUlrich Sch¨afer and Bernd Kiefer\\nwith subsumption packing and partial unpacking to achieve a similar eﬀect as\\nthe packed chart of a context-free parser.\\nThe local chart pruning results in a measurable speed-up with a negligible\\ndecrease in parsing accuracy; in fact, an increase in f-measure has been observed\\nbecause complicated sentences that had originally failed due to resource restric-\\ntions could now be parsed.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 70\\n 80\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nsentences x 1000\\nmean parse time (CPU s)\\nsentence length −→\\nFig. 3. Distribution of sentence length and mean parse times for mild pruning\\nProcessing Results. In total, we parsed 1,537,801 sentences, of which 57,832\\n(3.8%) could not be parsed because of lexicon errors which are mostly due to\\nOCR artifacts.\\nFigure 3 displays the average parse time of processing with moderate chart\\npruning, together with the mean quadratic error. In addition, it contains the\\ndistribution of input sentences over sentence length. Obviously, the vast majority\\nof sentences has a length up to 60 words maximum.\\nParse time was limited to 60 CPU seconds, and main memory consumption\\nto 4 GB, which was far more than ever needed by the processes. Overall, the\\nparse times only grow mildly due to the many optimization techniques in the\\noriginal system, and also the new chart pruning method. The sentence length\\ndistribution has been integrated into Figure 3 to show that the predominant part\\nof our real-world corpus can be processed using this information-rich method\\nwith very modest parse times.\\nThe large amount of short inputs is at ﬁrst surprising, moreover as most of\\nthese inputs can not be parsed, as can be seen in Figure 5. The explanation\\nAdvances in Deep Parsing of Scholarly Paper Content\\n9\\nis easy: most of these inputs are non-sentences such as headings, enumerations,\\nfootnotes and such. How we deal with this kind of input will be described in the\\nsection about fragmentary input.\\nAll measurements were carried out on an Intel XEON E5430 2.66GHz cluster\\ncomputer. Except for the parallelization, the used hardware equals a modern\\nstandard desktop PC, which again shows the feasibility of the used method.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nno pruning\\nmax400\\nmax100\\nsentence length −→\\nMean parse time (CPU sec) over sentence length\\nNo pruning\\nMax. 400 passive Max. 100 passive\\nAvg. Parse Time (CPU sec)\\n5.90\\n3.95\\n2.17\\nUnparsed Sentences\\n433104 (28.2%) 392758 (25.5%)\\n381019 (24.8%)\\nRecall\\n71.8%\\n74.5%\\n75.2%\\nBest Parse Lost\\n5.43%\\n19.7%\\nFig. 4. Comparison of results with diﬀerent chart pruning settings\\nFigure 4 shows the eﬀects of the chart pruning approach using moderate\\nas well as more aggressive pruning. The last row displays the amount of parsed\\nsentences which do not get the best results due to pruning. Note that the increase\\nin parsed sentences is only due to the reduced resource needs through pruning,\\nand that the lexical failures are not contained in the unparsed sentences ﬁgures.\\nFigure 5 shows the amount of unparsed sentences, split into two categories.\\nThe dots represent the sentences that could not be parsed due to time limitations,\\nthe solid lines those that were rejected by the grammar. Not surprisingly, the\\nfraction of sentences hitting the time bound increases noticeably for sentences\\n10\\nUlrich Sch¨afer and Bernd Kiefer\\nlonger that 60 words, but it should be noted that the percentage that can not\\nbe parsed because of grammatical reasons stays almost constant.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 70\\n 80\\n 90\\n 100\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nno pruning\\nno pruning timeout\\ncp400\\ncp400 timeout\\ncp100\\ncp100 timeout\\nsentence length −→\\nFig. 5. Percentage of unparsed sentences over sentence length\\nFor sentences with less than 40 words, aggressive chart pruning loses parses\\n(around 0.8%) that the mild pruning still does successfully, because edges needed\\nfor a full parse are pruned from the chart. In toto, the aggressive pruning gets\\nmore readings because it greatly improves recall on the longer sentences, but\\nsome are lost in the important middle range, which is also why we use the\\nresults from the mild pruning for the extraction of the semantics. An advanced\\nsystem could adapt pruning to the input length, or try to come up with better\\nlocal models that minimize the loss of useful subconstituents.\\nWe also compared the (absolute) scores of the discriminative model for the\\ntwo variants. While the method without chart pruning always ﬁnds the best\\nparse, this is not true for the pruned chart. The result is displayed in the fourth\\nrow of the table in Figure 4. Since the scores of the maximum entropy model\\nare not probabilities, we can not give meaningful numbers on the loss of quality,\\nbut a rough comparison of the scores suggests that in most cases the penalty is\\nminor.\\nFragmentary Input. There are several alternatives to deal with input like\\nheadings and footnotes, one to identify and handle them in a preprocessing\\nstep, another to use a special root condition in the deep analysis component\\nthat is able to combine phrases with well-deﬁned properties for inputs where no\\nspanning result could be found.\\nAdvances in Deep Parsing of Scholarly Paper Content\\n11\\nWe employed the second method, which has the advantage that it handles a\\nlarger range of phenomena in a homogeneous way. Figure 6 shows the change in\\npercentage of unparsed and timed out inputs for the mild pruning method with\\nand without the root condition combining fragments.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 70\\n 80\\n 90\\n 100\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nstrict\\nstrict timeout\\nstrict+fragments\\nstrict+fragments timeout\\nsentence length −→\\nFig. 6. Unparsed and timed out sentences with and without fragment combination\\nAs Figure 6 shows nicely, this changes the curve for unparsed sentences to-\\nwards more expected characteristics and removes the uncommonly high percent-\\nage of short sentences for which no parse can be found.\\nTogether with the parses for fragmented input, we get a recall (sentences\\nwith at least one parse) over the whole corpus of 85.9% (1,321,336 sentences),\\nwithout a signiﬁcant change for any of the other numbers.\\n2.6\\nParser Output\\nIn contrast to shallow parsers, the ERG not only handles detailed syntactic\\nanalyses of phrases, compounds, coordination, negation and other linguistic phe-\\nnomena that are important for extracting relations, but also generates a formal\\nsemantic representation of the meaning of the input sentence in the MRS repre-\\nsentation format (Minimal Recursion Semantics; [6]). It is comparable to a ﬁrst\\norder logic form. It consists of so-called elementary predications for each token\\nand larger constituents, connected via argument positions and variables/labels,\\nfrom which the predicate-argument structure can be derived (example in Fig-\\nure 7).\\n12\\nUlrich Sch¨afer and Bernd Kiefer\\n⟨h1,\\nh3:udef q(x5{PERS 3, NUM sg}, h4, h6),\\nh7: semantic a 1(e8{SF prop, TENSE untensed, MOOD indicative}, x5),\\nh7: similarity n to(x5, i9),\\nh10: measure v 1(e2{SF prop, TENSE pres, MOOD indicative, PROG -, PERF -}, p11, x5),\\nh10:parg d(e12{SF prop}, e2, x5),\\nh10: in p(e13{SF prop, TENSE untensed, MOOD indicative}, e2, x14{PERS 3, NUM pl, IND +}),\\nh15:udef q(x14, h16, h17),\\nh18: term n of(x14, x19{PERS 3, NUM pl}),\\nh20:udef q(x19, h21, h22),\\nh23:compound(e25{SF prop, TENSE untensed, MOOD indicative, PROG -, PERF -}, x19, x24),\\nh26:udef q(x24, h27, h28),\\nh29: similar a to(e30{SF prop, TENSE untensed, MOOD indicative}, x24),\\nh29:comp(e32{SF prop}, e30, u31),\\nh29: word n of(x24, i33),\\nh23: context n 1(x19)\\n{ h27 =q h29, h21 =q h23, h16 =q h18, h4 =q h7 } ⟩\\nFig. 7. Sample MRS for the sentence “Semantic similarity is measured in terms of\\nsimilar word contexts.”\\nAs in previous work [18] and because of the increased parsing recall, we again\\nopt for precision and only use results from the deep parser instead of extending\\nthe hybrid workﬂow (Figure 2) in such a way that a shallow parser with less\\ndetailed analyses is used as fall-back in case deep parsing fails (as done in an\\nintermediate system, [17]).\\n3\\nApplication: Semantic Search Based on Extracted\\nPredicate-Argument Structure\\nThe idea of the semantic search application is to use the sentence-wise semantic\\nrepresentations generated oﬄine by the deep parser. From its output, a normal-\\nized predicate-argument structure is extracted that is stored in a search index.\\nThe main motivation is at least partial abstraction from syntactic variants. Thus,\\nthe extraction process includes dividing sentences with coordination into inde-\\npendent structures, and using the semantic subject and object in both active\\nand passive sentence construction independently of the syntactic realization.\\nThe user interface for this application is simple. Instead of a single search text\\ninput ﬁeld, the user will see three: one for subject, one for predicate and another\\none for further objects. This is easy to understand also for non-linguists, and\\nﬁelds may be left emtpy to match anything. In the current version, the search\\ninterface supports the use of synsets of predicates only.\\n3.1\\nExtracting Predicate-Argument Structure from MRS\\nThe MRS representations resulting from hybrid parsing are relatively close to\\nlinguistic structures and contain more detailed information than a user would\\nAdvances in Deep Parsing of Scholarly Paper Content\\n13\\nlike to query and search for. Therefore, an additional extraction and abstraction\\nstep is necessary before storing the semantic structures in the search index.\\nThe format we devised for this purpose we call semantic tuples, a blend of\\ntriples and quintuples, as we store quintuples (subject, predicate, direct object,\\nother complements and adjunct), but to ease search term input for the user, only\\ndistinguish between a triple of subject, predicate and any other objects in the\\nquery structure.\\nThe algorithm to generate the semantic tuples ﬁrst performs an intermedi-\\nate transformation into isomorphic, serializable Java objects that can be made\\npersistent. On these objects, eﬃcient graph manipulation resulting in extracted\\nsemantic tuples can take place. Handling of coordination has been implemented\\nby generating multiple tuples. Passive constructions are elegantly handled by\\nthe grammar itself and lead to identical semantic tuples regardless of active or\\npassive formulation of the same proposition.\\nDue to semantic ambiguity, the deep parser may return more than one reading\\nper sentence. Currently up to three readings are considered (the most probable\\nones according to the treebank-trained parse ranking model), and semantic tu-\\nples are generated for each reading respectively. Multiple readings may collapse\\ninto the same semantic tuple structure, in which case only a single one is stored\\nin the database. Otherwise, a voting mechanism based on rank and number of\\nisomorphic semantic tuples decides for the best selection.\\nThe following sentence includes the semantic tuple structure (in brackets):\\n“[We]SUBJ [evaluate]PRED [the eﬃciency and performance]DOBJ\\n[against the corpus]ADJU.”\\nIn this example, the conjunction relation connects two noun phrases, both of\\nthem being DOBJ; therefore, no new semantic tuple is necessary. However, we\\ndecided to distinguish cases where conjunction connects two sentences or verb\\nphrases. In such cases, semantic tuples are generated for each part respectively.\\nThe following example shows an AND relation. Conjunction relations may also\\nbe realized in diﬀerent lexemes, e.g. and, but, or, as well as, etc.\\nFor the sentence “The system automatically extracts pairs of syntactic units\\nfrom a text and assigns a semantic relation to each pair.”, two semantic tuples\\nare generated separately with their own PRED, DOBJ and OCMP:\\n“[The system]SUBJ [extracts]PRED [pairs of syntactic units]DOBJ\\n[from a text]OCMP [automatically]ADJU.”\\nand\\n“[The system]SUBJ [assigns]PRED [a semantic relation]DOBJ\\n[to each pair]OCMP [automatically]ADJU.”\\nIn passive sentences, the syntactic subject becomes the semantic object and\\nvice versa:\\n“[Unseen input]DOBJ [was classiﬁed]PRED [by trained neural networks\\nwith varying error rates depending corpus type]SUBJ.”\\n14\\nUlrich Sch¨afer and Bernd Kiefer\\n3.2\\nFilling the Search Index\\nFor each sentence, the semantic tuple structure together with associated char-\\nacter span information relative to the sentence start is then stored in an Apache\\nSolr5 search index. It also contains metainformation on page number, sentence\\nnumber, oﬀset and document ID.\\nIn case a named entity is identiﬁed by the named entity recognizer, further in-\\nformation on span and type (such as location, person, time) of the item is stored.\\nThis named entity type information is used to identify the answer candidate type\\nin an additional question answering interface we will not further describe in this\\npaper. The following snippet from Solr input for a single sentence may give an\\nimpression of the underlying index schema.\\n<doc>\\n<field name=\"aclaid\">N07-1043</field>\\n<field name=\"page\">2</field>\\n<field name=\"sentno\">56</field>\\n<field name=\"prefix\">N07-1043-s56-p2</field>\\n<field name=\"offset\">353</field>\\n<field name=\"qgen\">PET</field>\\n<field name=\"sentence\">Sahami et al., (2006) measure semantic\\nsimilarity between two queries using the snippets returned\\nfor those queries by a search engine.</field>\\n<field name=\"subj\">Sahami 2006 et al.</field>\\n<field name=\"subj_start\">0</field>\\n<field name=\"subj_end\">12</field>\\n<field name=\"pred\">measure</field>\\n<field name=\"pred_start\">22</field>\\n<field name=\"pred_end\">28</field>\\n<field name=\"dobj\">semantic similarity</field>\\n<field name=\"dobj_start\">30</field>\\n<field name=\"dobj_end\">48</field>\\n<field name=\"ocmp\">between two queries using the snippets\\nreturned for those queries by a search engine</field>\\n<field name=\"ocmp_start\">0</field>\\n<field name=\"ocmp_end\">133</field>\\n<field name=\"ner_types\">citation ne-term ne-term </field>\\n<field name=\"ner_cstart\">0 30 121 </field>\\n<field name=\"ner_cend\">20 48 133 </field>\\n<field name=\"ner_surface\">\"Sahami et al., (2006)\"\\n\"semantic similarity\"\\n\"search engine\" </field>\\n</doc>\\nTo sum up the overall oﬄine analysis for search index generation, Figure 8\\ndepicts the oﬄine NLP and semantic tuple extraction workﬂow.\\n5 http://lucene.apache.org/solr\\nAdvances in Deep Parsing of Scholarly Paper Content\\n15\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\nsemantic tuples\\ndatabase\\nsemantic tuples\\ndatabase\\n● text cleaning\\n● XML encoding\\n● text cleaning\\n● XML encoding\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\nNLP grid with \\nJTok, TnT, \\nSProUT, PET\\nsemantic tuples\\nextraction\\nsemantic tuples\\nextraction\\n+\\n+\\nscholarly papers\\n2 GB PDF\\nOCR-based\\nPDF-to-text\\nextraction \\nOCR-based\\nPDF-to-text\\nextraction \\nHeart \\nof Gold\\nHeart \\nof Gold\\nAutomatic\\nNLP XML\\nannotation\\n1 GB Apache Solr Blob (approx. 1.5 million sentences)\\nFig. 8. Grid-based hybrid parsing of the scientiﬁc paper corpus\\n3.3\\nQuery Interface\\nAs depicted in Figure 9, the user interface for semantic paper search contains\\nthree text ﬁelds where the user can input subject, predicate and all remaining\\nstructures (rest). The latter is combined to ease input (otherwise users would\\nbecome worried about what to put in OCMP or ADJU) and will be expanded\\nto a disjunctive Solr/Lucene query expression.\\nFig. 9. Simple query interface\\nTo give an example, a semantic tuple search expression with input to ﬁeld\\nsubject=*, input to ﬁeld predicate=‘measure’, and input to ﬁeld rest=‘semantic\\nsimilarity’ is translated into an Apache Solr query\\npred:measure +(dobj:\"semantic similarity\"\\n16\\nUlrich Sch¨afer and Bernd Kiefer\\nOR ocmp:\"semantic similarity\"\\nOR adju:\"semantic similarity\")\\nIn case WordNet synset [10] expansion is enabled, measure is replaced by\\n(measure OR evaluate OR quantify OR value OR assess OR valuate).\\nIt is planned to also allow for synonym search in the SUBJ and REST ﬁeld.\\nHere, domain ontology information as well as automatically identiﬁed similar\\n(multi-word) terms could be used to expand the query.\\nSearch Results for * “measure” “semantic similarity”\\n– N07-1043: Sahami et al., (2006) [measure]PRED [semantic similarity]DOBJ\\nbetween two queries using the snippets returned for those queries by a\\nsearch engine.\\n– W04-0106: [Semantic similarity]DOBJ [is measured]PRED in terms of sim-\\nilar word contexts.\\n– N07-1044: [The semantic similarity]DOBJ between neighbors and senses [is\\nmeasured]PRED using a manually crafted taxonomy such as WordNet (see\\nBudanitsky and Hirst 2001 for an overview of WordNet-based similarity\\nmeasures).\\n– P08-1028: We [assessed]PRED [a wide range of semantic similarity\\nmeasures]DOBJ using the WordNet similarity package (Pedersen et al.,\\n2004).\\n– W06-3802:\\nUsing\\nWordNet,\\nwe\\n[can\\nmeasure]PRED\\n[the\\nsemantic\\nsimilarity]DOBJ or relatedness between a pair of concepts (or word senses),\\nand by extension, between a pair of sentences.\\n– W06-1659:\\nUsing\\nWordNet,\\nwe\\n[can\\nmeasure]PRED\\n[the\\nsemantic\\nsimilarity]DOBJ or relatedness between a pair of concepts (or word senses),\\nand by extension, between a pair of sentences.\\n– W05-1203: For entailment identiﬁcation, since this is a directional relation,\\nwe [only measure]PRED [the semantic similarity]DOBJ with respect to the\\nhypothesis (the text that is entailed).\\n– W06-1104: We [measured]PRED [semantic relat-edness instead of semantic\\nsimilarity]DOBJ.\\n– P06-1112:\\n3.\\n[The\\nsemantic\\nsimilarity\\nSemSim(h\\n,\\nh\\n)]DOBJ\\n[is\\nmeasured]PRED using Word-Net and eXtended WordNet.\\n. . .\\nFig. 10. The ﬁrst matching sentences in the ACL Anthology subset 2002-2008 with\\nrecognized variation in predicate synsets (assess, measure, evaluate) and passive con-\\nstructions\\nThe result is then a list of sentence snippets (Figure 10). By clicking on a\\nhyperlink underlying the snippet text, the original PDF is opened. By using\\nthe information on page and sentence text/oﬀset in the Apache Solr answer,\\nthe result sentence is highlighted as shown in Figure 11. This helps to quickly\\nidentify relevance of the answer by looking at context in the original layout.\\nAdvances in Deep Parsing of Scholarly Paper Content\\n17\\nFig. 11. First result sentence (from N07-1043) highlighted in original PDF\\n4\\nRelated Work\\nUsing HPSG combined with shallow domain-speciﬁc modeling for high-precision\\nanalysis of scientiﬁc texts is an emerging research area. Another ERG-based\\napproach to relation and information extraction from scientiﬁc texts is SciBorg\\n[13]. SciBorg mainly deals with chemistry research papers and handles domain-\\nspeciﬁc phenomena with a specialized named entity recognizer. It relies on a\\nshallow parser as robustness fall-back for MRS generation.\\nOther groups use less elaborated and ﬁne-grained HPSG grammars than\\nERG. [11] report on large-scale parsing of MEDLINE articles (1.4 billion words)\\nwith such a simpliﬁed grammar.\\n[14] use shallow dependency structure and results from HPSG parsing for\\nextracting protein-protein interactions (PPI) from research papers. The same\\ngroup has also worked on medical texts: MEDIE6 is a semantic search engine to\\nretrieve biomedical correlations from MEDLINE articles.\\nWhat distinguishes our approach from those, besides concentration on a dif-\\nferent scientiﬁc area, is the focus on and use of ontology information as integrated\\npart of linguistic analysis, use of the most comprehensive and elaborated HPSG\\ngrammar for English (ERG), and the interactive user interface (Scientist’s Work-\\nbench application; [17]) and editor [18].\\n5\\nConclusion and Future Work\\nWe have presented our recent advances in full, robust parsing of scientiﬁc papers\\ntexts. By careful preprocessing and novel approaches to eﬃcient parsing of long\\nsentences, we could improve coverage from 65 to more than 85%.\\nThe semantic search application built on the semantic representations gen-\\nerated by the deep grammar is a useful extension to cope with synonyms and\\nsyntactic variation when querying full scientiﬁc publication content. The search\\nspace, initially expanded by adding synonymns, can be again constrained by\\nimposing semantic subject-predicate-object structure in the query.\\n6 http://www-tsujii.is.s.u-tokyo.ac.jp/medie/\\n18\\nUlrich Sch¨afer and Bernd Kiefer\\nFurther research goals are improving robustness of the NLP tool chain. We\\nare also working on generic techniques to automatically extract and use sci-\\nence domain information from the underlying paper corpus to improve targeted\\nsearch. Three main tasks in our focus are coreference resolution, term extraction\\nand ontology extraction viz. population. The idea is that these techniques, in a\\nﬁrst step gained independently from the text corpus or partially from NLP anal-\\nyses of it, will beneﬁt from each other and can be used to build more reliable\\nand precise resources and tools in a bootstrapping process.\\nHandling of negation, modal constructions, subclauses etc. also fall into the\\ncategory deep NLP can handle, but this will be addressed in the future as it also\\nrequires lexico-semantic information of verbs etc. in the extraction process. It\\nwill deﬁnitely be an important extension helping to improve precision in search.\\nThe semantic search application is part of the Scientist’s workbench and is\\ncomplemented by a visualization and navigation tool TeeCeeGeeNav [16] that\\nsupports scientists in quickly getting an overview of a (new) research ﬁeld by\\nbrowsing through a typed citation graph computed from the scientiﬁc paper\\ncorpus. The citation classiﬁcation with categories such as use or refutation of\\nresults of the cited paper currently builds on shallow NLP (such as PoS tagging)\\nonly. In the future, deep semantics could help too further improve this diﬃcult\\nclassiﬁcation task.\\nAcknowledgments\\nThe authors would like to thank Peter Adolphs, Dan Flickinger and Stephan\\nOepen for their support and numerous fruitful discussions. We would also like\\nto thank Yi Zhang and Bart Cramer for the implementation of chart pruning in\\nPET and their support to put it into use. The work described in this paper has\\nbeen carried out in the context of the project TAKE (Technologies for Advanced\\nKnowledge Extraction), funded under contract 01IW08003 by the German Fed-\\neral Ministry of Education and Research, and in the context of the world-wide\\nDELPH-IN collaboration7.\\nReferences\\n1. Adolphs, P., Oepen, S., Callmeier, U., Crysmann, B., Flickinger, D., Kiefer, B.:\\nSome ﬁne points of hybrid natural language parsing. In: Proc. of LREC. pp. 1380–\\n1387. Marrakesh, Morocco (2008)\\n2. Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M., Kan, M.Y., Lee, D., Powley,\\nB., Radev, D., Tan, Y.F.: The ACL anthology reference corpus: A reference dataset\\nfor bibliographic research. In: Proc. of LREC. pp. 1755–1759. Marrakesh, Morocco\\n(2008)\\n3. Brants, T.: TnT – A Statistical Part-of-Speech Tagger. In: Proc. of ANLP-2000.\\npp. 224–231. Seattle, WA (2000)\\n7 DEep Linguistic Processing with Hpsg INitiative; http://www.delph-in.net\\nAdvances in Deep Parsing of Scholarly Paper Content\\n19\\n4. Callmeier, U.: PET – A platform for experimentation with eﬃcient HPSG process-\\ning techniques. Natural Language Engineering 6(1), 99–108 (2000)\\n5. Copestake, A., Flickinger, D.: An open-source grammar development environment\\nand broad-coverage English grammar using HPSG. In: Proc. of LREC. pp. 591–\\n598. Athens, Greece (2000)\\n6. Copestake, A., Flickinger, D., Sag, I.A., Pollard, C.: Minimal recursion semantics:\\nan introduction. Research on Language and Computation 3(2–3), 281–332 (2005)\\n7. Cramer, B., Zhang, Y.: Constraining robust constructions for broad-coverage pars-\\ning with precision grammars. In: Proc. of COLING. pp. 223–231. Beijing, China\\n(2010)\\n8. Dro˙zd˙zy´nski, W., Krieger, H.U., Piskorski, J., Sch¨afer, U., Xu, F.: Shallow process-\\ning with uniﬁcation and typed feature structures – foundations and applications.\\nK¨unstliche Intelligenz 2004(1), 17–23 (2004)\\n9. Flickinger, D., Oepen, S., Ytrestøl, G.: WikiWoods: Syntacto-semantic annotation\\nfor English Wikipedia. In: Proc. of LREC. pp. 1665–1671. Valletta, Malta (2010)\\n10. Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K.J.: Five papers on\\nWordNet. Tech. rep., Cognitive Science Laboratory, Princeton University (1993)\\n11. Ninomiya, T., Tsuruoka, Y., Miyao, Y., Taura, K., Tsujii, J.: Fast and scalable\\nHPSG parsing. Traitement automatique des langues (TAL) 46(2) (2006)\\n12. Pollard, C., Sag, I.A.: Head-Driven Phrase Structure Grammar. Studies in Con-\\ntemporary Linguistics, University of Chicago Press, Chicago (1994)\\n13. Rupp, C., Copestake, A., Corbett, P., Waldron, B.: Integrating general-purpose\\nand domain-speciﬁc components in the analysis of scientiﬁc text. In: Proc. of the\\nUK e-Science Programme All Hands Meeting 2007. Nottingham, UK (2007)\\n14. Sætre, R., Kenji, S., Tsujii, J.: Syntactic features for protein-protein interaction\\nextraction. In: Baker, C.J., Jian, S. (eds.) Short Paper Proc. of the 2nd Int. Symp.\\non Languages in Biology and Medicine (LBM 2007). pp. 6.1–6.14. Singapore (2008)\\n15. Sch¨afer, U.: Middleware for creating and combining multi-dimensional NLP\\nmarkup. In: Proc. of the EACL-2006 Workshop on Multi-dimensional Markup in\\nNatural Language Processing. pp. 81–84. Trento, Italy (2006)\\n16. Sch¨afer, U., Kasterka, U.: Scientiﬁc authoring support: A tool to navigate in typed\\ncitation graphs. In: Proc. of the NAACL-HLT 2010 Workshop on Computational\\nLinguistics and Writing. pp. 7–14. Los Angeles, CA (2010)\\n17. Sch¨afer, U., Spurk, C.: TAKE Scientist’s Workbench: Semantic search and citation-\\nbased visual navigation in scholar papers. In: Proc. of the 4th IEEE Int. Conference\\non Semantic Computing (ICSC-2010). pp. 317–324. Pittsburgh, PA (2010)\\n18. Sch¨afer, U., Uszkoreit, H., Federmann, C., Marek, T., Zhang, Y.: Extracting and\\nquerying relations in scientiﬁc papers. In: Proc. of the 31st Annual German Confer-\\nence on Artiﬁcial Intelligence (KI-2008). pp. 127–134. Springer LNAI 5243 (2008)\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f0bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c632b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Advances in Deep Parsing of Scholarly Paper\\nContent\\nUlrich Sch¨afer and Bernd Kiefer\\nLanguage Technology Lab\\nGerman Research Center for Artiﬁcial Intelligence (DFKI)\\nCampus D3 1, D-66123 Saarbr¨ucken, Germany\\n{ulrich.schaefer,kiefer}@dfki.de\\nhttp://www.dfki.de/lt\\nAbstract. We report on advances in deep linguistic parsing of the full\\ntextual content of 8200 papers from the ACL Anthology, a collection of\\nelectronically available scientiﬁc papers in the ﬁelds of Computational\\nLinguistics and Language Technology.\\nWe describe how – by incorporating new techniques – we increase both\\nspeed and robustness of deep analysis, speciﬁcally on long sentences'),\n",
       " Document(metadata={}, page_content='We describe how – by incorporating new techniques – we increase both\\nspeed and robustness of deep analysis, speciﬁcally on long sentences\\nwhere deep parsing often failed in former approaches. With the current\\nopen source HPSG (Head-driven phrase structure grammar) for English\\n(ERG), we obtain deep parses for more than 85% of the sentences in the\\n1.5 million sentences corpus, while the former approaches achieved only\\napprox. 65% coverage.\\nThe resulting sentence-wise semantic representations are used in the Sci-\\nentist’s Workbench, a platform demonstrating the use and beneﬁt of\\nnatural language processing (NLP) to support scientists or other knowl-'),\n",
       " Document(metadata={}, page_content='entist’s Workbench, a platform demonstrating the use and beneﬁt of\\nnatural language processing (NLP) to support scientists or other knowl-\\nedge workers in fast and better access to digital document content. With\\nthe generated NLP annotations, we are able to implement important,\\nnovel applications such as robust semantic search, citation classiﬁcation,\\nand (in the future) question answering and deﬁnition exploration.\\n1\\nIntroduction\\nScientists in all disciplines are nowadays faced with a ﬂood of new publications\\nevery day. In addition, more and more publications from the past become dig-\\nitally available and thus even increase the amount of data. Therefore, ﬁnding'),\n",
       " Document(metadata={}, page_content='itally available and thus even increase the amount of data. Therefore, ﬁnding\\nrelevant information and avoiding redundancy and duplication of work have be-\\ncome urgent issues to be addressed by the scientiﬁc community.\\nThe organization and preservation of scientiﬁc knowledge in scientiﬁc pub-\\nlications, vulgo text documents, thwarts these eﬀorts. From a viewpoint of a\\ncomputer scientist, scientiﬁc papers are just ‘unstructured information’.\\nAutomatically precomputed, normalized semantic representations of textual\\nutterances could help to structure the search space and ﬁnd equivalent or related\\npropositions even if they are expressed diﬀerently, e.g. in passive constructions,'),\n",
       " Document(metadata={}, page_content='propositions even if they are expressed diﬀerently, e.g. in passive constructions,\\nusing synonyms etc. Domain-relevant semantic similarity can be computed auto-\\nmatically and exploited as additional knowledge source to support robust search.\\n⋆Pre-print. The original publication is available at http://www.springerlink.com.\\n2\\nUlrich Sch¨afer and Bernd Kiefer\\nTo again constrain the so expanded search space, users can ask the system\\nin simply structured subject-predicate-object queries and get all matching, pre-\\ncomputed predicate-argument structures along with the original sentence from\\nthe paper. On the other hand, by storing the structure along with the original'),\n",
       " Document(metadata={}, page_content='the paper. On the other hand, by storing the structure along with the original\\ntext in a structured full-text search engine such as Apache Lucene, it can be\\nguaranteed that recall cannot fall behind the baseline of a fulltext search engine.\\nThe basis of our scientiﬁc paper corpus is a subset of the ACL Anthology1,\\na collection of conference and workshop papers in the ﬁeld of Computational\\nLinguistics and Language Technology. We concentrate on 8200 papers from the\\nyears 2002 through 2009 from which we extracted the textual content using\\nAbbyy PDF Transformer.\\nExcept for named entity recognition which is partly based on instances and'),\n",
       " Document(metadata={}, page_content='Abbyy PDF Transformer.\\nExcept for named entity recognition which is partly based on instances and\\nconcepts of a domain ontology, the processing pipeline we describe below is\\nindependent of the science domain.\\nTo make the deep parser robust, it is embedded in a hybrid NLP workﬂow\\nstarting with a tokenizer, a part-of-speech tagger, and a named entity recognizer.\\nThese components help to identify and classify open class words such as person\\nnames, events (e.g. conferences) or locations. The trigram-based tagger helps\\nto guess part-of-speech tags of words unknown to the deep lexicon. For both\\nunknown words and named entities, generic lexicon entries are generated in the'),\n",
       " Document(metadata={}, page_content='unknown words and named entities, generic lexicon entries are generated in the\\ndeep parser running the open source broad-coverage grammar ERG [5].\\nIn contrast to shallow parsers, the ERG not only handles detailed syntac-\\ntic analyses of phrases, compounds, coordination, negation and other linguistic\\nphenomena that are important for extracting semantic relations, but also gen-\\nerates a formal semantic representation of the meaning of the input sentence in\\nthe MRS (Minimal Recursion Semantics; [6]) representation format. Ambiguities\\nresulting in multiple readings per input sentence are ranked using a statistical\\nparse ranking model.'),\n",
       " Document(metadata={}, page_content='resulting in multiple readings per input sentence are ranked using a statistical\\nparse ranking model.\\nIn an earlier experiment, we obtained full deep parses for 64.89% of 955,581\\nsentences and 35.11% of the sentences were parsed by a fall-back shallow parser.\\nOnly 0.24% of the sentences could not be parsed at all.\\nIn this chapter, we describe the ﬁne-grained mapping of punctuation and\\nother tokenization details by means of a chart mapping technique [1] ensuring\\nthat this information is now optimally used by the deep grammar for disambigua-\\ntion. We also report on progress that we achieved by applying a chart pruning'),\n",
       " Document(metadata={}, page_content='tion. We also report on progress that we achieved by applying a chart pruning\\ntechnique [7] that, as already proven on another corpus, helps to considerably\\nincrease parsing speed of the deep parser and the number of successfully parsed\\nsentences. With both techniques applied together, we could not only increase\\nparsing speed considerably, but also the coverage on the ACL Anthology corpus\\nto more than 85%.\\nThis chapter is structured as follows. In section 2, we present the improved\\nparsing approach and results. In Section 3, we describe the semantic search\\napplication based on the improved parsing results. Section 4 discusses related'),\n",
       " Document(metadata={}, page_content='application based on the improved parsing results. Section 4 discusses related\\nwork, and we ﬁnally conclude and give an outlook to future work in Section 5.\\n1 http://www.aclweb.org/anthology\\nAdvances in Deep Parsing of Scholarly Paper Content\\n3\\n2\\nDeep Parsing of Scholarly Papers\\nThe general idea of the semantics-oriented access to scholarly paper content is\\nto apply NLP analysis to each sentence they contain and distill a structured\\nrepresentation that can be searched for in addition to fulltext. Diﬀerent levels of\\nanalysis such as part-of-speech (PoS) tagging, named entity recognition (NER),\\nchunking, shallow and deep parsing are suitable for diﬀerent tasks.'),\n",
       " Document(metadata={}, page_content='analysis such as part-of-speech (PoS) tagging, named entity recognition (NER),\\nchunking, shallow and deep parsing are suitable for diﬀerent tasks.\\nWhile citation sentence classiﬁcation in scholarly papers, a further applica-\\ntion described in [16], is currently based on shallow NLP tasks such as tokeniza-\\ntion, PoS tagging and patterns thereof only, the semantic search application is\\nbased on the full range of hybrid, robustness-oriented NLP. This includes shal-\\nlow preprocessing with statistical taggers up to full deep parsing with generation\\nof sentence semantics representations from which basically predicate-argument'),\n",
       " Document(metadata={}, page_content='of sentence semantics representations from which basically predicate-argument\\nstructure is derived. Thus, both applications share the preprocessing, and in the\\nfuture, also citation sentence classiﬁcation could make use of linguistic features\\nextracted by more advanced NLP.\\n2.1\\nThe Corpus\\nThe basis of our scientiﬁc paper corpus is a subset of the ACL Anthology [2],\\na collection of conference and workshop papers in the ﬁeld of Computational\\nLinguistics and Language Technology. We concentrate on 8200 papers from the\\nyears 2002 through 2009 available in a native PDF format, i.e. not optically\\nscanned at limited quality such as many older papers. Except for named entity'),\n",
       " Document(metadata={}, page_content='scanned at limited quality such as many older papers. Except for named entity\\nrecognition which is partly based an a domain ontology, the processing pipeline\\nwe describe below is independent of the science domain. However, we expect\\nimprovements in the future by modeling domain knowledge, e.g. through auto-\\nmatically extracted domain speciﬁc terms and ontology concepts.\\n2.2\\nPDF Extraction\\nThe preprocessing step starts extracting clean text from the digital PDF docu-\\nments. In a ﬁrst version, we used PDFBox2 to gain raw text content from the\\npapers. This works well for most (especially recent) papers. However, it is prob-'),\n",
       " Document(metadata={}, page_content='papers. This works well for most (especially recent) papers. However, it is prob-\\nlematic in general because PDFBox relies on the logical, digital content of the\\npage (layout) description language PDF. Its internal structure is very much de-\\npendent on the tool that was used to generate the PDF, and there are many tools\\nand of varying quality. Thus, decoding text from it does not work 100% correctly,\\nand imposes severe problems up to complete garbage because of non-standard\\ncharacter encodings or no output on about 10% of the corpus.\\nTo overcome these problems and become independent of the PDF encoder\\nthat was used to generate the digital paper, we recently moved to OCR-based'),\n",
       " Document(metadata={}, page_content='To overcome these problems and become independent of the PDF encoder\\nthat was used to generate the digital paper, we recently moved to OCR-based\\nPDF extraction with the commercial product Abbyy PDF Transformer3. It also\\n2 http://pdfbox.apache.org\\n3 http://www.abbyy.com\\n4\\nUlrich Sch¨afer and Bernd Kiefer\\nreliably resolves hyphenated words using its own language model as well as text\\n(order) in tables. Moreover, and in contrast to PDFBox, it also works on scanned\\ndocuments, provided that the scan quality is good enough. However, recognition\\nof non-Latin characters such as in mathematical formulae remains a problem.\\nIt can be ignored for the time being because the NLP tools used also do not'),\n",
       " Document(metadata={}, page_content='It can be ignored for the time being because the NLP tools used also do not\\nunderstand mathematics.\\nAfter text extraction, a sentence splitter segments into sentence units in order\\nto provide suitable input for subsequent NLP. For each sentence, we record a\\nunique document ID (in case of our corpus the ACL Anthology paper ID, e.g.\\nC02-1023 for a paper from the COLING-2002 proceedings), the page on which\\nit appeared, and the sentence number relative to the whole document. Amongst\\nothers, this information is important to highlight a search result or citation\\nsentence within the original PDF paper layout.\\n2.3\\nHybrid Parsing\\nTo make the deep parser robust, it is embedded in a hybrid NLP workﬂow'),\n",
       " Document(metadata={}, page_content='sentence within the original PDF paper layout.\\n2.3\\nHybrid Parsing\\nTo make the deep parser robust, it is embedded in a hybrid NLP workﬂow\\nimplemented using the hybrid NLP platform Heart of Gold [15]. Heart of Gold\\nis an XML-based middleware architecture for the integration of multilingual\\nshallow and deep natural language processing components, developed under the\\numbrella of the DELPH-IN initiative4.\\nThe employed Heart of Gold conﬁguration instance starts with a tokenizer,\\nthe shallow part-of-speech tagger TnT [3] and the named entity recognizer\\nSProUT [8]. These components help to identify and classify open class words\\nsuch as person names, events (e.g. conferences) or locations.'),\n",
       " Document(metadata={}, page_content='SProUT [8]. These components help to identify and classify open class words\\nsuch as person names, events (e.g. conferences) or locations.\\nThe (trigram-based) tagger helps to guess part-of-speech tags of words un-\\nknown to the deep lexicon. For both unknown words and named entities, generic\\nlexicon entries are generated in the deep parser. By means of the PET input\\nchart XML format FSC [1], the shallow preprocessing results are combined and\\npassed to the high-speed HPSG [12] parser PET [4] running the open source\\nbroad-coverage grammar ERG [5] (cf. Fig 2).\\n2.4\\nPrecise Preprocessing Integration with Chart Mapping\\nChart mapping [1] is a novel mechanism for the non-monotonic, rule-based ma-'),\n",
       " Document(metadata={}, page_content='2.4\\nPrecise Preprocessing Integration with Chart Mapping\\nChart mapping [1] is a novel mechanism for the non-monotonic, rule-based ma-\\nnipulation of chart items that are described by feature structures. There are\\ncurrently two chart mapping phases in PET during parsing: (1) Token map-\\nping, where input items as delivered by external preprocessors are adapted to\\nthe expectations of the grammar. This requires that input items are described\\nby feature structures – the token feature structures. (2) Lexical ﬁltering, where\\nlexical items can be ﬁltered by hard constraints after lexical parsing has ﬁnished.\\nToken mapping requires tokens to be described by feature structures. Token'),\n",
       " Document(metadata={}, page_content='Token mapping requires tokens to be described by feature structures. Token\\nfeature structures can be arbitrarily complex. This allows users to pass informa-\\ntion of various preprocessing modules into the parser. To this end, a new format,\\nthe XML-based FSC input format, was developed.\\n4 http://www.delph-in.net/heartofgold/\\nAdvances in Deep Parsing of Scholarly Paper Content\\n5\\nFollowing is an excerpt from the FSC for the sentence “Resnik and Smith\\n(2003) extract bilingual sentences from the Web to create parallel corpora for\\nmachine translation.” (from anthology document N07-1043) generated by Heart\\nof Gold preprocessing from TnT and SProUT output.\\n<fsc version=\"1.0\">'),\n",
       " Document(metadata={}, page_content='machine translation.” (from anthology document N07-1043) generated by Heart\\nof Gold preprocessing from TnT and SProUT output.\\n<fsc version=\"1.0\">\\n<chart id=\"hog://session1284321397757/collection1/TnT\">\\n<lattice init=\"v0\" final=\"v20\">\\n<edge source=\"v0\" target=\"v1\">\\n<fs type=\"token\">\\n<f name=\"+FORM\"><str>Resnik</str></f>\\n<f name=\"+FROM\"><str>0</str></f>\\n<f name=\"+TO\"><str>6</str></f>\\n<f name=\"+TNT\">\\n<fs type=\"tnt\">\\n<f name=\"+TAGS\" org=\"list\"><str>NNP</str></f>\\n<f name=\"+PRBS\" org=\"list\"><str>1.000000</str></f>\\n</fs>\\n</f>\\n</fs>\\n</edge>\\n... <!-- more token edges from TnT -->\\n<edge source=\"v6\" target=\"v7\">\\n<fs type=\"token\">\\n<f name=\"+FORM\"><str>extract</str></f>\\n<f name=\"+FROM\"><str>24</str></f>'),\n",
       " Document(metadata={}, page_content='<edge source=\"v6\" target=\"v7\">\\n<fs type=\"token\">\\n<f name=\"+FORM\"><str>extract</str></f>\\n<f name=\"+FROM\"><str>24</str></f>\\n<f name=\"+TO\"><str>31</str></f>\\n<f name=\"+TNT\">\\n<fs type=\"tnt\">\\n<f name=\"+TAGS\" org=\"list\"><str>VB</str></f>\\n<f name=\"+PRBS\" org=\"list\"><str>1.000000</str></f>\\n</fs>\\n</f>\\n</fs>\\n</edge>\\n... <!-- more token edges from TnT -->\\n<!-- this edge comes from the Named Entity Recognizer -->\\n<edge source=\"v0\" target=\"v6\">\\n<fs type=\"token\">\\n<f name=\"+FORM\"><str>Resnik and Smith (2003)</str></f>\\n<f name=\"+FROM\"><str>0</str></f>\\n<f name=\"+TO\"><str>23</str></f>\\n<f name=\"+TNT\"><fs type=\"null_tnt\"/></f>\\n<f name=\"+CLASS\"><fs type=\"proper_ne\"/></f>'),\n",
       " Document(metadata={}, page_content='<f name=\"+FROM\"><str>0</str></f>\\n<f name=\"+TO\"><str>23</str></f>\\n<f name=\"+TNT\"><fs type=\"null_tnt\"/></f>\\n<f name=\"+CLASS\"><fs type=\"proper_ne\"/></f>\\n<f name=\"+TRAIT\"><fs type=\"generic_trait\"/></f>\\n</fs>\\n</edge>\\n</lattice>\\n</chart>\\n</fsc>\\n6\\nUlrich Sch¨afer and Bernd Kiefer\\nFigure 1 shows how tokenized and PoS-tagged input is combined with pos-\\nsibly concurrent information from a named entity recognizer, in the example\\nSProUT delivering hypothetical information on named entities (here a citation\\nstring) in a single named entity item spanning over multiple words.\\nConcerning punctuation, the deep grammar can e.g. make use of information'),\n",
       " Document(metadata={}, page_content='string) in a single named entity item spanning over multiple words.\\nConcerning punctuation, the deep grammar can e.g. make use of information\\non opening and closing quotation marks. This information is often not explicit\\nin the input text, e.g. when gained through OCR techniques, which make no\\ndistinction between ‘ and ’ or “ and ”. However, a tokenizer can often guess\\n(reconstruct) leftness and rightness correctly. This information, passed to the\\ndeep parser via FSC, helps it to disambiguate.\\nv0\\nv1\\nFORM Resnik\\nFROM\\n0\\nTO\\n6\\nTNT\\nNNP\\nv6\\nFORM Resnik and Smith (2003)\\nFROM\\n0\\nTO\\n23\\nCLASS\\nproper_ne\\nv2\\nFORM and\\nFROM\\n7\\nTO\\n10\\nTNT\\nCC\\nv3\\nFORM Smith\\nFROM\\n11\\nTO\\n16\\nTNT\\nNNP\\nv4\\nFORM\\n(\\nFROM 17\\nTO\\n18\\nTNT\\n(\\nv5'),\n",
       " Document(metadata={}, page_content='FROM\\n0\\nTO\\n23\\nCLASS\\nproper_ne\\nv2\\nFORM and\\nFROM\\n7\\nTO\\n10\\nTNT\\nCC\\nv3\\nFORM Smith\\nFROM\\n11\\nTO\\n16\\nTNT\\nNNP\\nv4\\nFORM\\n(\\nFROM 17\\nTO\\n18\\nTNT\\n(\\nv5\\nFORM 2003\\nFROM\\n18\\nTO\\n22\\nTNT\\nCD\\nFORM\\n)\\nFROM 22\\nTO\\n23\\nTNT\\n)\\nFig. 1. FSC input to PET with combined information from tokenizer, PoS tagger and\\nconcurrent SProUT citation string item for input fragment “Resnik and Smith (2003)\\nextract ...”\\nFurthermore, a new way of generic lexical instantiation has been introduced\\nwith token feature structures and chart mapping. In this new setup, the parser\\ntries to instantiate all generic lexical entries for each word. Upon lexical instan-\\ntiation, the token feature is uniﬁed into a designated path of the lexical entry.'),\n",
       " Document(metadata={}, page_content='tiation, the token feature is uniﬁed into a designated path of the lexical entry.\\nOnly if this uniﬁcation succeeds, the lexical item is instantiated. In order to con-\\ntrol the instantiation of generic lexical entries, the token feature structures are\\nappropriately constrained in the generic lexical entry, for instance by requiring\\nthat a generic verbal entry is only applicable for token feature structures where\\nthe highest ranked part-of-speech tag is a verb.\\n2.5\\nIncreased Processing Speed and Coverage through Chart\\nPruning\\nThe use of statistical models for result selection is well established for parsing\\nwith PET and ERG. We use a discriminative maximum entropy model based on'),\n",
       " Document(metadata={}, page_content='with PET and ERG. We use a discriminative maximum entropy model based on\\nWeScience data [9] to compute the best parse results. Recently, [7] described the\\nuse of a generative model to increase eﬃciency by shaping the search space of\\nthe parser towards the more likely constituents and pruning very unlikely ones.\\nThis method not only results in lower parse times, but also in slightly better\\ncoverage, since sentences which could not be parsed due to timeouts now ﬁt into\\nthe given time bounds.\\nThe generative model is in fact a probabilistic context-free grammar (PCFG)\\ncomputed from the same tree banks as the discriminative model. The parser in'),\n",
       " Document(metadata={}, page_content='computed from the same tree banks as the discriminative model. The parser in\\nPET is a straightforward bottom-up chart parser with agenda, which makes it\\nAdvances in Deep Parsing of Scholarly Paper Content\\n7\\ninput\\nsentence\\ninput\\nsentence\\ntokenizer\\ntokenizer\\nPoS tagger\\nPoS tagger\\nnamed entity\\nrecognizer\\nnamed entity\\nrecognizer\\nPET parser\\nPET parser\\nPET XML\\ninput chart\\nPET XML\\ninput chart\\nMRX\\nMRX\\nsemantic tuples\\ndatabase\\nsemantic tuples\\ndatabase\\nsemantic tuples extractor\\nsemantic tuples extractor\\nFig. 2. Heart of Gold workﬂow for hybrid parsing and semantic tuples extraction\\neasy to use a model that has only local dependencies, such as PCFG. What'),\n",
       " Document(metadata={}, page_content='easy to use a model that has only local dependencies, such as PCFG. What\\nis missing is a heuristics to prune unlikely items in a way that has a small\\ncomputation overhead and will retain most of the items that are needed for the\\nglobally best results.\\n[11] did a very thorough comparison of diﬀerent performance optimization\\nstrategies, and among those also a local pruning strategy which is similar to the\\none used by [7]. It restricts the number of items given both their length and start\\npoint in the chart. This is easy to implement and avoids the use of complicated\\nheuristics to compensate the bias that shorter items become over longer chart'),\n",
       " Document(metadata={}, page_content='heuristics to compensate the bias that shorter items become over longer chart\\nitems because of decreasing probability, which leads, without compensation, to\\na breadth-ﬁrst strategy for the whole parse. The number of items per chart cell\\nis restricted to a ﬁxed number to hinder the parser from getting lost in local\\nprobability maxima.\\nThere is an important diﬀerence to the system of [11], namely that their\\nsystem works on a reduced context-free backbone of the grammar and then\\nreconstructs the full results, while PET uses the full HPSG grammar directly,\\n8\\nUlrich Sch¨afer and Bernd Kiefer\\nwith subsumption packing and partial unpacking to achieve a similar eﬀect as'),\n",
       " Document(metadata={}, page_content='8\\nUlrich Sch¨afer and Bernd Kiefer\\nwith subsumption packing and partial unpacking to achieve a similar eﬀect as\\nthe packed chart of a context-free parser.\\nThe local chart pruning results in a measurable speed-up with a negligible\\ndecrease in parsing accuracy; in fact, an increase in f-measure has been observed\\nbecause complicated sentences that had originally failed due to resource restric-\\ntions could now be parsed.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 70\\n 80\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nsentences x 1000\\nmean parse time (CPU s)\\nsentence length −→\\nFig. 3. Distribution of sentence length and mean parse times for mild pruning\\nProcessing Results. In total, we parsed 1,537,801 sentences, of which 57,832'),\n",
       " Document(metadata={}, page_content='Processing Results. In total, we parsed 1,537,801 sentences, of which 57,832\\n(3.8%) could not be parsed because of lexicon errors which are mostly due to\\nOCR artifacts.\\nFigure 3 displays the average parse time of processing with moderate chart\\npruning, together with the mean quadratic error. In addition, it contains the\\ndistribution of input sentences over sentence length. Obviously, the vast majority\\nof sentences has a length up to 60 words maximum.\\nParse time was limited to 60 CPU seconds, and main memory consumption\\nto 4 GB, which was far more than ever needed by the processes. Overall, the\\nparse times only grow mildly due to the many optimization techniques in the'),\n",
       " Document(metadata={}, page_content='parse times only grow mildly due to the many optimization techniques in the\\noriginal system, and also the new chart pruning method. The sentence length\\ndistribution has been integrated into Figure 3 to show that the predominant part\\nof our real-world corpus can be processed using this information-rich method\\nwith very modest parse times.\\nThe large amount of short inputs is at ﬁrst surprising, moreover as most of\\nthese inputs can not be parsed, as can be seen in Figure 5. The explanation\\nAdvances in Deep Parsing of Scholarly Paper Content\\n9\\nis easy: most of these inputs are non-sentences such as headings, enumerations,'),\n",
       " Document(metadata={}, page_content='Advances in Deep Parsing of Scholarly Paper Content\\n9\\nis easy: most of these inputs are non-sentences such as headings, enumerations,\\nfootnotes and such. How we deal with this kind of input will be described in the\\nsection about fragmentary input.\\nAll measurements were carried out on an Intel XEON E5430 2.66GHz cluster\\ncomputer. Except for the parallelization, the used hardware equals a modern\\nstandard desktop PC, which again shows the feasibility of the used method.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nno pruning\\nmax400\\nmax100\\nsentence length −→\\nMean parse time (CPU sec) over sentence length\\nNo pruning\\nMax. 400 passive Max. 100 passive\\nAvg. Parse Time (CPU sec)\\n5.90\\n3.95\\n2.17'),\n",
       " Document(metadata={}, page_content='Mean parse time (CPU sec) over sentence length\\nNo pruning\\nMax. 400 passive Max. 100 passive\\nAvg. Parse Time (CPU sec)\\n5.90\\n3.95\\n2.17\\nUnparsed Sentences\\n433104 (28.2%) 392758 (25.5%)\\n381019 (24.8%)\\nRecall\\n71.8%\\n74.5%\\n75.2%\\nBest Parse Lost\\n5.43%\\n19.7%\\nFig. 4. Comparison of results with diﬀerent chart pruning settings\\nFigure 4 shows the eﬀects of the chart pruning approach using moderate\\nas well as more aggressive pruning. The last row displays the amount of parsed\\nsentences which do not get the best results due to pruning. Note that the increase\\nin parsed sentences is only due to the reduced resource needs through pruning,'),\n",
       " Document(metadata={}, page_content='in parsed sentences is only due to the reduced resource needs through pruning,\\nand that the lexical failures are not contained in the unparsed sentences ﬁgures.\\nFigure 5 shows the amount of unparsed sentences, split into two categories.\\nThe dots represent the sentences that could not be parsed due to time limitations,\\nthe solid lines those that were rejected by the grammar. Not surprisingly, the\\nfraction of sentences hitting the time bound increases noticeably for sentences\\n10\\nUlrich Sch¨afer and Bernd Kiefer\\nlonger that 60 words, but it should be noted that the percentage that can not\\nbe parsed because of grammatical reasons stays almost constant.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 70\\n 80\\n 90'),\n",
       " Document(metadata={}, page_content='be parsed because of grammatical reasons stays almost constant.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 70\\n 80\\n 90\\n 100\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nno pruning\\nno pruning timeout\\ncp400\\ncp400 timeout\\ncp100\\ncp100 timeout\\nsentence length −→\\nFig. 5. Percentage of unparsed sentences over sentence length\\nFor sentences with less than 40 words, aggressive chart pruning loses parses\\n(around 0.8%) that the mild pruning still does successfully, because edges needed\\nfor a full parse are pruned from the chart. In toto, the aggressive pruning gets\\nmore readings because it greatly improves recall on the longer sentences, but\\nsome are lost in the important middle range, which is also why we use the'),\n",
       " Document(metadata={}, page_content='some are lost in the important middle range, which is also why we use the\\nresults from the mild pruning for the extraction of the semantics. An advanced\\nsystem could adapt pruning to the input length, or try to come up with better\\nlocal models that minimize the loss of useful subconstituents.\\nWe also compared the (absolute) scores of the discriminative model for the\\ntwo variants. While the method without chart pruning always ﬁnds the best\\nparse, this is not true for the pruned chart. The result is displayed in the fourth\\nrow of the table in Figure 4. Since the scores of the maximum entropy model\\nare not probabilities, we can not give meaningful numbers on the loss of quality,'),\n",
       " Document(metadata={}, page_content='are not probabilities, we can not give meaningful numbers on the loss of quality,\\nbut a rough comparison of the scores suggests that in most cases the penalty is\\nminor.\\nFragmentary Input. There are several alternatives to deal with input like\\nheadings and footnotes, one to identify and handle them in a preprocessing\\nstep, another to use a special root condition in the deep analysis component\\nthat is able to combine phrases with well-deﬁned properties for inputs where no\\nspanning result could be found.\\nAdvances in Deep Parsing of Scholarly Paper Content\\n11\\nWe employed the second method, which has the advantage that it handles a'),\n",
       " Document(metadata={}, page_content='Advances in Deep Parsing of Scholarly Paper Content\\n11\\nWe employed the second method, which has the advantage that it handles a\\nlarger range of phenomena in a homogeneous way. Figure 6 shows the change in\\npercentage of unparsed and timed out inputs for the mild pruning method with\\nand without the root condition combining fragments.\\n 0\\n 10\\n 20\\n 30\\n 40\\n 50\\n 60\\n 70\\n 80\\n 90\\n 100\\n 0\\n 20\\n 40\\n 60\\n 80\\n 100\\nstrict\\nstrict timeout\\nstrict+fragments\\nstrict+fragments timeout\\nsentence length −→\\nFig. 6. Unparsed and timed out sentences with and without fragment combination\\nAs Figure 6 shows nicely, this changes the curve for unparsed sentences to-'),\n",
       " Document(metadata={}, page_content='As Figure 6 shows nicely, this changes the curve for unparsed sentences to-\\nwards more expected characteristics and removes the uncommonly high percent-\\nage of short sentences for which no parse can be found.\\nTogether with the parses for fragmented input, we get a recall (sentences\\nwith at least one parse) over the whole corpus of 85.9% (1,321,336 sentences),\\nwithout a signiﬁcant change for any of the other numbers.\\n2.6\\nParser Output\\nIn contrast to shallow parsers, the ERG not only handles detailed syntactic\\nanalyses of phrases, compounds, coordination, negation and other linguistic phe-\\nnomena that are important for extracting relations, but also generates a formal'),\n",
       " Document(metadata={}, page_content='nomena that are important for extracting relations, but also generates a formal\\nsemantic representation of the meaning of the input sentence in the MRS repre-\\nsentation format (Minimal Recursion Semantics; [6]). It is comparable to a ﬁrst\\norder logic form. It consists of so-called elementary predications for each token\\nand larger constituents, connected via argument positions and variables/labels,\\nfrom which the predicate-argument structure can be derived (example in Fig-\\nure 7).\\n12\\nUlrich Sch¨afer and Bernd Kiefer\\n⟨h1,\\nh3:udef q(x5{PERS 3, NUM sg}, h4, h6),\\nh7: semantic a 1(e8{SF prop, TENSE untensed, MOOD indicative}, x5),\\nh7: similarity n to(x5, i9),'),\n",
       " Document(metadata={}, page_content='⟨h1,\\nh3:udef q(x5{PERS 3, NUM sg}, h4, h6),\\nh7: semantic a 1(e8{SF prop, TENSE untensed, MOOD indicative}, x5),\\nh7: similarity n to(x5, i9),\\nh10: measure v 1(e2{SF prop, TENSE pres, MOOD indicative, PROG -, PERF -}, p11, x5),\\nh10:parg d(e12{SF prop}, e2, x5),\\nh10: in p(e13{SF prop, TENSE untensed, MOOD indicative}, e2, x14{PERS 3, NUM pl, IND +}),\\nh15:udef q(x14, h16, h17),\\nh18: term n of(x14, x19{PERS 3, NUM pl}),\\nh20:udef q(x19, h21, h22),\\nh23:compound(e25{SF prop, TENSE untensed, MOOD indicative, PROG -, PERF -}, x19, x24),\\nh26:udef q(x24, h27, h28),\\nh29: similar a to(e30{SF prop, TENSE untensed, MOOD indicative}, x24),\\nh29:comp(e32{SF prop}, e30, u31),\\nh29: word n of(x24, i33),'),\n",
       " Document(metadata={}, page_content='h29: similar a to(e30{SF prop, TENSE untensed, MOOD indicative}, x24),\\nh29:comp(e32{SF prop}, e30, u31),\\nh29: word n of(x24, i33),\\nh23: context n 1(x19)\\n{ h27 =q h29, h21 =q h23, h16 =q h18, h4 =q h7 } ⟩\\nFig. 7. Sample MRS for the sentence “Semantic similarity is measured in terms of\\nsimilar word contexts.”\\nAs in previous work [18] and because of the increased parsing recall, we again\\nopt for precision and only use results from the deep parser instead of extending\\nthe hybrid workﬂow (Figure 2) in such a way that a shallow parser with less\\ndetailed analyses is used as fall-back in case deep parsing fails (as done in an\\nintermediate system, [17]).\\n3'),\n",
       " Document(metadata={}, page_content='detailed analyses is used as fall-back in case deep parsing fails (as done in an\\nintermediate system, [17]).\\n3\\nApplication: Semantic Search Based on Extracted\\nPredicate-Argument Structure\\nThe idea of the semantic search application is to use the sentence-wise semantic\\nrepresentations generated oﬄine by the deep parser. From its output, a normal-\\nized predicate-argument structure is extracted that is stored in a search index.\\nThe main motivation is at least partial abstraction from syntactic variants. Thus,\\nthe extraction process includes dividing sentences with coordination into inde-\\npendent structures, and using the semantic subject and object in both active'),\n",
       " Document(metadata={}, page_content='pendent structures, and using the semantic subject and object in both active\\nand passive sentence construction independently of the syntactic realization.\\nThe user interface for this application is simple. Instead of a single search text\\ninput ﬁeld, the user will see three: one for subject, one for predicate and another\\none for further objects. This is easy to understand also for non-linguists, and\\nﬁelds may be left emtpy to match anything. In the current version, the search\\ninterface supports the use of synsets of predicates only.\\n3.1\\nExtracting Predicate-Argument Structure from MRS\\nThe MRS representations resulting from hybrid parsing are relatively close to'),\n",
       " Document(metadata={}, page_content='3.1\\nExtracting Predicate-Argument Structure from MRS\\nThe MRS representations resulting from hybrid parsing are relatively close to\\nlinguistic structures and contain more detailed information than a user would\\nAdvances in Deep Parsing of Scholarly Paper Content\\n13\\nlike to query and search for. Therefore, an additional extraction and abstraction\\nstep is necessary before storing the semantic structures in the search index.\\nThe format we devised for this purpose we call semantic tuples, a blend of\\ntriples and quintuples, as we store quintuples (subject, predicate, direct object,\\nother complements and adjunct), but to ease search term input for the user, only'),\n",
       " Document(metadata={}, page_content='other complements and adjunct), but to ease search term input for the user, only\\ndistinguish between a triple of subject, predicate and any other objects in the\\nquery structure.\\nThe algorithm to generate the semantic tuples ﬁrst performs an intermedi-\\nate transformation into isomorphic, serializable Java objects that can be made\\npersistent. On these objects, eﬃcient graph manipulation resulting in extracted\\nsemantic tuples can take place. Handling of coordination has been implemented\\nby generating multiple tuples. Passive constructions are elegantly handled by\\nthe grammar itself and lead to identical semantic tuples regardless of active or\\npassive formulation of the same proposition.'),\n",
       " Document(metadata={}, page_content='the grammar itself and lead to identical semantic tuples regardless of active or\\npassive formulation of the same proposition.\\nDue to semantic ambiguity, the deep parser may return more than one reading\\nper sentence. Currently up to three readings are considered (the most probable\\nones according to the treebank-trained parse ranking model), and semantic tu-\\nples are generated for each reading respectively. Multiple readings may collapse\\ninto the same semantic tuple structure, in which case only a single one is stored\\nin the database. Otherwise, a voting mechanism based on rank and number of\\nisomorphic semantic tuples decides for the best selection.'),\n",
       " Document(metadata={}, page_content='in the database. Otherwise, a voting mechanism based on rank and number of\\nisomorphic semantic tuples decides for the best selection.\\nThe following sentence includes the semantic tuple structure (in brackets):\\n“[We]SUBJ [evaluate]PRED [the eﬃciency and performance]DOBJ\\n[against the corpus]ADJU.”\\nIn this example, the conjunction relation connects two noun phrases, both of\\nthem being DOBJ; therefore, no new semantic tuple is necessary. However, we\\ndecided to distinguish cases where conjunction connects two sentences or verb\\nphrases. In such cases, semantic tuples are generated for each part respectively.\\nThe following example shows an AND relation. Conjunction relations may also'),\n",
       " Document(metadata={}, page_content='The following example shows an AND relation. Conjunction relations may also\\nbe realized in diﬀerent lexemes, e.g. and, but, or, as well as, etc.\\nFor the sentence “The system automatically extracts pairs of syntactic units\\nfrom a text and assigns a semantic relation to each pair.”, two semantic tuples\\nare generated separately with their own PRED, DOBJ and OCMP:\\n“[The system]SUBJ [extracts]PRED [pairs of syntactic units]DOBJ\\n[from a text]OCMP [automatically]ADJU.”\\nand\\n“[The system]SUBJ [assigns]PRED [a semantic relation]DOBJ\\n[to each pair]OCMP [automatically]ADJU.”\\nIn passive sentences, the syntactic subject becomes the semantic object and\\nvice versa:'),\n",
       " Document(metadata={}, page_content='[to each pair]OCMP [automatically]ADJU.”\\nIn passive sentences, the syntactic subject becomes the semantic object and\\nvice versa:\\n“[Unseen input]DOBJ [was classiﬁed]PRED [by trained neural networks\\nwith varying error rates depending corpus type]SUBJ.”\\n14\\nUlrich Sch¨afer and Bernd Kiefer\\n3.2\\nFilling the Search Index\\nFor each sentence, the semantic tuple structure together with associated char-\\nacter span information relative to the sentence start is then stored in an Apache\\nSolr5 search index. It also contains metainformation on page number, sentence\\nnumber, oﬀset and document ID.\\nIn case a named entity is identiﬁed by the named entity recognizer, further in-'),\n",
       " Document(metadata={}, page_content='number, oﬀset and document ID.\\nIn case a named entity is identiﬁed by the named entity recognizer, further in-\\nformation on span and type (such as location, person, time) of the item is stored.\\nThis named entity type information is used to identify the answer candidate type\\nin an additional question answering interface we will not further describe in this\\npaper. The following snippet from Solr input for a single sentence may give an\\nimpression of the underlying index schema.\\n<doc>\\n<field name=\"aclaid\">N07-1043</field>\\n<field name=\"page\">2</field>\\n<field name=\"sentno\">56</field>\\n<field name=\"prefix\">N07-1043-s56-p2</field>\\n<field name=\"offset\">353</field>\\n<field name=\"qgen\">PET</field>'),\n",
       " Document(metadata={}, page_content='<field name=\"sentno\">56</field>\\n<field name=\"prefix\">N07-1043-s56-p2</field>\\n<field name=\"offset\">353</field>\\n<field name=\"qgen\">PET</field>\\n<field name=\"sentence\">Sahami et al., (2006) measure semantic\\nsimilarity between two queries using the snippets returned\\nfor those queries by a search engine.</field>\\n<field name=\"subj\">Sahami 2006 et al.</field>\\n<field name=\"subj_start\">0</field>\\n<field name=\"subj_end\">12</field>\\n<field name=\"pred\">measure</field>\\n<field name=\"pred_start\">22</field>\\n<field name=\"pred_end\">28</field>\\n<field name=\"dobj\">semantic similarity</field>\\n<field name=\"dobj_start\">30</field>\\n<field name=\"dobj_end\">48</field>'),\n",
       " Document(metadata={}, page_content='<field name=\"dobj\">semantic similarity</field>\\n<field name=\"dobj_start\">30</field>\\n<field name=\"dobj_end\">48</field>\\n<field name=\"ocmp\">between two queries using the snippets\\nreturned for those queries by a search engine</field>\\n<field name=\"ocmp_start\">0</field>\\n<field name=\"ocmp_end\">133</field>\\n<field name=\"ner_types\">citation ne-term ne-term </field>\\n<field name=\"ner_cstart\">0 30 121 </field>\\n<field name=\"ner_cend\">20 48 133 </field>\\n<field name=\"ner_surface\">\"Sahami et al., (2006)\"\\n\"semantic similarity\"\\n\"search engine\" </field>\\n</doc>\\nTo sum up the overall oﬄine analysis for search index generation, Figure 8\\ndepicts the oﬄine NLP and semantic tuple extraction workﬂow.'),\n",
       " Document(metadata={}, page_content='</doc>\\nTo sum up the overall oﬄine analysis for search index generation, Figure 8\\ndepicts the oﬄine NLP and semantic tuple extraction workﬂow.\\n5 http://lucene.apache.org/solr\\nAdvances in Deep Parsing of Scholarly Paper Content\\n15\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\nsemantic tuples\\ndatabase\\nsemantic tuples\\ndatabase\\n● text cleaning\\n● XML encoding\\n● text cleaning\\n● XML encoding\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\n Heart \\n of Gold\\nNLP grid with \\nJTok, TnT, \\nSProUT, PET\\nsemantic tuples\\nextraction\\nsemantic tuples\\nextraction\\n+\\n+\\nscholarly papers\\n2 GB PDF\\nOCR-based\\nPDF-to-text\\nextraction \\nOCR-based\\nPDF-to-text\\nextraction \\nHeart \\nof Gold\\nHeart \\nof Gold'),\n",
       " Document(metadata={}, page_content='extraction\\n+\\n+\\nscholarly papers\\n2 GB PDF\\nOCR-based\\nPDF-to-text\\nextraction \\nOCR-based\\nPDF-to-text\\nextraction \\nHeart \\nof Gold\\nHeart \\nof Gold\\nAutomatic\\nNLP XML\\nannotation\\n1 GB Apache Solr Blob (approx. 1.5 million sentences)\\nFig. 8. Grid-based hybrid parsing of the scientiﬁc paper corpus\\n3.3\\nQuery Interface\\nAs depicted in Figure 9, the user interface for semantic paper search contains\\nthree text ﬁelds where the user can input subject, predicate and all remaining\\nstructures (rest). The latter is combined to ease input (otherwise users would\\nbecome worried about what to put in OCMP or ADJU) and will be expanded\\nto a disjunctive Solr/Lucene query expression.\\nFig. 9. Simple query interface'),\n",
       " Document(metadata={}, page_content='become worried about what to put in OCMP or ADJU) and will be expanded\\nto a disjunctive Solr/Lucene query expression.\\nFig. 9. Simple query interface\\nTo give an example, a semantic tuple search expression with input to ﬁeld\\nsubject=*, input to ﬁeld predicate=‘measure’, and input to ﬁeld rest=‘semantic\\nsimilarity’ is translated into an Apache Solr query\\npred:measure +(dobj:\"semantic similarity\"\\n16\\nUlrich Sch¨afer and Bernd Kiefer\\nOR ocmp:\"semantic similarity\"\\nOR adju:\"semantic similarity\")\\nIn case WordNet synset [10] expansion is enabled, measure is replaced by\\n(measure OR evaluate OR quantify OR value OR assess OR valuate).'),\n",
       " Document(metadata={}, page_content='In case WordNet synset [10] expansion is enabled, measure is replaced by\\n(measure OR evaluate OR quantify OR value OR assess OR valuate).\\nIt is planned to also allow for synonym search in the SUBJ and REST ﬁeld.\\nHere, domain ontology information as well as automatically identiﬁed similar\\n(multi-word) terms could be used to expand the query.\\nSearch Results for * “measure” “semantic similarity”\\n– N07-1043: Sahami et al., (2006) [measure]PRED [semantic similarity]DOBJ\\nbetween two queries using the snippets returned for those queries by a\\nsearch engine.\\n– W04-0106: [Semantic similarity]DOBJ [is measured]PRED in terms of sim-\\nilar word contexts.'),\n",
       " Document(metadata={}, page_content='search engine.\\n– W04-0106: [Semantic similarity]DOBJ [is measured]PRED in terms of sim-\\nilar word contexts.\\n– N07-1044: [The semantic similarity]DOBJ between neighbors and senses [is\\nmeasured]PRED using a manually crafted taxonomy such as WordNet (see\\nBudanitsky and Hirst 2001 for an overview of WordNet-based similarity\\nmeasures).\\n– P08-1028: We [assessed]PRED [a wide range of semantic similarity\\nmeasures]DOBJ using the WordNet similarity package (Pedersen et al.,\\n2004).\\n– W06-3802:\\nUsing\\nWordNet,\\nwe\\n[can\\nmeasure]PRED\\n[the\\nsemantic\\nsimilarity]DOBJ or relatedness between a pair of concepts (or word senses),\\nand by extension, between a pair of sentences.\\n– W06-1659:\\nUsing\\nWordNet,\\nwe\\n[can'),\n",
       " Document(metadata={}, page_content='and by extension, between a pair of sentences.\\n– W06-1659:\\nUsing\\nWordNet,\\nwe\\n[can\\nmeasure]PRED\\n[the\\nsemantic\\nsimilarity]DOBJ or relatedness between a pair of concepts (or word senses),\\nand by extension, between a pair of sentences.\\n– W05-1203: For entailment identiﬁcation, since this is a directional relation,\\nwe [only measure]PRED [the semantic similarity]DOBJ with respect to the\\nhypothesis (the text that is entailed).\\n– W06-1104: We [measured]PRED [semantic relat-edness instead of semantic\\nsimilarity]DOBJ.\\n– P06-1112:\\n3.\\n[The\\nsemantic\\nsimilarity\\nSemSim(h\\n,\\nh\\n)]DOBJ\\n[is\\nmeasured]PRED using Word-Net and eXtended WordNet.\\n. . .'),\n",
       " Document(metadata={}, page_content='similarity]DOBJ.\\n– P06-1112:\\n3.\\n[The\\nsemantic\\nsimilarity\\nSemSim(h\\n,\\nh\\n)]DOBJ\\n[is\\nmeasured]PRED using Word-Net and eXtended WordNet.\\n. . .\\nFig. 10. The ﬁrst matching sentences in the ACL Anthology subset 2002-2008 with\\nrecognized variation in predicate synsets (assess, measure, evaluate) and passive con-\\nstructions\\nThe result is then a list of sentence snippets (Figure 10). By clicking on a\\nhyperlink underlying the snippet text, the original PDF is opened. By using\\nthe information on page and sentence text/oﬀset in the Apache Solr answer,\\nthe result sentence is highlighted as shown in Figure 11. This helps to quickly'),\n",
       " Document(metadata={}, page_content='the result sentence is highlighted as shown in Figure 11. This helps to quickly\\nidentify relevance of the answer by looking at context in the original layout.\\nAdvances in Deep Parsing of Scholarly Paper Content\\n17\\nFig. 11. First result sentence (from N07-1043) highlighted in original PDF\\n4\\nRelated Work\\nUsing HPSG combined with shallow domain-speciﬁc modeling for high-precision\\nanalysis of scientiﬁc texts is an emerging research area. Another ERG-based\\napproach to relation and information extraction from scientiﬁc texts is SciBorg\\n[13]. SciBorg mainly deals with chemistry research papers and handles domain-\\nspeciﬁc phenomena with a specialized named entity recognizer. It relies on a'),\n",
       " Document(metadata={}, page_content='speciﬁc phenomena with a specialized named entity recognizer. It relies on a\\nshallow parser as robustness fall-back for MRS generation.\\nOther groups use less elaborated and ﬁne-grained HPSG grammars than\\nERG. [11] report on large-scale parsing of MEDLINE articles (1.4 billion words)\\nwith such a simpliﬁed grammar.\\n[14] use shallow dependency structure and results from HPSG parsing for\\nextracting protein-protein interactions (PPI) from research papers. The same\\ngroup has also worked on medical texts: MEDIE6 is a semantic search engine to\\nretrieve biomedical correlations from MEDLINE articles.\\nWhat distinguishes our approach from those, besides concentration on a dif-'),\n",
       " Document(metadata={}, page_content='retrieve biomedical correlations from MEDLINE articles.\\nWhat distinguishes our approach from those, besides concentration on a dif-\\nferent scientiﬁc area, is the focus on and use of ontology information as integrated\\npart of linguistic analysis, use of the most comprehensive and elaborated HPSG\\ngrammar for English (ERG), and the interactive user interface (Scientist’s Work-\\nbench application; [17]) and editor [18].\\n5\\nConclusion and Future Work\\nWe have presented our recent advances in full, robust parsing of scientiﬁc papers\\ntexts. By careful preprocessing and novel approaches to eﬃcient parsing of long\\nsentences, we could improve coverage from 65 to more than 85%.'),\n",
       " Document(metadata={}, page_content='texts. By careful preprocessing and novel approaches to eﬃcient parsing of long\\nsentences, we could improve coverage from 65 to more than 85%.\\nThe semantic search application built on the semantic representations gen-\\nerated by the deep grammar is a useful extension to cope with synonyms and\\nsyntactic variation when querying full scientiﬁc publication content. The search\\nspace, initially expanded by adding synonymns, can be again constrained by\\nimposing semantic subject-predicate-object structure in the query.\\n6 http://www-tsujii.is.s.u-tokyo.ac.jp/medie/\\n18\\nUlrich Sch¨afer and Bernd Kiefer\\nFurther research goals are improving robustness of the NLP tool chain. We'),\n",
       " Document(metadata={}, page_content='18\\nUlrich Sch¨afer and Bernd Kiefer\\nFurther research goals are improving robustness of the NLP tool chain. We\\nare also working on generic techniques to automatically extract and use sci-\\nence domain information from the underlying paper corpus to improve targeted\\nsearch. Three main tasks in our focus are coreference resolution, term extraction\\nand ontology extraction viz. population. The idea is that these techniques, in a\\nﬁrst step gained independently from the text corpus or partially from NLP anal-\\nyses of it, will beneﬁt from each other and can be used to build more reliable\\nand precise resources and tools in a bootstrapping process.'),\n",
       " Document(metadata={}, page_content='yses of it, will beneﬁt from each other and can be used to build more reliable\\nand precise resources and tools in a bootstrapping process.\\nHandling of negation, modal constructions, subclauses etc. also fall into the\\ncategory deep NLP can handle, but this will be addressed in the future as it also\\nrequires lexico-semantic information of verbs etc. in the extraction process. It\\nwill deﬁnitely be an important extension helping to improve precision in search.\\nThe semantic search application is part of the Scientist’s workbench and is\\ncomplemented by a visualization and navigation tool TeeCeeGeeNav [16] that\\nsupports scientists in quickly getting an overview of a (new) research ﬁeld by'),\n",
       " Document(metadata={}, page_content='supports scientists in quickly getting an overview of a (new) research ﬁeld by\\nbrowsing through a typed citation graph computed from the scientiﬁc paper\\ncorpus. The citation classiﬁcation with categories such as use or refutation of\\nresults of the cited paper currently builds on shallow NLP (such as PoS tagging)\\nonly. In the future, deep semantics could help too further improve this diﬃcult\\nclassiﬁcation task.\\nAcknowledgments\\nThe authors would like to thank Peter Adolphs, Dan Flickinger and Stephan\\nOepen for their support and numerous fruitful discussions. We would also like\\nto thank Yi Zhang and Bart Cramer for the implementation of chart pruning in'),\n",
       " Document(metadata={}, page_content='to thank Yi Zhang and Bart Cramer for the implementation of chart pruning in\\nPET and their support to put it into use. The work described in this paper has\\nbeen carried out in the context of the project TAKE (Technologies for Advanced\\nKnowledge Extraction), funded under contract 01IW08003 by the German Fed-\\neral Ministry of Education and Research, and in the context of the world-wide\\nDELPH-IN collaboration7.\\nReferences\\n1. Adolphs, P., Oepen, S., Callmeier, U., Crysmann, B., Flickinger, D., Kiefer, B.:\\nSome ﬁne points of hybrid natural language parsing. In: Proc. of LREC. pp. 1380–\\n1387. Marrakesh, Morocco (2008)'),\n",
       " Document(metadata={}, page_content='Some ﬁne points of hybrid natural language parsing. In: Proc. of LREC. pp. 1380–\\n1387. Marrakesh, Morocco (2008)\\n2. Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M., Kan, M.Y., Lee, D., Powley,\\nB., Radev, D., Tan, Y.F.: The ACL anthology reference corpus: A reference dataset\\nfor bibliographic research. In: Proc. of LREC. pp. 1755–1759. Marrakesh, Morocco\\n(2008)\\n3. Brants, T.: TnT – A Statistical Part-of-Speech Tagger. In: Proc. of ANLP-2000.\\npp. 224–231. Seattle, WA (2000)\\n7 DEep Linguistic Processing with Hpsg INitiative; http://www.delph-in.net\\nAdvances in Deep Parsing of Scholarly Paper Content\\n19\\n4. Callmeier, U.: PET – A platform for experimentation with eﬃcient HPSG process-'),\n",
       " Document(metadata={}, page_content='Advances in Deep Parsing of Scholarly Paper Content\\n19\\n4. Callmeier, U.: PET – A platform for experimentation with eﬃcient HPSG process-\\ning techniques. Natural Language Engineering 6(1), 99–108 (2000)\\n5. Copestake, A., Flickinger, D.: An open-source grammar development environment\\nand broad-coverage English grammar using HPSG. In: Proc. of LREC. pp. 591–\\n598. Athens, Greece (2000)\\n6. Copestake, A., Flickinger, D., Sag, I.A., Pollard, C.: Minimal recursion semantics:\\nan introduction. Research on Language and Computation 3(2–3), 281–332 (2005)\\n7. Cramer, B., Zhang, Y.: Constraining robust constructions for broad-coverage pars-'),\n",
       " Document(metadata={}, page_content='7. Cramer, B., Zhang, Y.: Constraining robust constructions for broad-coverage pars-\\ning with precision grammars. In: Proc. of COLING. pp. 223–231. Beijing, China\\n(2010)\\n8. Dro˙zd˙zy´nski, W., Krieger, H.U., Piskorski, J., Sch¨afer, U., Xu, F.: Shallow process-\\ning with uniﬁcation and typed feature structures – foundations and applications.\\nK¨unstliche Intelligenz 2004(1), 17–23 (2004)\\n9. Flickinger, D., Oepen, S., Ytrestøl, G.: WikiWoods: Syntacto-semantic annotation\\nfor English Wikipedia. In: Proc. of LREC. pp. 1665–1671. Valletta, Malta (2010)\\n10. Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K.J.: Five papers on'),\n",
       " Document(metadata={}, page_content='10. Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K.J.: Five papers on\\nWordNet. Tech. rep., Cognitive Science Laboratory, Princeton University (1993)\\n11. Ninomiya, T., Tsuruoka, Y., Miyao, Y., Taura, K., Tsujii, J.: Fast and scalable\\nHPSG parsing. Traitement automatique des langues (TAL) 46(2) (2006)\\n12. Pollard, C., Sag, I.A.: Head-Driven Phrase Structure Grammar. Studies in Con-\\ntemporary Linguistics, University of Chicago Press, Chicago (1994)\\n13. Rupp, C., Copestake, A., Corbett, P., Waldron, B.: Integrating general-purpose\\nand domain-speciﬁc components in the analysis of scientiﬁc text. In: Proc. of the\\nUK e-Science Programme All Hands Meeting 2007. Nottingham, UK (2007)'),\n",
       " Document(metadata={}, page_content='UK e-Science Programme All Hands Meeting 2007. Nottingham, UK (2007)\\n14. Sætre, R., Kenji, S., Tsujii, J.: Syntactic features for protein-protein interaction\\nextraction. In: Baker, C.J., Jian, S. (eds.) Short Paper Proc. of the 2nd Int. Symp.\\non Languages in Biology and Medicine (LBM 2007). pp. 6.1–6.14. Singapore (2008)\\n15. Sch¨afer, U.: Middleware for creating and combining multi-dimensional NLP\\nmarkup. In: Proc. of the EACL-2006 Workshop on Multi-dimensional Markup in\\nNatural Language Processing. pp. 81–84. Trento, Italy (2006)\\n16. Sch¨afer, U., Kasterka, U.: Scientiﬁc authoring support: A tool to navigate in typed'),\n",
       " Document(metadata={}, page_content='Natural Language Processing. pp. 81–84. Trento, Italy (2006)\\n16. Sch¨afer, U., Kasterka, U.: Scientiﬁc authoring support: A tool to navigate in typed\\ncitation graphs. In: Proc. of the NAACL-HLT 2010 Workshop on Computational\\nLinguistics and Writing. pp. 7–14. Los Angeles, CA (2010)\\n17. Sch¨afer, U., Spurk, C.: TAKE Scientist’s Workbench: Semantic search and citation-\\nbased visual navigation in scholar papers. In: Proc. of the 4th IEEE Int. Conference\\non Semantic Computing (ICSC-2010). pp. 317–324. Pittsburgh, PA (2010)\\n18. Sch¨afer, U., Uszkoreit, H., Federmann, C., Marek, T., Zhang, Y.: Extracting and\\nquerying relations in scientiﬁc papers. In: Proc. of the 31st Annual German Confer-'),\n",
       " Document(metadata={}, page_content='querying relations in scientiﬁc papers. In: Proc. of the 31st Annual German Confer-\\nence on Artiﬁcial Intelligence (KI-2008). pp. 127–134. Springer LNAI 5243 (2008)')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d066a563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': 'Downloaded PDF successfully.',\n",
       " 'data': {'pdf_url': 'https://papers.nips.cc/paper_files/paper/2020/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf',\n",
       "  'path': 'temp/pdfs/test.pdf',\n",
       "  'bytes': 13026651}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from research_trend_analyzer_light.utils.paper_process import download_pdf, parse_pdf\n",
    "\n",
    "download_pdf(\"https://papers.nips.cc/paper_files/paper/2020/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf\", paper_path = \"temp/pdfs/test.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9faf5296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n",
      "1 []\n",
      "2 []\n",
      "3 []\n",
      "4 []\n",
      "5 []\n",
      "6 []\n",
      "7 []\n",
      "8 []\n",
      "9 [Rect(108.0, 464.3731384277344, 163.54383850097656, 476.3283386230469)]\n",
      "10 []\n",
      "11 []\n",
      "12 []\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "paper_path = \"temp/pdfs/test.pdf\"\n",
    "with pymupdf.open(paper_path) as doc:\n",
    "    for i, page in enumerate(doc):\n",
    "        print(i, page.search_for(\"References\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c75d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Tuple, Dict\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "\n",
    "# --- Section title patterns (extend as needed) ---\n",
    "ACK_TITLES = [r\"acknowledg(e)?ment(s)?\"]  # acknowledgment / acknowledgement / acknowledgments\n",
    "REF_TITLES = [r\"references?\", r\"bibliography\", r\"works\\s+cited\"]\n",
    "\n",
    "ACK_RE = re.compile(rf\"^\\s*({'|'.join(ACK_TITLES)})\\s*$\", re.I)\n",
    "REF_RE = re.compile(rf\"^\\s*({'|'.join(REF_TITLES)})\\s*$\", re.I)\n",
    "\n",
    "# Generic \"looks like a heading\" (not strictly required here, but useful if you extend logic)\n",
    "GEN_HEADING_RE = re.compile(r\"^[A-Z0-9][A-Z0-9\\s\\-:&]{2,}$\")\n",
    "\n",
    "\n",
    "def _first_anchor_on_page(page: fitz.Page, title_re: re.Pattern, title_variants) -> Optional[Tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Try to find a heading match on a page; return (y0, matched_text) if found.\n",
    "    Strategy:\n",
    "      1) font-aware scan via get_text('dict') to match exact heading lines\n",
    "      2) fallback to search_for() for string variants (returns Rects)\n",
    "    \"\"\"\n",
    "    info = page.get_text(\"dict\")\n",
    "    best = None\n",
    "    best_size = -1.0\n",
    "\n",
    "    for block in info.get(\"blocks\", []):\n",
    "        if block.get(\"type\") != 0:\n",
    "            continue\n",
    "        for line in block.get(\"lines\", []):\n",
    "            text = \"\".join(span.get(\"text\", \"\") for span in line.get(\"spans\", [])).strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            if title_re.match(text):\n",
    "                sizes = [span.get(\"size\", 0) for span in line.get(\"spans\", [])]\n",
    "                avg_size = sum(sizes) / len(sizes) if sizes else 0\n",
    "                y0 = line[\"spans\"][0][\"bbox\"][1]\n",
    "                if avg_size > best_size:\n",
    "                    best = (y0, text)\n",
    "                    best_size = avg_size\n",
    "\n",
    "    if best:\n",
    "        return best\n",
    "\n",
    "    # Fallback: literal search for common variants\n",
    "    for variant in title_variants:\n",
    "        hits = page.search_for(variant)  # returns list[Rect]\n",
    "        if hits:\n",
    "            return (hits[0].y0, variant)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_earliest_anchor(doc: fitz.Document) -> Optional[Tuple[int, float, str]]:\n",
    "    \"\"\"\n",
    "    Scan pages forward to find the earliest of Acknowledgments or References.\n",
    "    Returns (page_index, y_start, label) where label is 'acknowledgments' or 'references'.\n",
    "    \"\"\"\n",
    "    for pno in range(doc.page_count):\n",
    "        page = doc[pno]\n",
    "        # Check ACK\n",
    "        a = _first_anchor_on_page(page, ACK_RE, [\"Acknowledgments\", \"Acknowledgements\", \"Acknowledgment\"])\n",
    "        if a:\n",
    "            y, _ = a\n",
    "            return (pno, y, \"acknowledgments\")\n",
    "        # Check REF\n",
    "        r = _first_anchor_on_page(page, REF_RE, [\"References\", \"Bibliography\", \"Works Cited\"])\n",
    "        if r:\n",
    "            y, _ = r\n",
    "            return (pno, y, \"references\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _page_text_up_to_y(page: fitz.Page, y_limit: float) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from the page strictly ABOVE y_limit, using line-level positions.\n",
    "    \"\"\"\n",
    "    info = page.get_text(\"dict\")\n",
    "    out_lines = []\n",
    "    for block in info.get(\"blocks\", []):\n",
    "        if block.get(\"type\") != 0:\n",
    "            continue\n",
    "        for line in block.get(\"lines\", []):\n",
    "            # take line's first span y0 as line baseline\n",
    "            if not line.get(\"spans\"):\n",
    "                continue\n",
    "            y0 = line[\"spans\"][0][\"bbox\"][1]\n",
    "            if y0 < y_limit - 1e-3:\n",
    "                text = \"\".join(span.get(\"text\", \"\") for span in line.get(\"spans\", [])).rstrip()\n",
    "                if text:\n",
    "                    out_lines.append(text)\n",
    "    return \"\\n\".join(out_lines).strip()\n",
    "\n",
    "\n",
    "def extract_until_ack_or_refs(pdf_path: str, include_anchor_page: bool = True) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract main text from the start of the PDF until the first occurrence of\n",
    "    Acknowledgments/References.\n",
    "\n",
    "    If include_anchor_page=True:\n",
    "        - include the entire page that contains the detected section\n",
    "        - then stop (exclude subsequent pages)\n",
    "\n",
    "    If include_anchor_page=False:\n",
    "        - stop right before the heading on that page (slice by y)\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"text\": <extracted_text or None>,\n",
    "          \"stop_section\": \"acknowledgments\" | \"references\" | None,\n",
    "          \"stop_page\": <int or None>\n",
    "        }\n",
    "    \"\"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        if doc.page_count == 0:\n",
    "            return {\"text\": None, \"stop_section\": None, \"stop_page\": None}\n",
    "\n",
    "        anchor = _find_earliest_anchor(doc)\n",
    "\n",
    "        # No anchor? Return all text\n",
    "        if anchor is None:\n",
    "            full = []\n",
    "            for pno in range(doc.page_count):\n",
    "                full.append(doc[pno].get_text(\"text\"))\n",
    "            return {\"text\": \"\\n\".join(full).strip(), \"stop_section\": None, \"stop_page\": None}\n",
    "\n",
    "        stop_page, y_anchor, label = anchor\n",
    "\n",
    "        # Collect all pages before stop_page\n",
    "        parts = [doc[p].get_text(\"text\") for p in range(stop_page)]\n",
    "\n",
    "        # Handle stop_page\n",
    "        if include_anchor_page:\n",
    "            parts.append(doc[stop_page].get_text(\"text\"))\n",
    "        else:\n",
    "            parts.append(_page_text_up_to_y(doc[stop_page], y_anchor))\n",
    "\n",
    "        return {\n",
    "            \"text\": \"\\n\".join(parts).strip(),\n",
    "            \"stop_section\": label,\n",
    "            \"stop_page\": stop_page,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(paper_path: str):\n",
    "    \"\"\"\n",
    "        Parse text from a local PDF at `paper_path`, and return the text. \n",
    "\n",
    "        Args:\n",
    "            paper_path (str): Absolute or relative path to the PDF file on disk.\n",
    "    \"\"\"\n",
    "    if not isinstance(paper_path, str) or not paper_path.strip():\n",
    "        return make_response(\"error\", \"No paper_path provided.\", None)\n",
    "\n",
    "    try:\n",
    "        with pymupdf.open(paper_path) as doc:\n",
    "            text = \"\".join(page.get_text() for page in doc)\n",
    "            page_count = doc.page_count\n",
    "        return make_response(\"success\", f\"Parsed {page_count} page(s).\",\n",
    "                                         {\"text\": text, \"page_count\": page_count})\n",
    "    except Exception as e:\n",
    "        return make_response(\"error\", f\"Failed to parse PDF: {e}\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "import fitz  # PyMuPDF\n",
    "from utils.helper_func import make_response\n",
    "from research_trend_analyzer_light.utils.paper_process import download_pdf\n",
    "\n",
    "# Compile once\n",
    "ACK_PAT = re.compile(r\"^\\s*acknowledg(e)?ment(s)?\\s*$\", re.IGNORECASE)\n",
    "REF_PAT = re.compile(r\"^\\s*(references?|bibliography|works\\s+cited)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def validate_pdf_path(pdf_path: str) -> None:\n",
    "    if not isinstance(pdf_path, str) or not pdf_path.strip():\n",
    "        raise ValueError(\"pdf_path must be a non-empty string.\")\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        raise ValueError(f\"Not a file: {pdf_path}\")\n",
    "    if not pdf_path.lower().endswith(\".pdf\"):\n",
    "        raise ValueError(\"The provided path does not have a .pdf extension.\")\n",
    "\n",
    "def parse_pdf(\n",
    "    pdf_path: str,\n",
    "    include_anchor_page: bool = False,  # <- default now EXCLUDES text after the heading\n",
    "    early_stop: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract text from the PDF. By default, stops BEFORE the first line that looks like an\n",
    "    'Acknowledgments' or 'References' heading (case-insensitive, whole-line match) and\n",
    "    EXCLUDES that heading and everything after it on that page.\n",
    "\n",
    "    Returns via make_response(status, message, data) where data is the extracted text or None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        validate_pdf_path(pdf_path)\n",
    "    except Exception as e:\n",
    "        return make_response(\"error\", str(e), None)\n",
    "\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            page_count = doc.page_count\n",
    "            if page_count == 0:\n",
    "                return make_response(\"warning\", \"Empty PDF (0 pages).\", None)\n",
    "\n",
    "            # If not stopping early, return the entire document text\n",
    "            if not early_stop:\n",
    "                try:\n",
    "                    text = \"\".join(doc[pno].get_text(\"text\") for pno in range(page_count))\n",
    "                except Exception as e:\n",
    "                    return make_response(\"error\", f\"Failed to extract text: {e}\", None)\n",
    "                return make_response(\"success\", f\"Parsed {page_count} page(s).\", text)\n",
    "\n",
    "            # early_stop=True: stop at first matching heading\n",
    "            parts = []\n",
    "            stop_section: Optional[str] = None\n",
    "            stop_page: Optional[int] = None\n",
    "\n",
    "            for pno in range(page_count):\n",
    "                try:\n",
    "                    page_text = doc[pno].get_text(\"text\")\n",
    "                except Exception as e:\n",
    "                    return make_response(\"error\", f\"Failed to read page {pno}: {e}\", None)\n",
    "\n",
    "                lines = page_text.splitlines()\n",
    "\n",
    "                hit_idx = None\n",
    "                hit_label = None\n",
    "                for i, ln in enumerate(lines):\n",
    "                    if ACK_PAT.match(ln):\n",
    "                        hit_idx, hit_label = i, \"acknowledgments\"\n",
    "                        break\n",
    "                    if REF_PAT.match(ln):\n",
    "                        hit_idx, hit_label = i, \"references\"\n",
    "                        break\n",
    "\n",
    "                if hit_idx is None:\n",
    "                    parts.append(page_text)\n",
    "                    continue\n",
    "\n",
    "                # Found a stop section on this page\n",
    "                stop_section = hit_label\n",
    "                stop_page = pno\n",
    "\n",
    "                if include_anchor_page:\n",
    "                    # Include entire anchor page, then stop\n",
    "                    parts.append(page_text)\n",
    "                else:\n",
    "                    # EXCLUDE the heading and everything after it on this page\n",
    "                    before = \"\\n\".join(lines[:hit_idx]).rstrip()\n",
    "                    parts.append(before)\n",
    "\n",
    "                break  # stop after handling the anchor page\n",
    "\n",
    "            text_out = \"\\n\".join(parts).strip() if parts else None\n",
    "\n",
    "            if stop_section is not None and stop_page is not None:\n",
    "                return make_response(\n",
    "                    \"success\",\n",
    "                    f\"Stopped at {stop_section} on page {stop_page}\",\n",
    "                    text_out,\n",
    "                )\n",
    "            else:\n",
    "                # No stop section found — return everything read\n",
    "                return make_response(\n",
    "                    \"success\",\n",
    "                    \"Reached end of document without finding a stop section.\",\n",
    "                    text_out,\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        return make_response(\"error\", f\"Failed to parse PDF: {e}\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5172bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at acknowledgments on page 8\n",
      "ity from data sets about people while offering formal\n",
      "guarantees of privacy to individuals who contribute data. While these beneﬁts are largely positive,\n",
      "unintended harms could arise due to misapplication of differential privacy or misconceptions about its\n",
      "guarantees. Additionally, difﬁcult social choices are faced when deciding how to balance privacy and\n",
      "utility. Our work addresses a foundational differential privacy task and enables better utility-privacy\n",
      "tradeoffs within this broader context.\n",
      "********************\n",
      "Stopped at references on page 9\n",
      "ess\n",
      "permission using smart contracts and efficient trail of data access [1].\n",
      "While our study observed many gaps between the user’s privacy\n",
      "goals and the data practices by service providers, our findings call\n",
      "for interdisciplinary research to complement different approaches at\n",
      "system and design levels to design privacy inclusive IoT solutions.\n",
      "We hope this paper helps to guide the directions for future research\n",
      "in the domains of privacy and IoT that benefit both end users and\n",
      "businesses involved.\n",
      "********************\n",
      "Stopped at acknowledgments on page 12\n",
      "\n",
      "future work can be divided into two directions: 1) finding the\n",
      "optimal table without modifying the transformation function (in\n",
      "this study, we used the summing) or 2) modifying the\n",
      "transformation function itself. As this paper has shown, finding an\n",
      "analytically optimal solution for the former approach would be\n",
      "very challenging. The latter approach requires considering both\n",
      "the transformation function and the algorithm used to generate\n",
      "the table. We will continue this research in both approaches.\n",
      "********************\n",
      "Stopped at acknowledgments on page 13\n",
      " set of information (e.g., in user surveys). This study\n",
      "would require extensions to our infrastructure, including signing in\n",
      "with different user profiles and collecting web forms on one website\n",
      "for an extended period of time to observe the differences.\n",
      "To facilitate future extensions and applications, we have released\n",
      "the source code [90] and the datasets [91] associated with this study.\n",
      "17\n",
      "\n",
      "Proceedings on Privacy Enhancing Technologies 2025(1)\n",
      "Hao Cui, Rahmadi Trimananda, and Athina Markopoulou\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    \"https://papers.nips.cc/paper_files/paper/2020/file/01e00f2f4bfcbb7505cb641066f2859b-Paper.pdf\",\n",
    "    \"https://arxiv.org/pdf/2410.12336\", \n",
    "    \"https://petsymposium.org/popets/2025/popets-2025-0003.pdf\",\n",
    "    \"https://petsymposium.org/popets/2025/popets-2025-0002.pdf\",\n",
    "         ]\n",
    "\n",
    "for path in paths:\n",
    "    download_pdf(path, paper_path = \"temp/pdfs/test.pdf\")\n",
    "    res = parse_pdf(\"temp/pdfs/test.pdf\")\n",
    "    if res['status'] == 'success':\n",
    "        print(res['message'])\n",
    "        print(res['data'][-500:])\n",
    "\n",
    "    print(\"*\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c073c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mres\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "print(res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e198ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_func import parse_markdown_summary, load_md_file\n",
    "\n",
    "md_file = load_md_file(\"papers/paper_summary/neurips_2020/CH/adversarially_robust_streaming_algorithms_via_differential_privacy.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f1fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = md_file['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63839a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = parse_markdown_summary(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5657c229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Paper Info', 'Brief Summary', 'Detailed Summary'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb35fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Title', 'Authors', 'Affiliations'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['data']['Paper Info'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a185767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Highlight', 'Keywords'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['data']['Brief Summary'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3edcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Ryan Mckenna\", \"Daniel R. Sheldon\"]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb269a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Ryan Mckenna, Daniel R. Sheldon]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['Ryan Mckenna', 'Daniel R. Sheldon']\n",
    "f\"[{', '.join(a)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c46b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "\n",
    "client = arxiv.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78eefc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-22 - SafeSpace: An Integrated Web Application for Digital Safety and Emotional Well-being\n",
      "2025-07-03 - Rethinking Data Protection in the (Generative) Artificial Intelligence Era\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    '(ti:\"usable privacy\" OR abs:\"usable privacy\") '\n",
    "    'AND (cat:cs.CR OR cat:cs.CY OR cat:cs.HC OR cat:cs.LG OR cat:stat.ML)'\n",
    ")\n",
    "\n",
    "# 2) Ask arXiv for the newest submissions first.\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=200,  # pull a larger pool, we’ll filter for recency below if you want\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    ")\n",
    "\n",
    "# 3) Fetch results (newer arxiv library style).\n",
    "client = arxiv.Client(page_size=100, delay_seconds=3, num_retries=3)\n",
    "results = list(client.results(search))\n",
    "\n",
    "# Optional: keep only papers from, say, the last 12 months.\n",
    "cutoff = datetime.now(timezone.utc) - timedelta(days=90)\n",
    "recent = [r for r in results if r.published and r.published >= cutoff]\n",
    "\n",
    "# Demo print\n",
    "for r in recent[:20]:\n",
    "    print(r.published.date(), \"-\", r.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c9afdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8bb9a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/2509.05382v1', updated=datetime.datetime(2025, 9, 5, 1, 1, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 5, 1, 1, 21, tzinfo=datetime.timezone.utc), title=\"User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies\", authors=[arxiv.Result.Author('Jennifer King'), arxiv.Result.Author('Kevin Klyman'), arxiv.Result.Author('Emily Capstick'), arxiv.Result.Author('Tiffany Saade'), arxiv.Result.Author('Victoria Hsieh')], summary=\"Hundreds of millions of people now regularly interact with large language\\nmodels via chatbots. Model developers are eager to acquire new sources of\\nhigh-quality training data as they race to improve model capabilities and win\\nmarket share. This paper analyzes the privacy policies of six U.S. frontier AI\\ndevelopers to understand how they use their users' chats to train models.\\nDrawing primarily on the California Consumer Privacy Act, we develop a novel\\nqualitative coding schema that we apply to each developer's relevant privacy\\npolicies to compare data collection and use practices across the six companies.\\nWe find that all six developers appear to employ their users' chat data to\\ntrain and improve their models by default, and that some retain this data\\nindefinitely. Developers may collect and train on personal information\\ndisclosed in chats, including sensitive information such as biometric and\\nhealth data, as well as files uploaded by users. Four of the six companies we\\nexamined appear to include children's chat data for model training, as well as\\ncustomer data from other products. On the whole, developers' privacy policies\\noften lack essential information about their practices, highlighting the need\\nfor greater transparency and accountability. We address the implications of\\nusers' lack of consent for the use of their chat data for model training, data\\nsecurity issues arising from indefinite chat data retention, and training on\\nchildren's chat data. We conclude by providing recommendations to policymakers\\nand developers to address the data privacy challenges posed by LLM-powered\\nchatbots.\", comment='See additional files for appendices', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.05382v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.05382v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.04358v1', updated=datetime.datetime(2025, 9, 4, 16, 19, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 4, 16, 19, 24, tzinfo=datetime.timezone.utc), title='Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity', authors=[arxiv.Result.Author('Atikkhan Faridkhan Nilgar'), arxiv.Result.Author('Manuel Dietrich'), arxiv.Result.Author('Kristof Van Laerhoven')], summary=\"Social robots are increasingly recognized as valuable supporters in the field\\nof well-being coaching. They can function as independent coaches or provide\\nsupport alongside human coaches, and healthcare professionals. In coaching\\ninteractions, these robots often handle sensitive information shared by users,\\nmaking privacy a relevant issue. Despite this, little is known about the\\nfactors that shape users' privacy perceptions. This research aims to examine\\nthree key factors systematically: (1) the transparency about information usage,\\n(2) the level of specific user control over how the robot uses their\\ninformation, and (3) the robot's behavioral approach - whether it acts\\nproactively or only responds on demand. Our results from an online study (N =\\n200) show that even when users grant the robot general access to personal data,\\nthey additionally expect the ability to explicitly control how that information\\nis interpreted and shared during sessions. Experimental conditions that\\nprovided such control received significantly higher ratings for perceived\\nprivacy appropriateness and trust. Compared to user control, the effects of\\ntransparency and proactivity on privacy appropriateness perception were low,\\nand we found no significant impact. The results suggest that merely informing\\nusers or proactive sharing is insufficient without accompanying user control.\\nThese insights underscore the need for further research on mechanisms that\\nallow users to manage robots' information processing and sharing, especially\\nwhen social robots take on more proactive roles alongside humans.\", comment=None, journal_ref='2025 IEEE International Conference on Robot and Human Interactive\\n  Communication (RO-MAN)', doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.RO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.04358v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.04358v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.03472v1', updated=datetime.datetime(2025, 9, 3, 16, 51, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 3, 16, 51, 26, tzinfo=datetime.timezone.utc), title='DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling', authors=[arxiv.Result.Author('Yubo Gao'), arxiv.Result.Author('Renbo Tu'), arxiv.Result.Author('Gennady Pekhimenko'), arxiv.Result.Author('Nandita Vijaykumar')], summary='Differentially-Private SGD (DP-SGD) is a powerful technique to protect user\\nprivacy when using sensitive data to train neural networks. During training,\\nconverting model weights and activations into low-precision formats, i.e.,\\nquantization, can drastically reduce training times, energy consumption, and\\ncost, and is thus a widely used technique. In this work, we demonstrate that\\nquantization causes significantly higher accuracy degradation in DP-SGD\\ncompared to regular SGD. We observe that this is caused by noise injection in\\nDP-SGD, which amplifies quantization variance, leading to disproportionately\\nlarge accuracy degradation. To address this challenge, we present QPQuant, a\\ndynamic quantization framework that adaptively selects a changing subset of\\nlayers to quantize at each epoch. Our method combines two key ideas that\\neffectively reduce quantization variance: (i) probabilistic sampling of the\\nlayers that rotates which layers are quantized every epoch, and (ii) loss-aware\\nlayer prioritization, which uses a differentially private loss sensitivity\\nestimator to identify layers that can be quantized with minimal impact on model\\nquality. This estimator consumes a negligible fraction of the overall privacy\\nbudget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,\\nand DenseNet121 across a range of datasets demonstrate that DPQuant\\nconsistently outperforms static quantization baselines, achieving near\\nPareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical\\nthroughput improvements on low-precision hardware, with less than 2% drop in\\nvalidation accuracy.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.03472v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.03472v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.03024v1', updated=datetime.datetime(2025, 9, 3, 5, 15, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 3, 5, 15, 45, tzinfo=datetime.timezone.utc), title='Efficient Privacy-Preserving Recommendation on Sparse Data using Fully Homomorphic Encryption', authors=[arxiv.Result.Author('Moontaha Nishat Chowdhury'), arxiv.Result.Author('André Bauer'), arxiv.Result.Author('Minxuan Zhou')], summary=\"In today's data-driven world, recommendation systems personalize user\\nexperiences across industries but rely on sensitive data, raising privacy\\nconcerns. Fully homomorphic encryption (FHE) can secure these systems, but a\\nsignificant challenge in applying FHE to recommendation systems is efficiently\\nhandling the inherently large and sparse user-item rating matrices. FHE\\noperations are computationally intensive, and naively processing various sparse\\nmatrices in recommendation systems would be prohibitively expensive.\\nAdditionally, the communication overhead between parties remains a critical\\nconcern in encrypted domains. We propose a novel approach combining Compressed\\nSparse Row (CSR) representation with FHE-based matrix factorization that\\nefficiently handles matrix sparsity in the encrypted domain while minimizing\\ncommunication costs. Our experimental results demonstrate high recommendation\\naccuracy with encrypted data while achieving the lowest communication costs,\\neffectively preserving user privacy.\", comment=\"The paper is accepted at the 21st IEEE International eScience\\n  Conference (eScience'25) and will be published soon. Link:\\n  https://www.escience-conference.org/2025/papers\", journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.03024v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.03024v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.01470v1', updated=datetime.datetime(2025, 9, 1, 13, 38, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 1, 13, 38, 11, tzinfo=datetime.timezone.utc), title='Privacy-preserving authentication for military 5G networks', authors=[arxiv.Result.Author('I. D. Lutz'), arxiv.Result.Author('A. M. Hill'), arxiv.Result.Author('M. C. Valenti')], summary='As 5G networks gain traction in defense applications, ensuring the privacy\\nand integrity of the Authentication and Key Agreement (AKA) protocol is\\ncritical. While 5G AKA improves upon previous generations by concealing\\nsubscriber identities, it remains vulnerable to replay-based synchronization\\nand linkability threats under realistic adversary models. This paper provides a\\nunified analysis of the standardized 5G AKA flow, identifying several\\nvulnerabilities and highlighting how each exploits protocol behavior to\\ncompromise user privacy. To address these risks, we present five lightweight\\nmitigation strategies. We demonstrate through prototype implementation and\\ntesting that these enhancements strengthen resilience against linkability\\nattacks with minimal computational and signaling overhead. Among the solutions\\nstudied, those introducing a UE-generated nonce emerge as the most promising,\\neffectively neutralizing the identified tracking and correlation attacks with\\nnegligible additional overhead. Integrating this extension as an optional\\nfeature to the standard 5G AKA protocol offers a backward-compatible,\\nlow-overhead path toward a more privacy-preserving authentication framework for\\nboth commercial and military 5G deployments.', comment='To appear in Proc. IEEE Military Commun. Conf. (MILCOM), (Los\\n  Angeles, CA), Oct. 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.01470v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.01470v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.20613v1', updated=datetime.datetime(2025, 8, 28, 10, 0, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 28, 10, 0, 39, tzinfo=datetime.timezone.utc), title='Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization', authors=[arxiv.Result.Author('Yixiang Qiu'), arxiv.Result.Author('Yanhan Liu'), arxiv.Result.Author('Hongyao Yu'), arxiv.Result.Author('Hao Fang'), arxiv.Result.Author('Bin Chen'), arxiv.Result.Author('Shu-Tao Xia'), arxiv.Result.Author('Ke Xu')], summary='The growing complexity of Deep Neural Networks (DNNs) has led to the adoption\\nof Split Inference (SI), a collaborative paradigm that partitions computation\\nbetween edge devices and the cloud to reduce latency and protect user privacy.\\nHowever, recent advances in Data Reconstruction Attacks (DRAs) reveal that\\nintermediate features exchanged in SI can be exploited to recover sensitive\\ninput data, posing significant privacy risks. Existing DRAs are typically\\neffective only on shallow models and fail to fully leverage semantic priors,\\nlimiting their reconstruction quality and generalizability across datasets and\\nmodel architectures. In this paper, we propose a novel GAN-based DRA framework\\nwith Progressive Feature Optimization (PFO), which decomposes the generator\\ninto hierarchical blocks and incrementally refines intermediate representations\\nto enhance the semantic fidelity of reconstructed images. To stabilize the\\noptimization and improve image realism, we introduce an L1-ball constraint\\nduring reconstruction. Extensive experiments show that our method outperforms\\nprior attacks by a large margin, especially in high-resolution scenarios,\\nout-of-distribution settings, and against deeper and more complex DNNs.', comment='10 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.20613v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.20613v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.00078v1', updated=datetime.datetime(2025, 8, 26, 20, 40, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 26, 20, 40, 24, tzinfo=datetime.timezone.utc), title='ChipChat: Low-Latency Cascaded Conversational Agent in MLX', authors=[arxiv.Result.Author('Tatiana Likhomanenko'), arxiv.Result.Author('Luke Carlson'), arxiv.Result.Author('Richard He Bai'), arxiv.Result.Author('Zijin Gu'), arxiv.Result.Author('Han Tran'), arxiv.Result.Author('Zakaria Aldeneh'), arxiv.Result.Author('Yizhe Zhang'), arxiv.Result.Author('Ruixiang Zhang'), arxiv.Result.Author('Huangjie Zheng'), arxiv.Result.Author('Navdeep Jaitly')], summary='The emergence of large language models (LLMs) has transformed spoken dialog\\nsystems, yet the optimal architecture for real-time on-device voice agents\\nremains an open question. While end-to-end approaches promise theoretical\\nadvantages, cascaded systems (CSs) continue to outperform them in language\\nunderstanding tasks, despite being constrained by sequential processing\\nlatency. In this work, we introduce ChipChat, a novel low-latency CS that\\novercomes traditional bottlenecks through architectural innovations and\\nstreaming optimizations. Our system integrates streaming (a) conversational\\nspeech recognition with mixture-of-experts, (b) state-action augmented LLM, (c)\\ntext-to-speech synthesis, (d) neural vocoder, and (e) speaker modeling.\\nImplemented using MLX, ChipChat achieves sub-second response latency on a Mac\\nStudio without dedicated GPUs, while preserving user privacy through complete\\non-device processing. Our work shows that strategically redesigned CSs can\\novercome their historical latency limitations, offering a promising path\\nforward for practical voice-based AI agents.', comment='ASRU 2025', journal_ref=None, doi=None, primary_category='eess.AS', categories=['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.00078v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.00078v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.18453v2', updated=datetime.datetime(2025, 9, 3, 0, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 25, 20, 2, 7, tzinfo=datetime.timezone.utc), title='Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication', authors=[arxiv.Result.Author('Yaser Baseri'), arxiv.Result.Author('Abdelhakim Senhaji Hafid'), arxiv.Result.Author('Dimitrios Makrakis'), arxiv.Result.Author('Hamidreza Fereidouni')], summary=\"Balancing robust security with strong privacy guarantees is critical for\\nRisk-Based Adaptive Authentication (RBA), particularly in decentralized\\nsettings. Federated Learning (FL) offers a promising solution by enabling\\ncollaborative risk assessment without centralizing user data. However, existing\\nFL approaches struggle with Non-Independent and Identically Distributed\\n(Non-IID) user features, resulting in biased, unstable, and poorly generalized\\nglobal models. This paper introduces FL-RBA2, a novel Federated Learning\\nframework for Risk-Based Adaptive Authentication that addresses Non-IID\\nchallenges through a mathematically grounded similarity transformation. By\\nconverting heterogeneous user features (including behavioral, biometric,\\ncontextual, interaction-based, and knowledge-based modalities) into IID\\nsimilarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk\\nmodeling across distributed clients. The framework mitigates cold-start\\nlimitations via clustering-based risk labeling, incorporates Differential\\nPrivacy (DP) to safeguard sensitive information, and employs Message\\nAuthentication Codes (MACs) to ensure model integrity and authenticity.\\nFederated updates are securely aggregated into a global model, achieving strong\\nbalance between user privacy, scalability, and adaptive authentication\\nrobustness. Rigorous game-based security proofs in the Random Oracle Model\\nformally establish privacy, correctness, and adaptive security guarantees.\\nExtensive experiments on keystroke, mouse, and contextual datasets validate\\nFL-RBA2's effectiveness in high-risk user detection and its resilience to model\\ninversion and inference attacks, even under strong DP constraints.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.18453v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.18453v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.17962v1', updated=datetime.datetime(2025, 8, 25, 12, 22, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 25, 12, 22, 25, tzinfo=datetime.timezone.utc), title='\"Nobody should control the end user\": Exploring Privacy Perspectives of Indian Internet Users in Light of DPDPA', authors=[arxiv.Result.Author('Sana Athar'), arxiv.Result.Author('Devashish Gosain'), arxiv.Result.Author('Anja Feldmann'), arxiv.Result.Author('Mannat Kaur'), arxiv.Result.Author('Ha Dao')], summary=\"With the rapid increase in online interactions, concerns over data privacy\\nand transparency of data processing practices have become more pronounced.\\nWhile regulations like the GDPR have driven the widespread adoption of cookie\\nbanners in the EU, India's Digital Personal Data Protection Act (DPDPA)\\npromises similar changes domestically, aiming to introduce a framework for data\\nprotection. However, certain clauses within the DPDPA raise concerns about\\npotential infringements on user privacy, given the exemptions for government\\naccountability and user consent requirements. In this study, for the first\\ntime, we explore Indian Internet users' awareness and perceptions of cookie\\nbanners, online privacy, and privacy regulations, especially in light of the\\nnewly passed DPDPA. We conducted an online anonymous survey with 428 Indian\\nparticipants, which addressed: (1) users' perspectives on cookie banners, (2)\\ntheir attitudes towards online privacy and privacy regulations, and (3) their\\nacceptance of 10 contentious DPDPA clauses that favor state authorities and may\\nenable surveillance. Our findings reveal that privacy-conscious users often\\nlack consistent awareness of privacy mechanisms, and their concerns do not\\nalways lead to protective actions. Our thematic analysis of 143 open ended\\nresponses shows that users' privacy and data protection concerns are rooted in\\nskepticism towards the government, shaping their perceptions of the DPDPA and\\nfueling demands for policy revisions. Our study highlights the need for clearer\\ncommunication regarding the DPDPA, user-centric consent mechanisms, and policy\\nrefinements to enhance data privacy practices in India.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.17962v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.17962v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.19286v1', updated=datetime.datetime(2025, 8, 25, 4, 38, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 25, 4, 38, 19, tzinfo=datetime.timezone.utc), title='RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting', authors=[arxiv.Result.Author('Zhan Shi'), arxiv.Result.Author('Yefeng Yuan'), arxiv.Result.Author('Yuhong Liu'), arxiv.Result.Author('Liang Cheng'), arxiv.Result.Author('Yi Fang')], summary='The performance of modern machine learning systems depends on access to\\nlarge, high-quality datasets, often sourced from user-generated content or\\nproprietary, domain-specific corpora. However, these rich datasets inherently\\ncontain sensitive personal information, raising significant concerns about\\nprivacy, data security, and compliance with regulatory frameworks. While\\nconventional anonymization techniques can remove explicit identifiers, such\\nremoval may result in performance drop in downstream machine learning tasks.\\nMore importantly, simple anonymization may not be effective against inference\\nattacks that exploit implicit signals such as writing style, topical focus, or\\ndemographic cues, highlighting the need for more robust privacy safeguards\\nduring model training. To address the challenging issue of balancing user\\nprivacy and data utility, we propose a reinforcement learning framework that\\nfine-tunes a large language model (LLM) using a composite reward function that\\njointly optimizes for explicit and implicit privacy, semantic fidelity, and\\noutput diversity. To effectively capture population level regularities, the\\nprivacy reward combines semantic cues with structural patterns derived from a\\nminimum spanning tree (MST) over latent representations. By modeling these\\nprivacy-sensitive signals in their distributional context, the proposed\\napproach guides the model to generate synthetic rewrites that preserve utility\\nwhile mitigating privacy risks. Empirical results show that the proposed method\\nsignificantly enhances author obfuscation and privacy metrics without degrading\\nsemantic quality, providing a scalable and model-agnostic solution for privacy\\npreserving data generation in the era of large language models.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.19286v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.19286v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.16765v1', updated=datetime.datetime(2025, 8, 22, 19, 49, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 22, 19, 49, 3, tzinfo=datetime.timezone.utc), title='Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models', authors=[arxiv.Result.Author('GodsGift Uzor'), arxiv.Result.Author('Hasan Al-Qudah'), arxiv.Result.Author('Ynes Ineza'), arxiv.Result.Author('Abdul Serwadda')], summary='The interactive nature of Large Language Models (LLMs), which closely track\\nuser data and context, has prompted users to share personal and private\\ninformation in unprecedented ways. Even when users opt out of allowing their\\ndata to be used for training, these privacy settings offer limited protection\\nwhen LLM providers operate in jurisdictions with weak privacy laws, invasive\\ngovernment surveillance, or poor data security practices. In such cases, the\\nrisk of sensitive information, including Personally Identifiable Information\\n(PII), being mishandled or exposed remains high. To address this, we propose\\nthe concept of an \"LLM gatekeeper\", a lightweight, locally run model that\\nfilters out sensitive information from user queries before they are sent to the\\npotentially untrustworthy, though highly capable, cloud-based LLM. Through\\nexperiments with human subjects, we demonstrate that this dual-model approach\\nintroduces minimal overhead while significantly enhancing user privacy, without\\ncompromising the quality of LLM responses.', comment='2025 19th International Conference on Semantic Computing (ICSC)', journal_ref=None, doi='10.1109/ICSC64641.2025.00040', primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.CL'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICSC64641.2025.00040', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2508.16765v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.16765v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.16703v1', updated=datetime.datetime(2025, 8, 22, 7, 41, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 22, 7, 41, 35, tzinfo=datetime.timezone.utc), title='Dynamic Sparse Attention on Mobile SoCs', authors=[arxiv.Result.Author('Wangsong Yin'), arxiv.Result.Author('Daliang Xu'), arxiv.Result.Author('Mengwei Xu'), arxiv.Result.Author('Gang Huang'), arxiv.Result.Author('Xuanzhe Liu')], summary='On-device running Large Language Models (LLMs) is nowadays a critical enabler\\ntowards preserving user privacy. We observe that the attention operator falls\\nback from the special-purpose NPU to the general-purpose CPU/GPU because of\\nquantization sensitivity in state-of-the-art frameworks. This fallback results\\nin a degraded user experience and increased complexity in system scheduling. To\\nthis end, this paper presents shadowAttn, a system-algorithm codesigned sparse\\nattention module with minimal reliance on CPU/GPU by only sparsely calculating\\nthe attention on a tiny portion of tokens. The key idea is to hide the overhead\\nof estimating the important tokens with a NPU-based pilot compute. Further,\\nshadowAttn proposes insightful techniques such as NPU compute graph bucketing,\\nhead-wise NPU-CPU/GPU pipeline and per-head fine-grained sparsity ratio to\\nachieve high accuracy and efficiency. shadowAttn delivers the best performance\\nwith highly limited CPU/GPU resource; it requires much less CPU/GPU resource to\\ndeliver on-par performance of SoTA frameworks.', comment='Technical Report', journal_ref=None, doi=None, primary_category='cs.PF', categories=['cs.PF', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.16703v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.16703v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.15036v1', updated=datetime.datetime(2025, 8, 20, 20, 2, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 20, 20, 2, 35, tzinfo=datetime.timezone.utc), title='MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs', authors=[arxiv.Result.Author('Ruyi Ding'), arxiv.Result.Author('Tianhong Xu'), arxiv.Result.Author('Xinyi Shen'), arxiv.Result.Author('Aidong Adam Ding'), arxiv.Result.Author('Yunsi Fei')], summary='The transformer architecture has become a cornerstone of modern AI, fueling\\nremarkable progress across applications in natural language processing,\\ncomputer vision, and multimodal learning. As these models continue to scale\\nexplosively for performance, implementation efficiency remains a critical\\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\\nspecialized subnetworks (experts), offer a unique balance between model\\naccuracy and computational cost. However, the adaptive routing in MoE\\narchitectures, where input tokens are dynamically directed to specialized\\nexperts based on their semantic meaning inadvertently opens up a new attack\\nsurface for privacy breaches. These input-dependent activation patterns leave\\ndistinctive temporal and spatial traces in hardware execution, which\\nadversaries could exploit to deduce sensitive user data. In this work, we\\npropose MoEcho, discovering a side channel analysis based attack surface that\\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\\nintroduce four novel architectural side channels on different computing\\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\\nthese vulnerabilities, we propose four attacks that effectively breach user\\nprivacy in large language models (LLMs) and vision language models (VLMs) based\\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\\nruntime architecture level security analysis of the popular MoE structure\\ncommon in modern transformers, highlighting a serious security and privacy\\nthreat and calling for effective and timely safeguards when harnessing MoE\\nbased models for developing efficient large scale AI services.', comment='This paper will appear in CCS 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.15036v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.15036v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.14815v1', updated=datetime.datetime(2025, 8, 20, 16, 6, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 20, 16, 6, 19, tzinfo=datetime.timezone.utc), title='A Lightweight Privacy-Preserving Smart Metering Billing Protocol with Dynamic Tariff Policy Adjustment', authors=[arxiv.Result.Author('Farid Zaredar'), arxiv.Result.Author('Morteza Amini')], summary=\"The integration of information and communication technology (ICT) with\\ntraditional power grids has led to the emergence of smart grids. Advanced\\nmetering infrastructure (AMI) plays a crucial role in smart grids by\\nfacilitating two-way communication between smart meters and the utility\\nprovider. This bidirectional communication allows intelligent meters to report\\nfine-grained consumption data at predefined intervals, enabling accurate\\nbilling, efficient grid monitoring and management, and rapid outage detection.\\nHowever, the collection of detailed consumption data can inadvertently disclose\\nconsumers' daily activities, raising privacy concerns and potentially leading\\nto privacy violations. To address these issues and preserve individuals'\\nprivacy, we propose a lightweight privacy-preserving smart metering protocol\\nspecifically designed to support real-time tariff billing service with dynamic\\npolicy adjustment. Our scheme employs an efficient data perturbation technique\\nto obscure precise energy usage data from internal adversaries, including the\\nintermediary gateways and the utility provider. Subsequently, we validate the\\nefficiency and security of our protocol through comprehensive performance and\\nprivacy evaluations. We examined the computational, memory, and communication\\noverhead of the proposed scheme. The execution time of our secure and\\nprivacy-aware billing system is approximately 3.94540 seconds for a complete\\nyear. Furthermore, we employed the Jensen-Shannon divergence as a privacy\\nmetric to demonstrate that our protocol can effectively safeguard users'\\nprivacy by increasing the noise scale.\", comment='12 pages, 8 figures, 7 tables', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.14815v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.14815v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.14744v1', updated=datetime.datetime(2025, 8, 20, 14, 40, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 20, 14, 40, 33, tzinfo=datetime.timezone.utc), title='A Collusion-Resistance Privacy-Preserving Smart Metering Protocol for Operational Utility', authors=[arxiv.Result.Author('Farid Zaredar'), arxiv.Result.Author('Morteza Amini')], summary=\"Modern grids have adopted advanced metering infrastructure (AMI) to\\nfacilitate bidirectional communication between smart meters and control\\ncenters. This enables smart meters to report consumption values at predefined\\nintervals to utility providers for purposes including demand balancing, load\\nforecasting, dynamic billing, and operational efficiency. Compared to\\ntraditional power grids, smart grids offer advantages such as enhanced\\nreliability, improved energy efficiency, and increased security. However,\\nutility providers can compromise user privacy by analyzing fine-grained\\nreadings and extracting individuals' daily activities from this time-series\\ndata. To address this concern, we propose a collusion-resistant,\\nprivacy-preserving aggregation protocol for smart metering in operational\\nservices. Our protocol ensures privacy by leveraging techniques such as\\npartially additive homomorphic encryption, aggregation, data perturbation, and\\ndata minimization. The scheme aggregates perturbed readings using the additive\\nhomomorphic property of the Paillier cryptosystem to provide results for\\nmultiple operational purposes. We evaluate the protocol in terms of both\\nperformance and privacy. Computational, memory, and communication overhead were\\nexamined. The total execution time with 1024-bit key size is about 2.21\\nseconds. We also evaluated privacy through the normalized conditional entropy\\n(NCE) metric. Higher NCE values, closer to 1, indicate stronger privacy. By\\nincreasing noise scale, the NCE value rises, showing perturbed values retain\\nminimal information about the original, thereby reducing risks. Overall,\\nevaluation demonstrates the protocol's efficiency while employing various\\nprivacy-preserving techniques.\", comment='13 pages, 9 figures, 7 tables', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.14744v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.14744v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.14284v1', updated=datetime.datetime(2025, 8, 19, 21, 53, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 19, 21, 53, 31, tzinfo=datetime.timezone.utc), title='Differentially Private aggregate hints in mev-share', authors=[arxiv.Result.Author('Jonathan Passerat-Palmbach'), arxiv.Result.Author('Sarisht Wadhwa')], summary='Flashbots recently released mev-share to empower users with control over the\\namount of information they share with searchers for extracting Maximal\\nExtractable Value (MEV). Searchers require more information to maintain\\non-chain exchange efficiency and profitability, while users aim to prevent\\nfrontrunning by withholding information. After analyzing two searching\\nstrategies in mev-share to reason about searching techniques, this paper\\nintroduces Differentially-Private (DP) aggregate hints as a new type of hints\\nto disclose information quantitatively. DP aggregate hints enable users to\\nformally quantify their privacy loss to searchers, and thus better estimate the\\nlevel of rebates to ask in return. The paper discusses the current properties\\nand privacy loss in mev-share and lays out how DP aggregate hints could enhance\\nthe system for both users and searchers. We leverage Differential Privacy in\\nthe Trusted Curator Model to design our aggregate hints. Additionally, we\\nexplain how random sampling can defend against sybil attacks and amplify\\noverall user privacy while providing valuable hints to searchers for improved\\nbackrunning extraction and frontrunning prevention.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.14284v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.14284v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.13135v2', updated=datetime.datetime(2025, 8, 19, 1, 52, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 18, 17, 49, 10, tzinfo=datetime.timezone.utc), title='Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]', authors=[arxiv.Result.Author('Yueyang Liu'), arxiv.Result.Author('Lance Kennedy'), arxiv.Result.Author('Ruochen Kong'), arxiv.Result.Author('Joon-Seok Kim'), arxiv.Result.Author('Andreas Züfle')], summary='Individual-level human mobility prediction has emerged as a significant topic\\nof research with applications in infectious disease monitoring, child, and\\nelderly care. Existing studies predominantly focus on the microscopic aspects\\nof human trajectories: such as predicting short-term trajectories or the next\\nlocation visited, while offering limited attention to macro-level mobility\\npatterns and the corresponding life routines. In this paper, we focus on an\\nunderexplored problem in human mobility prediction: determining the best\\npractices to train a machine learning model using historical data to forecast\\nan individuals complete trajectory over the next days and weeks. In this\\nexperiment paper, we undertake a comprehensive experimental analysis of diverse\\nmodels, parameter configurations, and training strategies, accompanied by an\\nin-depth examination of the statistical distribution inherent in human mobility\\npatterns. Our empirical evaluations encompass both Long Short-Term Memory and\\nTransformer-based architectures, and further investigate how incorporating\\nindividual life patterns can enhance the effectiveness of the prediction. We\\nshow that explicitly including semantic information such as day-of-the-week and\\nuser-specific historical information can help the model better understand\\nindividual patterns of life and improve predictions. Moreover, since the\\nabsence of explicit user information is often missing due to user privacy, we\\nshow that the sampling of users may exacerbate data skewness and result in a\\nsubstantial loss in predictive accuracy. To mitigate data imbalance and\\npreserve diversity, we apply user semantic clustering with stratified sampling\\nto ensure that the sampled dataset remains representative. Our results further\\nshow that small-batch stochastic gradient optimization improves model\\nperformance, especially when human mobility training data is limited.', comment='This paper is the extended version of our work accepted at the 33rd\\n  ACM International Conference on Advances in Geographic Information Systems', journal_ref=None, doi='10.1145/3748636.3762740', primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3748636.3762740', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2508.13135v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.13135v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.11742v1', updated=datetime.datetime(2025, 8, 15, 17, 54, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 15, 17, 54, 27, tzinfo=datetime.timezone.utc), title='Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach', authors=[arxiv.Result.Author('Minhao Jin'), arxiv.Result.Author('Hongyu He'), arxiv.Result.Author('Maria Apostolaki')], summary='Current synthetic traffic generators (SynNetGens) promise privacy but lack\\ncomprehensive guarantees or empirical validation, even as their fidelity\\nsteadily improves. We introduce the first attack-grounded benchmark for\\nassessing the privacy of SynNetGens directly from the traffic they produce. We\\nframe privacy as membership inference at the traffic-source level--a realistic\\nand actionable threat for data holders. To this end, we present TraceBleed, the\\nfirst attack that exploits behavioral fingerprints across flows using\\ncontrastive learning and temporal chunking, outperforming prior membership\\ninference baselines by 172%. Our large-scale study across GAN-, diffusion-, and\\nGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level\\ninformation; (ii) differential privacy either fails to stop these attacks or\\nseverely degrades fidelity; and (iii) sharing more synthetic data amplifies\\nleakage by 59% on average. Finally, we introduce TracePatch, the first\\nSynNetGen-agnostic defense that combines adversarial ML with SMT constraints to\\nmitigate leakage while preserving fidelity.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.NI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.11742v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.11742v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.09980v1', updated=datetime.datetime(2025, 8, 13, 17, 52, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 13, 17, 52, 7, tzinfo=datetime.timezone.utc), title='On the Consistency and Performance of the Iterative Bayesian Update', authors=[arxiv.Result.Author('Ehab ElSalamouny'), arxiv.Result.Author('Catuscia Palamidessi')], summary=\"For many social, scientific, and commercial purposes, it is often important\\nto estimate the distribution of the users' data regarding a sensitive\\nattribute, e.g., their ages, locations, etc. To allow this estimation while\\nprotecting the users' privacy, every user applies a local privacy protection\\nmechanism that releases a noisy (sanitized) version of their original datum to\\nthe data collector; then the original distribution is estimated using one of\\nthe known methods, such as the matrix inversion (INV), RAPPOR's estimator, and\\nthe iterative Bayesian update (IBU). Unlike the other estimators, the\\nconsistency of IBU, i.e., the convergence of its estimate to the real\\ndistribution as the amount of noisy data grows, has been either ignored or\\nincorrectly proved in the literature. In this article, we use the fact that IBU\\nis a maximum likelihood estimator to prove that IBU is consistent. We also\\nshow, through experiments on real datasets, that IBU significantly outperforms\\nthe other methods when the users' data are sanitized by geometric, Laplace, and\\nexponential mechanisms, whereas it is comparable to the other methods in the\\ncase of the k-RR and RAPPOR mechanisms. Finally, we consider the case when the\\nalphabet of the sensitive data is infinite, and we show a technique that allows\\nIBU to operate in this case too.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.09980v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.09980v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.07672v1', updated=datetime.datetime(2025, 8, 11, 6, 51, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 11, 6, 51, 44, tzinfo=datetime.timezone.utc), title=\"Towards Aligning Personalized Conversational Recommendation Agents with Users' Privacy Preferences\", authors=[arxiv.Result.Author('Shuning Zhang'), arxiv.Result.Author('Ying Ma'), arxiv.Result.Author('Jingruo Chen'), arxiv.Result.Author('Simin Li'), arxiv.Result.Author('Xin Yi'), arxiv.Result.Author('Hewu Li')], summary=\"The proliferation of AI agents, with their complex and context-dependent\\nactions, renders conventional privacy paradigms obsolete. This position paper\\nargues that the current model of privacy management, rooted in a user's\\nunilateral control over a passive tool, is inherently mismatched with the\\ndynamic and interactive nature of AI agents. We contend that ensuring effective\\nprivacy protection necessitates that the agents proactively align with users'\\nprivacy preferences instead of passively waiting for the user to control. To\\nground this shift, and using personalized conversational recommendation agents\\nas a case, we propose a conceptual framework built on Contextual Integrity (CI)\\ntheory and Privacy Calculus theory. This synthesis first reframes automatically\\ncontrolling users' privacy as an alignment problem, where AI agents initially\\ndid not know users' preferences, and would learn their privacy preferences\\nthrough implicit or explicit feedback. Upon receiving the preference feedback,\\nthe agents used alignment and Pareto optimization for aligning preferences and\\nbalancing privacy and utility. We introduced formulations and instantiations,\\npotential applications, as well as five challenges.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.07672v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.07672v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.07664v1', updated=datetime.datetime(2025, 8, 11, 6, 26, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 11, 6, 26, 30, tzinfo=datetime.timezone.utc), title=\"Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory\", authors=[arxiv.Result.Author('Shuning Zhang'), arxiv.Result.Author('Rongjun Ma'), arxiv.Result.Author('Ying Ma'), arxiv.Result.Author('Shixuan Li'), arxiv.Result.Author('Yiqun Xu'), arxiv.Result.Author('Xin Yi'), arxiv.Result.Author('Hewu Li')], summary=\"Large Language Models (LLMs) are increasingly integrating memory\\nfunctionalities to provide personalized and context-aware interactions.\\nHowever, user understanding, practices and expectations regarding these memory\\nsystems are not yet well understood. This paper presents a thematic analysis of\\nsemi-structured interviews with 18 users to explore their mental models of\\nLLM's Retrieval Augmented Generation (RAG)-based memory, current usage\\npractices, perceived benefits and drawbacks, privacy concerns and expectations\\nfor future memory systems. Our findings reveal diverse and often incomplete\\nmental models of how memory operates. While users appreciate the potential for\\nenhanced personalization and efficiency, significant concerns exist regarding\\nprivacy, control and the accuracy of remembered information. Users express a\\ndesire for granular control over memory generation, management, usage and\\nupdating, including clear mechanisms for reviewing, editing, deleting and\\ncategorizing memories, as well as transparent insight into how memories and\\ninferred information are used. We discuss design implications for creating more\\nuser-centric, transparent, and trustworthy LLM memory systems.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.07664v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.07664v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.06760v1', updated=datetime.datetime(2025, 8, 9, 0, 22, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 9, 0, 22, 46, tzinfo=datetime.timezone.utc), title='Understanding Privacy Norms Around LLM-Based Chatbots: A Contextual Integrity Perspective', authors=[arxiv.Result.Author('Sarah Tran'), arxiv.Result.Author('Hongfan Lu'), arxiv.Result.Author('Isaac Slaughter'), arxiv.Result.Author('Bernease Herman'), arxiv.Result.Author('Aayushi Dangol'), arxiv.Result.Author('Yue Fu'), arxiv.Result.Author('Lufei Chen'), arxiv.Result.Author('Biniyam Gebreyohannes'), arxiv.Result.Author('Bill Howe'), arxiv.Result.Author('Alexis Hiniker'), arxiv.Result.Author('Nicholas Weber'), arxiv.Result.Author('Robert Wolfe')], summary='LLM-driven chatbots like ChatGPT have created large volumes of conversational\\ndata, but little is known about how user privacy expectations are evolving with\\nthis technology. We conduct a survey experiment with 300 US ChatGPT users to\\nunderstand emerging privacy norms for sharing chatbot data. Our findings reveal\\na stark disconnect between user concerns and behavior: 82% of respondents rated\\nchatbot conversations as sensitive or highly sensitive - more than email or\\nsocial media posts - but nearly half reported discussing health topics and over\\none-third discussed personal finances with ChatGPT. Participants expressed\\nstrong privacy concerns (t(299) = 8.5, p < .01) and doubted their conversations\\nwould remain private (t(299) = -6.9, p < .01). Despite this, respondents\\nuniformly rejected sharing personal data (search history, emails, device\\naccess) for improved services, even in exchange for premium features worth\\n$200. To identify which factors influence appropriate chatbot data sharing, we\\npresented participants with factorial vignettes manipulating seven contextual\\nfactors. Linear mixed models revealed that only the transmission factors such\\nas informed consent, data anonymization, or the removal of personally\\nidentifiable information, significantly affected perceptions of appropriateness\\nand concern for data access. Surprisingly, contextual factors including the\\nrecipient of the data (hospital vs. tech company), purpose (research vs.\\nadvertising), type of content, and geographic location did not show significant\\neffects. Our results suggest that users apply consistent baseline privacy\\nexpectations to chatbot data, prioritizing procedural safeguards over recipient\\ntrustworthiness. This has important implications for emerging agentic AI\\nsystems that assume user willingness to integrate personal data across\\nplatforms.', comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.06760v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.06760v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.06208v1', updated=datetime.datetime(2025, 8, 8, 10, 44, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 8, 10, 44, 33, tzinfo=datetime.timezone.utc), title='Graph Federated Learning for Personalized Privacy Recommendation', authors=[arxiv.Result.Author('Ce Na'), arxiv.Result.Author('Kai Yang'), arxiv.Result.Author('Dengzhao Fang'), arxiv.Result.Author('Yu Li'), arxiv.Result.Author('Jingtong Gao'), arxiv.Result.Author('Chengcheng Zhu'), arxiv.Result.Author('Jiale Zhang'), arxiv.Result.Author('Xiaobing Sun'), arxiv.Result.Author('Yi Chang')], summary=\"Federated recommendation systems (FedRecs) have gained significant attention\\nfor providing privacy-preserving recommendation services. However, existing\\nFedRecs assume that all users have the same requirements for privacy\\nprotection, i.e., they do not upload any data to the server. The approaches\\noverlook the potential to enhance the recommendation service by utilizing\\npublicly available user data. In real-world applications, users can choose to\\nbe private or public. Private users' interaction data is not shared, while\\npublic users' interaction data can be shared. Inspired by the issue, this paper\\nproposes a novel Graph Federated Learning for Personalized Privacy\\nRecommendation (GFed-PP) that adapts to different privacy requirements while\\nimproving recommendation performance. GFed-PP incorporates the interaction data\\nof public users to build a user-item interaction graph, which is then used to\\nform a user relationship graph. A lightweight graph convolutional network (GCN)\\nis employed to learn each user's user-specific personalized item embedding. To\\nprotect user privacy, each client learns the user embedding and the scoring\\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\\nrecommendation framework through the initialization of item embedding on\\nclients and the aggregation of the user relationship graph on the server.\\nExperimental results demonstrate that GFed-PP significantly outperforms\\nexisting methods for five datasets, offering superior recommendation accuracy\\nwithout compromising privacy. This framework provides a practical solution for\\naccommodating varying privacy preferences in federated recommendation systems.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.06208v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.06208v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.04846v1', updated=datetime.datetime(2025, 8, 6, 19, 50, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 6, 19, 50, 29, tzinfo=datetime.timezone.utc), title='Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)', authors=[arxiv.Result.Author('Mahdi Nazari Ashani'), arxiv.Result.Author('Ali Asghar Alesheikh'), arxiv.Result.Author('Saba Kazemi'), arxiv.Result.Author('Kimya Kheirkhah'), arxiv.Result.Author('Yasin Mohammadi'), arxiv.Result.Author('Fatemeh Rezaie'), arxiv.Result.Author('Amir Mahdi Manafi'), arxiv.Result.Author('Hedieh Zarkesh')], summary=\"Autonomous web-based geographical information systems (AWebGIS) aim to\\nperform geospatial operations from natural language input, providing intuitive,\\nintelligent, and hands-free interaction. However, most current solutions rely\\non cloud-based large language models (LLMs), which require continuous internet\\naccess and raise users' privacy and scalability issues due to centralized\\nserver processing. This study compares three approaches to enabling AWebGIS:\\n(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)\\na semi-automated offline method using classical machine learning classifiers\\nsuch as support vector machine and random forest; and (3) a fully autonomous\\noffline (client-side) method based on a fine-tuned small language model (SLM),\\nspecifically T5-small model, executed in the client's web browser. The third\\napproach, which leverages SLMs, achieved the highest accuracy among all\\nmethods, with an exact matching accuracy of 0.93, Levenshtein similarity of\\n0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L\\nscores of 0.98. Crucially, this client-side computation strategy reduces the\\nload on backend servers by offloading processing to the user's device,\\neliminating the need for server-based inference. These results highlight the\\nfeasibility of browser-executable models for AWebGIS solutions.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CL', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.04846v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.04846v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.04202v1', updated=datetime.datetime(2025, 8, 6, 8, 32, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 6, 8, 32, 54, tzinfo=datetime.timezone.utc), title=\"Unplug, Mute, Avoid: Investigating smart speaker users' privacy protection behaviours in Saudi Homes\", authors=[arxiv.Result.Author('Abdulrhman Alorini'), arxiv.Result.Author('Yufeng Wu'), arxiv.Result.Author('Abdullah Bin Sawad'), arxiv.Result.Author('Mukesh Prasad'), arxiv.Result.Author('A. Baki Kocaballi')], summary='Smart speakers are increasingly integrated into domestic life worldwide, yet\\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\\nconcerns within collectivist, gendered, and often multigenerational households.\\nUsing cultural probes followed by semi-structured interviews with 16\\nparticipants, we uncover everyday privacy-protective behaviours including\\nunplugging devices, muting microphones, and avoiding voice interactions\\naltogether. These practices are shaped not only by individual risk perceptions\\nbut also by household norms, room configurations, and interpersonal dynamics.\\nWe contribute empirical insights from an underrepresented region, theoretical\\nextensions to contextual integrity frameworks, and design directions for\\nculturally responsive voice interfaces. This work expands the global\\nconversation on smart speaker privacy and informs more inclusive HCI practices\\nin increasingly diverse smart home environments.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.04202v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.04202v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.03681v1', updated=datetime.datetime(2025, 8, 5, 17, 51, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 5, 17, 51, 1, tzinfo=datetime.timezone.utc), title='What If, But Privately: Private Counterfactual Retrieval', authors=[arxiv.Result.Author('Shreya Meel'), arxiv.Result.Author('Mohamed Nomeir'), arxiv.Result.Author('Pasan Dissanayake'), arxiv.Result.Author('Sanghamitra Dutta'), arxiv.Result.Author('Sennur Ulukus')], summary=\"Transparency and explainability are two important aspects to be considered\\nwhen employing black-box machine learning models in high-stake applications.\\nProviding counterfactual explanations is one way of catering this requirement.\\nHowever, this also poses a threat to the privacy of the institution that is\\nproviding the explanation, as well as the user who is requesting it. In this\\nwork, we are primarily concerned with the user's privacy who wants to retrieve\\na counterfactual instance, without revealing their feature vector to the\\ninstitution. Our framework retrieves the exact nearest neighbor counterfactual\\nexplanation from a database of accepted points while achieving perfect,\\ninformation-theoretic, privacy for the user. First, we introduce the problem of\\nprivate counterfactual retrieval (PCR) and propose a baseline PCR scheme that\\nkeeps the user's feature vector information-theoretically private from the\\ninstitution. Building on this, we propose two other schemes that reduce the\\namount of information leaked about the institution database to the user,\\ncompared to the baseline scheme. Second, we relax the assumption of mutability\\nof all features, and consider the setting of immutable PCR (I-PCR). Here, the\\nuser retrieves the nearest counterfactual without altering a private subset of\\ntheir features, which constitutes the immutable set, while keeping their\\nfeature vector and immutable set private from the institution. For this, we\\npropose two schemes that preserve the user's privacy information-theoretically,\\nbut ensure varying degrees of database privacy. Third, we extend our PCR and\\nI-PCR schemes to incorporate user's preference on transforming their\\nattributes, so that a more actionable explanation can be received. Finally, we\\npresent numerical results to support our theoretical findings, and compare the\\ndatabase leakage of the proposed schemes.\", comment='arXiv admin note: text overlap with arXiv:2410.13812,\\n  arXiv:2411.10429', journal_ref=None, doi=None, primary_category='cs.IT', categories=['cs.IT', 'cs.CR', 'cs.LG', 'cs.NI', 'eess.SP', 'math.IT'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.03681v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.03681v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.02943v1', updated=datetime.datetime(2025, 8, 4, 22, 53, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 4, 22, 53, 36, tzinfo=datetime.timezone.utc), title='A Non-leveled and Reliable Approximate FHE Framework through Binarized Polynomial Rings', authors=[arxiv.Result.Author('Baigang Chen'), arxiv.Result.Author('Dongfang Zhao')], summary=\"Homomorphic encryption (HE) enables secure computation on encrypted data,\\nsafeguarding user privacy in domains such as cloud computing, healthcare, and\\nfinance. Among fully homomorphic encryption (FHE) schemes, CKKS is notable for\\nsupporting approximate arithmetic over complex numbers, a key requirement for\\nmachine-learning and numerical workloads. However, CKKS incurs rapid noise\\ngrowth, complex parameter tuning, and relies on costly modulus switching. We\\npropose a binary variant of CKKS that operates entirely over binary-coefficient\\npolynomial rings and replaces rescaling with a lightweight bootstrapping\\nmechanism. To mitigate additional bit-flip errors introduced by binary\\nencoding, we integrate BCH error-correcting codes for robust decryption. Our\\nopen-source implementation, built on the HElib library, preserves the core\\nalgebraic structure of CKKS while introducing binary-coefficient encoding,\\nenabling efficient evaluation in small ring dimensions and unbounded-depth\\ncomputation. Empirical evaluations demonstrate the framework's practicality and\\nscalability across a range of settings.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.02943v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.02943v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.02461v1', updated=datetime.datetime(2025, 8, 4, 14, 28, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 4, 14, 28, 19, tzinfo=datetime.timezone.utc), title='Experimental Evaluation of Post-Quantum Homomorphic Encryption for Privacy-Preserving V2X Communication', authors=[arxiv.Result.Author('Abdullah Al Mamun'), arxiv.Result.Author('Kyle Yates'), arxiv.Result.Author('Antsa Rakotondrafara'), arxiv.Result.Author('Mashrur Chowdhury'), arxiv.Result.Author('Ryann Cartor'), arxiv.Result.Author('Shuhong Gao')], summary='Intelligent Transportation Systems (ITS) fundamentally rely on\\nvehicle-generated data for applications such as congestion monitoring and route\\noptimization, making the preservation of user privacy a critical challenge.\\nHomomorphic Encryption (HE) offers a promising solution by enabling computation\\non encrypted data without revealing underlying content. This study presents the\\nfirst real-world experimental evaluation of three post-quantum secure HE\\nschemes, i.e., Brakerski-Fan-Vercauteren (BFV), Brakerski-Gentry-Vaikuntanathan\\n(BGV), and Cheon-Kim-Kim-Song (CKKS), for vehicular communication scenarios.\\nTwo representative privacy-preserving use cases are considered: encrypted\\nvehicle counting and average speed aggregation. Experiments are conducted over\\nboth Wi-Fi and Ethernet to assess performance under wireless and wired\\nvehicle-to-everything (V2X) settings. Results show that BFV and BGV are\\nsuitable for latency-tolerant applications such as intersection monitoring and\\nregional traffic analysis, with total end-to-end latencies under 10 seconds.\\nWhile CKKS experiences higher overhead, it remains viable for periodic\\nencrypted aggregation of numerical data. The experimental results demonstrate\\nthat HE can be feasibly deployed in ITS environments under 128-bit post-quantum\\nsecurity, provided that scheme-specific latency constraints are considered.\\nThis reinforces its potential to serve as a foundational tool for secure and\\nprivacy-preserving V2X data processing.', comment='This version has been submitted to the TRB Annual Meeting 2026 and is\\n  currently under review', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.02461v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.02461v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.02454v1', updated=datetime.datetime(2025, 8, 4, 14, 18, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 4, 14, 18, 45, tzinfo=datetime.timezone.utc), title='Thwart Me If You Can: An Empirical Analysis of Android Platform Armoring Against Stalkerware', authors=[arxiv.Result.Author('Malvika Jadhav'), arxiv.Result.Author('Wenxuan Bao'), arxiv.Result.Author('Vincent Bindschaedler')], summary=\"Stalkerware is a serious threat to individuals' privacy that is receiving\\nincreased attention from the security and privacy research communities.\\nExisting works have largely focused on studying leading stalkerware apps,\\ndual-purpose apps, monetization of stalkerware, or the experience of survivors.\\nHowever, there remains a need to understand potential defenses beyond the\\ndetection-and-removal approach, which may not necessarily be effective in the\\ncontext of stalkerware.\\n  In this paper, we perform a systematic analysis of a large corpus of recent\\nAndroid stalkerware apps. We combine multiple analysis techniques to quantify\\nstalkerware behaviors and capabilities and how these evolved over time. Our\\nprimary goal is understanding: how (and whether) recent Android platform\\nchanges -- largely designed to improve user privacy -- have thwarted\\nstalkerware functionality; how stalkerware may have adapted as a result; and\\nwhat we may conclude about potential defenses. Our investigation reveals new\\ninsights into tactics used by stalkerware and may inspire alternative defense\\nstrategies.\", comment='15 pages, 2 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.02454v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.02454v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2508.02008v1', updated=datetime.datetime(2025, 8, 4, 2, 54, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 8, 4, 2, 54, 10, tzinfo=datetime.timezone.utc), title='A Comprehensive Analysis of Evolving Permission Usage in Android Apps: Trends, Threats, and Ecosystem Insights', authors=[arxiv.Result.Author('Ali Alkinoon'), arxiv.Result.Author('Trung Cuong Dang'), arxiv.Result.Author('Ahod Alghuried'), arxiv.Result.Author('Abdulaziz Alghamdi'), arxiv.Result.Author('Soohyeon Choi'), arxiv.Result.Author('Manar Mohaisen'), arxiv.Result.Author('An Wang'), arxiv.Result.Author('Saeed Salem'), arxiv.Result.Author('David Mohaisen')], summary=\"The proper use of Android app permissions is crucial to the success and\\nsecurity of these apps. Users must agree to permission requests when installing\\nor running their apps. Despite official Android platform documentation on\\nproper permission usage, there are still many cases of permission abuse. This\\nstudy provides a comprehensive analysis of the Android permission landscape,\\nhighlighting trends and patterns in permission requests across various\\napplications from the Google Play Store. By distinguishing between benign and\\nmalicious applications, we uncover developers' evolving strategies, with\\nmalicious apps increasingly requesting fewer permissions to evade detection,\\nwhile benign apps request more to enhance functionality. In addition to\\nexamining permission trends across years and app features such as\\nadvertisements, in-app purchases, content ratings, and app sizes, we leverage\\nassociation rule mining using the FP-Growth algorithm. This allows us to\\nuncover frequent permission combinations across the entire dataset, specific\\nyears, and 16 app genres. The analysis reveals significant differences in\\npermission usage patterns, providing a deeper understanding of co-occurring\\npermissions and their implications for user privacy and app functionality. By\\ncategorizing permissions into high-level semantic groups and examining their\\napplication across distinct app categories, this study offers a structured\\napproach to analyzing the dynamics within the Android ecosystem. The findings\\nemphasize the importance of continuous monitoring, user education, and\\nregulatory oversight to address permission misuse effectively.\", comment='16 pages, 6 figures, 14 tables. In submission to Journal of\\n  Cybersecurity and Privacy', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2508.02008v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2508.02008v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.22447v1', updated=datetime.datetime(2025, 7, 30, 7, 46, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 30, 7, 46, 49, tzinfo=datetime.timezone.utc), title='Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for Malicious JavaScript Detection', authors=[arxiv.Result.Author('Zhihong Liang'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Zhenhuang Hu'), arxiv.Result.Author('Liangliang Song'), arxiv.Result.Author('Lin Chen'), arxiv.Result.Author('Jingjing Guo'), arxiv.Result.Author('Yanbin Wang'), arxiv.Result.Author('Ye Tian')], summary=\"With the rapid expansion of web-based applications and cloud services,\\nmalicious JavaScript code continues to pose significant threats to user\\nprivacy, system integrity, and enterprise security. But, detecting such threats\\nremains challenging due to sophisticated code obfuscation techniques and\\nJavaScript's inherent language characteristics, particularly its nested closure\\nstructures and syntactic flexibility. In this work, we propose DeCoda, a hybrid\\ndefense framework that combines large language model (LLM)-based deobfuscation\\nwith code graph learning: (1) We first construct a sophisticated\\nprompt-learning pipeline with multi-stage refinement, where the LLM\\nprogressively reconstructs the original code structure from obfuscated inputs\\nand then generates normalized Abstract Syntax Tree (AST) representations; (2)\\nIn JavaScript ASTs, dynamic typing scatters semantically similar nodes while\\ndeeply nested functions fracture scope capturing, introducing structural noise\\nand semantic ambiguity. To address these challenges, we then propose to learn\\nhierarchical code graph representations via a Cluster-wise Graph that\\nsynergistically integrates graph transformer network, node clustering, and\\nnode-to-cluster attention to simultaneously capture both local node-level\\nsemantics and global cluster-induced structural relationships from AST graph.\\nExperimental results demonstrate that our method achieves F1-scores of 94.64%\\nand 97.71% on two benchmark datasets, demonstrating absolute improvements of\\n10.74% and 13.85% over state-of-the-art baselines. In false-positive control\\nevaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers\\n4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing\\nbaseline. These results highlight the effectiveness of LLM-based deobfuscation\\nand underscore the importance of modeling cluster-level relationships in\\ndetecting malicious code.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.22447v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.22447v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.22153v1', updated=datetime.datetime(2025, 7, 29, 18, 37, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 29, 18, 37, 24, tzinfo=datetime.timezone.utc), title='Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality', authors=[arxiv.Result.Author('Ethan Wilson'), arxiv.Result.Author('Vincent Bindschaedler'), arxiv.Result.Author('Sophie Jörg'), arxiv.Result.Author('Sean Sheikholeslam'), arxiv.Result.Author('Kevin Butler'), arxiv.Result.Author('Eakta Jain')], summary=\"Photorealistic 3D avatar generation has rapidly improved in recent years, and\\nrealistic avatars that match a user's true appearance are more feasible in\\nMixed Reality (MR) than ever before. Yet, there are known risks to sharing\\none's likeness online, and photorealistic MR avatars could exacerbate these\\nrisks. If user likenesses were to be shared broadly, there are risks for cyber\\nabuse or targeted fraud based on user appearances. We propose an alternate\\navatar rendering scheme for broader social MR -- synthesizing realistic avatars\\nthat preserve a user's demographic identity while being distinct enough from\\nthe individual user to protect facial biometric information. We introduce a\\nmethodology for privatizing appearance by isolating identity within the feature\\nspace of identity-encoding generative models. We develop two algorithms that\\nthen obfuscate identity: \\\\epsmethod{} provides differential privacy guarantees\\nand \\\\thetamethod{} provides fine-grained control for the level of identity\\noffset. These methods are shown to successfully generate de-identified virtual\\navatars across multiple generative architectures in 2D and 3D. With these\\ntechniques, it is possible to protect user privacy while largely preserving\\nattributes related to sense of self. Employing these techniques in public\\nsettings could enable the use of photorealistic avatars broadly in MR,\\nmaintaining high realism and immersion without privacy risk.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.22153v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.22153v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.20806v1', updated=datetime.datetime(2025, 7, 28, 13, 17, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 28, 13, 17, 25, tzinfo=datetime.timezone.utc), title='Collusion Resistant DNS With Private Information Retrieval', authors=[arxiv.Result.Author('Yunming Xiao'), arxiv.Result.Author('Peizhi Liu'), arxiv.Result.Author('Ruijie Yu'), arxiv.Result.Author('Chenkai Weng'), arxiv.Result.Author('Matteo Varvello'), arxiv.Result.Author('Aleksandar Kuzmanovic')], summary=\"There has been a growing interest in Internet user privacy, demonstrated by\\nthe popularity of privacy-preserving products such as Telegram and Brave, and\\nthe widespread adoption of HTTPS. The Domain Name System (DNS) is a key\\ncomponent of Internet-based communication and its privacy has been neglected\\nfor years. Recently, DNS over HTTPS (DoH) has improved the situation by fixing\\nthe issue of in-path middleboxes. Further progress has been made with\\nproxy-based solutions such as Oblivious DoH (ODoH), which separate a user's\\nidentity from their DNS queries. However, these solutions rely on non-collusion\\nassumptions between DNS resolvers and proxies -- an assumption difficult to\\nguarantee in practice. To address this, we explore integrating single-server\\nPrivate Information Retrieval (PIR) into DNS to enable encrypted query\\nprocessing without relying on trust assumptions. However, applying PIR to DNS\\nis challenging due to its hierarchical nature -- particularly, interactions\\nwith recursive resolvers can still leak information. Navigating performance and\\nprivacy trade-offs, we propose PDNS, a DNS extension leveraging single-server\\nPIR to strengthen privacy guarantees. We have implemented a prototype of PDNS\\nand compared its performance against state-of-the-art solutions via\\ntrace-driven experiments. The results show that PDNS achieves acceptable\\nperformance (2x faster than DoH over Tor with similar privacy guarantees) and\\nstrong privacy guarantees today, mainly at the cost of its scalability, which\\nspecialized hardware for PIR can address in the near future.\", comment=None, journal_ref=None, doi=None, primary_category='cs.NI', categories=['cs.NI', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.20806v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.20806v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.17491v2', updated=datetime.datetime(2025, 7, 28, 4, 24, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 23, 13, 18, 44, tzinfo=datetime.timezone.utc), title='Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement', authors=[arxiv.Result.Author('Nazatul H. Sultan'), arxiv.Result.Author('Xinlong Guan'), arxiv.Result.Author('Josef Pieprzyk'), arxiv.Result.Author('Wei Ni'), arxiv.Result.Author('Sharif Abuadbba'), arxiv.Result.Author('Hajime Suzuki')], summary=\"As 5G networks expand into critical infrastructure, secure and efficient user\\nauthentication is more important than ever. The 5G-AKA protocol, standardized\\nby 3GPP in TS 33.501, is central to authentication in current 5G deployments.\\nIt provides mutual authentication, user privacy, and key secrecy. However,\\ndespite its adoption, 5G-AKA has known limitations in both security and\\nperformance. While it focuses on protecting privacy against passive attackers,\\nrecent studies show its vulnerabilities to active attacks. It also relies on a\\nsequence number mechanism to prevent replay attacks, requiring perfect\\nsynchronization between the device and the core network. This stateful design\\nadds complexity, causes desynchronization, and incurs extra communication\\noverhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing\\npast communications if long-term keys are compromised-an increasing concern\\namid sophisticated threats. This paper proposes an enhanced authentication\\nprotocol that builds on 5G-AKA's design while addressing its shortcomings.\\nFirst, we introduce a stateless version that removes sequence number reliance,\\nreducing complexity while staying compatible with existing SIM cards and\\ninfrastructure. We then extend this design to add PFS with minimal\\ncryptographic overhead. Both protocols are rigorously analyzed using ProVerif,\\nconfirming their compliance with all major security requirements, including\\nresistance to passive and active attacks, as well as those defined by 3GPP and\\nacademic studies. We also prototype both protocols and evaluate their\\nperformance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the\\nproposed protocols offer stronger security with only minor computational\\noverhead, making them practical, future-ready solutions for 5G and beyond.\", comment='Accepted at RAID 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.NI', '68M25', 'C.2.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.17491v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.17491v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.15460v3', updated=datetime.datetime(2025, 7, 23, 3, 40, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 21, 10, 14, tzinfo=datetime.timezone.utc), title='Privacy-Preserving Multimodal News Recommendation through Federated Learning', authors=[arxiv.Result.Author('Mehdi Khalaj'), arxiv.Result.Author('Shahrzad Golestani Najafabadi'), arxiv.Result.Author('Julita Vassileva')], summary=\"Personalized News Recommendation systems (PNR) have emerged as a solution to\\ninformation overload by predicting and suggesting news items tailored to\\nindividual user interests. However, traditional PNR systems face several\\nchallenges, including an overreliance on textual content, common neglect of\\nshort-term user interests, and significant privacy concerns due to centralized\\ndata storage. This paper addresses these issues by introducing a novel\\nmultimodal federated learning-based approach for news recommendation. First, it\\nintegrates both textual and visual features of news items using a multimodal\\nmodel, enabling a more comprehensive representation of content. Second, it\\nemploys a time-aware model that balances users' long-term and short-term\\ninterests through multi-head self-attention networks, improving recommendation\\naccuracy. Finally, to enhance privacy, a federated learning framework is\\nimplemented, enabling collaborative model training without sharing user data.\\nThe framework divides the recommendation model into a large server-maintained\\nnews model and a lightweight user model shared between the server and clients.\\nThe client requests news representations (vectors) and a user model from the\\ncentral server, then computes gradients with user local data, and finally sends\\ntheir locally computed gradients to the server for aggregation. The central\\nserver aggregates gradients to update the global user model and news model. The\\nupdated news model is further used to infer news representation by the server.\\nTo further safeguard user privacy, a secure aggregation algorithm based on\\nShamir's secret sharing is employed. Experiments on a real-world news dataset\\ndemonstrate strong performance compared to existing systems, representing a\\nsignificant advancement in privacy-preserving personalized news recommendation.\", comment=None, journal_ref=None, doi=None, primary_category='cs.SI', categories=['cs.SI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.15460v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.15460v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.13575v3', updated=datetime.datetime(2025, 8, 27, 16, 34, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 17, 23, 37, 19, tzinfo=datetime.timezone.utc), title='Apple Intelligence Foundation Language Models: Tech Report 2025', authors=[arxiv.Result.Author('Ethan Li'), arxiv.Result.Author('Anders Boesen Lindbo Larsen'), arxiv.Result.Author('Chen Zhang'), arxiv.Result.Author('Xiyou Zhou'), arxiv.Result.Author('Jun Qin'), arxiv.Result.Author('Dian Ang Yap'), arxiv.Result.Author('Narendran Raghavan'), arxiv.Result.Author('Xuankai Chang'), arxiv.Result.Author('Margit Bowler'), arxiv.Result.Author('Eray Yildiz'), arxiv.Result.Author('John Peebles'), arxiv.Result.Author('Hannah Gillis Coleman'), arxiv.Result.Author('Matteo Ronchi'), arxiv.Result.Author('Peter Gray'), arxiv.Result.Author('Keen You'), arxiv.Result.Author('Anthony Spalvieri-Kruse'), arxiv.Result.Author('Ruoming Pang'), arxiv.Result.Author('Reed Li'), arxiv.Result.Author('Yuli Yang'), arxiv.Result.Author('Emad Soroush'), arxiv.Result.Author('Zhiyun Lu'), arxiv.Result.Author('Crystal Xiao'), arxiv.Result.Author('Rong Situ'), arxiv.Result.Author('Jordan Huffaker'), arxiv.Result.Author('David Griffiths'), arxiv.Result.Author('Zaid Ahmed'), arxiv.Result.Author('Peng Zhang'), arxiv.Result.Author('Daniel Parilla'), arxiv.Result.Author('Asaf Liberman'), arxiv.Result.Author('Jennifer Mallalieu'), arxiv.Result.Author('Parsa Mazaheri'), arxiv.Result.Author('Qibin Chen'), arxiv.Result.Author('Manjot Bilkhu'), arxiv.Result.Author('Aonan Zhang'), arxiv.Result.Author('Eric Wang'), arxiv.Result.Author('Dave Nelson'), arxiv.Result.Author('Michael FitzMaurice'), arxiv.Result.Author('Thomas Voice'), arxiv.Result.Author('Jeremy Liu'), arxiv.Result.Author('Josh Shaffer'), arxiv.Result.Author('Shiwen Zhao'), arxiv.Result.Author('Prasanth Yadla'), arxiv.Result.Author('Farzin Rasteh'), arxiv.Result.Author('Pengsheng Guo'), arxiv.Result.Author('Arsalan Farooq'), arxiv.Result.Author('Jeremy Snow'), arxiv.Result.Author('Stephen Murphy'), arxiv.Result.Author('Tao Lei'), arxiv.Result.Author('Minsik Cho'), arxiv.Result.Author('George Horrell'), arxiv.Result.Author('Sam Dodge'), arxiv.Result.Author('Lindsay Hislop'), arxiv.Result.Author('Sumeet Singh'), arxiv.Result.Author('Alex Dombrowski'), arxiv.Result.Author('Aiswarya Raghavan'), arxiv.Result.Author('Sasha Sirovica'), arxiv.Result.Author('Mandana Saebi'), arxiv.Result.Author('Faye Lao'), arxiv.Result.Author('Max Lam'), arxiv.Result.Author('TJ Lu'), arxiv.Result.Author('Zhaoyang Xu'), arxiv.Result.Author('Karanjeet Singh'), arxiv.Result.Author('Marc Kirchner'), arxiv.Result.Author('David Mizrahi'), arxiv.Result.Author('Rajat Arora'), arxiv.Result.Author('Haotian Zhang'), arxiv.Result.Author('Henry Mason'), arxiv.Result.Author('Lawrence Zhou'), arxiv.Result.Author('Yi Hua'), arxiv.Result.Author('Ankur Jain'), arxiv.Result.Author('Felix Bai'), arxiv.Result.Author('Joseph Astrauskas'), arxiv.Result.Author('Floris Weers'), arxiv.Result.Author('Josh Gardner'), arxiv.Result.Author('Mira Chiang'), arxiv.Result.Author('Yi Zhang'), arxiv.Result.Author('Pulkit Agrawal'), arxiv.Result.Author('Tony Sun'), arxiv.Result.Author('Quentin Keunebroek'), arxiv.Result.Author('Matthew Hopkins'), arxiv.Result.Author('Bugu Wu'), arxiv.Result.Author('Tao Jia'), arxiv.Result.Author('Chen Chen'), arxiv.Result.Author('Xingyu Zhou'), arxiv.Result.Author('Nanzhu Wang'), arxiv.Result.Author('Peng Liu'), arxiv.Result.Author('Ruixuan Hou'), arxiv.Result.Author('Rene Rauch'), arxiv.Result.Author('Yuan Gao'), arxiv.Result.Author('Afshin Dehghan'), arxiv.Result.Author('Jonathan Janke'), arxiv.Result.Author('Zirui Wang'), arxiv.Result.Author('Cha Chen'), arxiv.Result.Author('Xiaoyi Ren'), arxiv.Result.Author('Feng Nan'), arxiv.Result.Author('Josh Elman'), arxiv.Result.Author('Dong Yin'), arxiv.Result.Author('Yusuf Goren'), arxiv.Result.Author('Jeff Lai'), arxiv.Result.Author('Yiran Fei'), arxiv.Result.Author('Syd Evans'), arxiv.Result.Author('Muyang Yu'), arxiv.Result.Author('Guoli Yin'), arxiv.Result.Author('Yi Qin'), arxiv.Result.Author('Erin Feldman'), arxiv.Result.Author('Isha Garg'), arxiv.Result.Author('Aparna Rajamani'), arxiv.Result.Author('Karla Vega'), arxiv.Result.Author('Walker Cheng'), arxiv.Result.Author('TJ Collins'), arxiv.Result.Author('Hans Han'), arxiv.Result.Author('Raul Rea Menacho'), arxiv.Result.Author('Simon Yeung'), arxiv.Result.Author('Sophy Lee'), arxiv.Result.Author('Phani Mutyala'), arxiv.Result.Author('Ying-Chang Cheng'), arxiv.Result.Author('Zhe Gan'), arxiv.Result.Author('Sprite Chu'), arxiv.Result.Author('Justin Lazarow'), arxiv.Result.Author('Alessandro Pappalardo'), arxiv.Result.Author('Federico Scozzafava'), arxiv.Result.Author('Jing Lu'), arxiv.Result.Author('Erik Daxberger'), arxiv.Result.Author('Laurent Duchesne'), arxiv.Result.Author('Jen Liu'), arxiv.Result.Author('David Güera'), arxiv.Result.Author('Stefano Ligas'), arxiv.Result.Author('Mary Beth Kery'), arxiv.Result.Author('Brent Ramerth'), arxiv.Result.Author('Ciro Sannino'), arxiv.Result.Author('Marcin Eichner'), arxiv.Result.Author('Haoshuo Huang'), arxiv.Result.Author('Rui Qian'), arxiv.Result.Author('Moritz Schwarzer-Becker'), arxiv.Result.Author('David Riazati'), arxiv.Result.Author('Mingfei Gao'), arxiv.Result.Author('Bailin Wang'), arxiv.Result.Author('Jack Cackler'), arxiv.Result.Author('Yang Lu'), arxiv.Result.Author('Ransen Niu'), arxiv.Result.Author('John Dennison'), arxiv.Result.Author('Guillaume Klein'), arxiv.Result.Author('Jeffrey Bigham'), arxiv.Result.Author('Deepak Gopinath'), arxiv.Result.Author('Navid Shiee'), arxiv.Result.Author('Darren Botten'), arxiv.Result.Author('Guillaume Tartavel'), arxiv.Result.Author('Alex Guillen Garcia'), arxiv.Result.Author('Sam Xu'), arxiv.Result.Author('Victoria MönchJuan Haladjian'), arxiv.Result.Author('Zi-Yi Dou'), arxiv.Result.Author('Matthias Paulik'), arxiv.Result.Author('Adolfo Lopez Mendez'), arxiv.Result.Author('Zhen Li'), arxiv.Result.Author('Hong-You Chen'), arxiv.Result.Author('Chao Jia'), arxiv.Result.Author('Dhaval Doshi'), arxiv.Result.Author('Zhengdong Zhang'), arxiv.Result.Author('Raunak Manjani'), arxiv.Result.Author('Aaron Franklin'), arxiv.Result.Author('Zhile Ren'), arxiv.Result.Author('David Chen'), arxiv.Result.Author('Artsiom Peshko'), arxiv.Result.Author('Nandhitha Raghuram'), arxiv.Result.Author('Hans Hao'), arxiv.Result.Author('Jiulong Shan'), arxiv.Result.Author('Kavya Nerella'), arxiv.Result.Author('Ramsey Tantawi'), arxiv.Result.Author('Vivek Kumar'), arxiv.Result.Author('Saiwen Wang'), arxiv.Result.Author('Brycen Wershing'), arxiv.Result.Author('Bhuwan Dhingra'), arxiv.Result.Author('Dhruti Shah'), arxiv.Result.Author('Ob Adaranijo'), arxiv.Result.Author('Xin Zheng'), arxiv.Result.Author('Tait Madsen'), arxiv.Result.Author('Hadas Kotek'), arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Yin Xia'), arxiv.Result.Author('Hanli Li'), arxiv.Result.Author('Suma Jayaram'), arxiv.Result.Author('Yanchao Sun'), arxiv.Result.Author('Ahmed Fakhry'), arxiv.Result.Author('Vasileios Saveris'), arxiv.Result.Author('Dustin Withers'), arxiv.Result.Author('Yanghao Li'), arxiv.Result.Author('Alp Aygar'), arxiv.Result.Author('Andres Romero Mier Y Teran'), arxiv.Result.Author('Kaiwei Huang'), arxiv.Result.Author('Mark Lee'), arxiv.Result.Author('Xiujun Li'), arxiv.Result.Author('Yuhong Li'), arxiv.Result.Author('Tyler Johnson'), arxiv.Result.Author('Jay Tang'), arxiv.Result.Author('Joseph Yitan Cheng'), arxiv.Result.Author('Futang Peng'), arxiv.Result.Author('Andrew Walkingshaw'), arxiv.Result.Author('Lucas Guibert'), arxiv.Result.Author('Abhishek Sharma'), arxiv.Result.Author('Cheng Shen'), arxiv.Result.Author('Piotr Maj'), arxiv.Result.Author('Yasutaka Tanaka'), arxiv.Result.Author('You-Cyuan Jhang'), arxiv.Result.Author('Vivian Ma'), arxiv.Result.Author('Tommi Vehvilainen'), arxiv.Result.Author('Kelvin Zou'), arxiv.Result.Author('Jeff Nichols'), arxiv.Result.Author('Matthew Lei'), arxiv.Result.Author('David Qiu'), arxiv.Result.Author('Yihao Qian'), arxiv.Result.Author('Gokul Santhanam'), arxiv.Result.Author('Wentao Wu'), arxiv.Result.Author('Yena Han'), arxiv.Result.Author('Dominik Moritz'), arxiv.Result.Author('Haijing Fu'), arxiv.Result.Author('Mingze Xu'), arxiv.Result.Author('Vivek Rathod'), arxiv.Result.Author('Jian Liu'), arxiv.Result.Author(\"Louis D'hauwe\"), arxiv.Result.Author('Qin Ba'), arxiv.Result.Author('Haitian Sun'), arxiv.Result.Author('Haoran Yan'), arxiv.Result.Author('Philipp Dufter'), arxiv.Result.Author('Anh Nguyen'), arxiv.Result.Author('Yihao Feng'), arxiv.Result.Author('Emma Wang'), arxiv.Result.Author('Keyu He'), arxiv.Result.Author('Rahul Nair'), arxiv.Result.Author('Sanskruti Shah'), arxiv.Result.Author('Jiarui Lu'), arxiv.Result.Author('Patrick Sonnenberg'), arxiv.Result.Author('Jeremy Warner'), arxiv.Result.Author('Yuanzhi Li'), arxiv.Result.Author('Bowen Pan'), arxiv.Result.Author('Ziyi Zhong'), arxiv.Result.Author('Joe Zhou'), arxiv.Result.Author('Sam Davarnia'), arxiv.Result.Author('Olli Saarikivi'), arxiv.Result.Author('Irina Belousova'), arxiv.Result.Author('Rachel Burger'), arxiv.Result.Author('Shang-Chen Wu'), arxiv.Result.Author('Di Feng'), arxiv.Result.Author('Bas Straathof'), arxiv.Result.Author('James Chou'), arxiv.Result.Author('Yuanyang Zhang'), arxiv.Result.Author('Marco Zuliani'), arxiv.Result.Author('Eduardo Jimenez'), arxiv.Result.Author('Abhishek Sundararajan'), arxiv.Result.Author('Xianzhi Du'), arxiv.Result.Author('Chang Lan'), arxiv.Result.Author('Nilesh Shahdadpuri'), arxiv.Result.Author('Peter Grasch'), arxiv.Result.Author('Sergiu Sima'), arxiv.Result.Author('Josh Newnham'), arxiv.Result.Author('Varsha Paidi'), arxiv.Result.Author('Jianyu Wang'), arxiv.Result.Author('Kaelen Haag'), arxiv.Result.Author('Alex Braunstein'), arxiv.Result.Author('Daniele Molinari'), arxiv.Result.Author('Richard Wei'), arxiv.Result.Author('Brenda Yang'), arxiv.Result.Author('Nicholas Lusskin'), arxiv.Result.Author('Joanna Arreaza-Taylor'), arxiv.Result.Author('Meng Cao'), arxiv.Result.Author('Nicholas Seidl'), arxiv.Result.Author('Simon Wang'), arxiv.Result.Author('Jiaming Hu'), arxiv.Result.Author('Yiping Ma'), arxiv.Result.Author('Mengyu Li'), arxiv.Result.Author('Kieran Liu'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Sachin Ravi'), arxiv.Result.Author('Chong Wang'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Kevin Smith'), arxiv.Result.Author('Haoxuan You'), arxiv.Result.Author('Binazir Karimzadeh'), arxiv.Result.Author('Rui Li'), arxiv.Result.Author('Jinhao Lei'), arxiv.Result.Author('Wei Fang'), arxiv.Result.Author('Alec Doane'), arxiv.Result.Author('Sam Wiseman'), arxiv.Result.Author('Ismael Fernandez'), arxiv.Result.Author('Jane Li'), arxiv.Result.Author('Andrew Hansen'), arxiv.Result.Author('Javier Movellan'), arxiv.Result.Author('Christopher Neubauer'), arxiv.Result.Author('Hanzhi Zhou'), arxiv.Result.Author('Chris Chaney'), arxiv.Result.Author('Nazir Kamaldin'), arxiv.Result.Author('Valentin Wolf'), arxiv.Result.Author('Fernando Bermúdez-Medina'), arxiv.Result.Author('Joris Pelemans'), arxiv.Result.Author('Peter Fu'), arxiv.Result.Author('Howard Xing'), arxiv.Result.Author('Xiang Kong'), arxiv.Result.Author('Wayne Shan'), arxiv.Result.Author('Gabriel Jacoby-Cooper'), arxiv.Result.Author('Dongcai Shen'), arxiv.Result.Author('Tom Gunter'), arxiv.Result.Author('Guillaume Seguin'), arxiv.Result.Author('Fangping Shi'), arxiv.Result.Author('Shiyu Li'), arxiv.Result.Author('Yang Xu'), arxiv.Result.Author('Areeba Kamal'), arxiv.Result.Author('Dan Masi'), arxiv.Result.Author('Saptarshi Guha'), arxiv.Result.Author('Qi Zhu'), arxiv.Result.Author('Jenna Thibodeau'), arxiv.Result.Author('Changyuan Zhang'), arxiv.Result.Author('Rebecca Callahan'), arxiv.Result.Author('Charles Maalouf'), arxiv.Result.Author('Wilson Tsao'), arxiv.Result.Author('Boyue Li'), arxiv.Result.Author('Qingqing Cao'), arxiv.Result.Author('Naomy Sabo'), arxiv.Result.Author('Cheng Leong'), arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Anupama Mann Anupama'), arxiv.Result.Author('Colorado Reed'), arxiv.Result.Author('Kenneth Jung'), arxiv.Result.Author('Zhifeng Chen'), arxiv.Result.Author('Mohana Prasad Sathya Moorthy'), arxiv.Result.Author('Yifei He'), arxiv.Result.Author('Erik Hornberger'), arxiv.Result.Author('Devi Krishna'), arxiv.Result.Author('Senyu Tong'), arxiv.Result.Author('Michael'), arxiv.Result.Author('Lee'), arxiv.Result.Author('David Haldimann'), arxiv.Result.Author('Yang Zhao'), arxiv.Result.Author('Bowen Zhang'), arxiv.Result.Author('Chang Gao'), arxiv.Result.Author('Chris Bartels'), arxiv.Result.Author('Sushma Rao'), arxiv.Result.Author('Nathalie Tran'), arxiv.Result.Author('Simon Lehnerer'), arxiv.Result.Author('Co Giang'), arxiv.Result.Author('Patrick Dong'), arxiv.Result.Author('Junting Pan'), arxiv.Result.Author('Biyao Wang'), arxiv.Result.Author('Dongxu Li'), arxiv.Result.Author('Mehrdad Farajtabar'), arxiv.Result.Author('Dongseong Hwang'), arxiv.Result.Author('Grace Duanmu'), arxiv.Result.Author('Eshan Verma'), arxiv.Result.Author('Sujeeth Reddy'), arxiv.Result.Author('Qi Shan'), arxiv.Result.Author('Hongbin Gao'), arxiv.Result.Author('Nan Du'), arxiv.Result.Author('Pragnya Sridhar'), arxiv.Result.Author('Forrest Huang'), arxiv.Result.Author('Yingbo Wang'), arxiv.Result.Author('Nikhil Bhendawade'), arxiv.Result.Author('Diane Zhu'), arxiv.Result.Author('Sai Aitharaju'), arxiv.Result.Author('Fred Hohman'), arxiv.Result.Author('Lauren Gardiner'), arxiv.Result.Author('Chung-Cheng Chiu'), arxiv.Result.Author('Yinfei Yang'), arxiv.Result.Author('Alper Kokmen'), arxiv.Result.Author('Frank Chu'), arxiv.Result.Author('Ke Ye'), arxiv.Result.Author('Kaan Elgin'), arxiv.Result.Author('Oron Levy'), arxiv.Result.Author('John Park'), arxiv.Result.Author('Donald Zhang'), arxiv.Result.Author('Eldon Schoop'), arxiv.Result.Author('Nina Wenzel'), arxiv.Result.Author('Michael Booker'), arxiv.Result.Author('Hyunjik Kim'), arxiv.Result.Author('Chinguun Erdenebileg'), arxiv.Result.Author('Nan Dun'), arxiv.Result.Author('Eric Liang Yang'), arxiv.Result.Author('Priyal Chhatrapati'), arxiv.Result.Author('Vishaal Mahtani'), arxiv.Result.Author('Haiming Gang'), arxiv.Result.Author('Kohen Chia'), arxiv.Result.Author('Deepa Seshadri'), arxiv.Result.Author('Donghan Yu'), arxiv.Result.Author('Yan Meng'), arxiv.Result.Author('Kelsey Peterson'), arxiv.Result.Author('Zhen Yang'), arxiv.Result.Author('Yongqiang Wang'), arxiv.Result.Author('Carina Peng'), arxiv.Result.Author('Doug Kang'), arxiv.Result.Author('Anuva Agarwal'), arxiv.Result.Author('Albert Antony'), arxiv.Result.Author('Juan Lao Tebar'), arxiv.Result.Author('Albin Madappally Jose'), arxiv.Result.Author('Regan Poston'), arxiv.Result.Author('Andy De Wang'), arxiv.Result.Author('Gerard Casamayor'), arxiv.Result.Author('Elmira Amirloo'), arxiv.Result.Author('Violet Yao'), arxiv.Result.Author('Wojciech Kryscinski'), arxiv.Result.Author('Kun Duan'), arxiv.Result.Author('Lezhi L')], summary=\"We introduce two multilingual, multimodal foundation language models that\\npower Apple Intelligence features across Apple devices and services: i a\\n3B-parameter on-device model optimized for Apple silicon through architectural\\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\\ncomputation, and interleaved global-local attention to deliver high quality\\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\\nare trained on large-scale multilingual and multimodal datasets sourced via\\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\\nthen further refined with supervised fine-tuning and reinforcement learning on\\na new asynchronous platform. The resulting models support several additional\\nlanguages while understanding images and executing tool calls. In public\\nbenchmarks and human evaluations, both the server model and the on-device model\\nmatch or surpass comparably sized open baselines.\\n  A new Swift-centric Foundation Models framework exposes guided generation,\\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\\nintegrate these capabilities with a few lines of code. The latest advancements\\nin Apple Intelligence models are grounded in our Responsible AI approach with\\nsafeguards like content filtering and locale-specific evaluation, as well as\\nour commitment to protecting our users' privacy with innovations like Private\\nCloud Compute.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.13575v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.13575v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.12050v1', updated=datetime.datetime(2025, 7, 16, 9, 10, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 16, 9, 10, 40, tzinfo=datetime.timezone.utc), title='IDFace: Face Template Protection for Efficient and Secure Identification', authors=[arxiv.Result.Author('Sunpill Kim'), arxiv.Result.Author('Seunghun Paik'), arxiv.Result.Author('Chanwoo Hwang'), arxiv.Result.Author('Dongsoo Kim'), arxiv.Result.Author('Junbum Shin'), arxiv.Result.Author('Jae Hong Seo')], summary=\"As face recognition systems (FRS) become more widely used, user privacy\\nbecomes more important. A key privacy issue in FRS is protecting the user's\\nface template, as the characteristics of the user's face image can be recovered\\nfrom the template. Although recent advances in cryptographic tools such as\\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\\nparticular, although HE is functionally complete for arbitrary programs, it is\\nbasically designed for algebraic operations on encrypted data of predetermined\\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\\nthe system can yield very inefficient performance, and many previous HE-based\\nface template protection methods are hundreds of times slower than plain\\nsystems without protection. In this study, we propose IDFace, a new HE-based\\nsecure and efficient face identification method with template protection.\\nIDFace is designed on the basis of two novel techniques for efficient searching\\non a (homomorphically encrypted) biometric database with an angular metric. The\\nfirst technique is a template representation transformation that sharply\\nreduces the unit cost for the matching test. The second is a space-efficient\\nencoding that reduces wasted space from the encryption algorithm, thus saving\\nthe number of operations on encrypted templates. Through experiments, we show\\nthat IDFace can identify a face template from among a database of 1M encrypted\\ntemplates in 126ms, showing only 2X overhead compared to the identification\\nover plaintexts.\", comment='Accepted to ICCV 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.CV', 'I.5.4; K.6.5; D.4.6; I.4.7'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.12050v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.12050v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.10695v1', updated=datetime.datetime(2025, 7, 14, 18, 10, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 14, 18, 10, 21, tzinfo=datetime.timezone.utc), title='Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health', authors=[arxiv.Result.Author('Jabari Kwesi'), arxiv.Result.Author('Jiaxun Cao'), arxiv.Result.Author('Riya Manchanda'), arxiv.Result.Author('Pardis Emami-Naeini')], summary='Individuals are increasingly relying on large language model (LLM)-enabled\\nconversational agents for emotional support. While prior research has examined\\nprivacy and security issues in chatbots specifically designed for mental health\\npurposes, these chatbots are overwhelmingly \"rule-based\" offerings that do not\\nleverage generative AI. Little empirical research currently measures users\\'\\nprivacy and security concerns, attitudes, and expectations when using\\ngeneral-purpose LLM-enabled chatbots to manage and improve mental health.\\nThrough 21 semi-structured interviews with U.S. participants, we identified\\ncritical misconceptions and a general lack of risk awareness. Participants\\nconflated the human-like empathy exhibited by LLMs with human-like\\naccountability and mistakenly believed that their interactions with these\\nchatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures\\nwith a licensed therapist. We introduce the concept of \"intangible\\nvulnerability,\" where emotional or psychological disclosures are undervalued\\ncompared to more tangible forms of information (e.g., financial or\\nlocation-based data). To address this, we propose recommendations to safeguard\\nuser mental health disclosures with general-purpose LLM-enabled chatbots more\\neffectively.', comment='Accepted to the 34th USENIX Security Symposium', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.AI', 'cs.CR', 'cs.ET', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.10695v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.10695v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.10640v1', updated=datetime.datetime(2025, 7, 14, 14, 58, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 14, 14, 58, 4, tzinfo=datetime.timezone.utc), title='SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications', authors=[arxiv.Result.Author('Labiba Farah'), arxiv.Result.Author('Mohammad Ridwan Kabir'), arxiv.Result.Author('Shohel Ahmed'), arxiv.Result.Author('MD Mohaymen Ul Anam'), arxiv.Result.Author('Md. Sakibul Islam')], summary=\"The widespread use of social media applications has raised significant\\nprivacy concerns, often highlighted in user reviews. These reviews also provide\\ndevelopers with valuable insights into improving apps by addressing issues and\\nintroducing better features. However, the sheer volume and nuanced nature of\\nreviews make manual identification and prioritization of privacy-related\\nconcerns challenging for developers. Previous studies have developed software\\nutilities to automatically classify user reviews as privacy-relevant,\\nprivacy-irrelevant, bug reports, feature requests, etc., using machine\\nlearning. Notably, there is a lack of focus on classifying reviews specifically\\nas privacy-related feature requests, privacy-related bug reports, or\\nprivacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated\\nonline annotation tool designed to help developers annotate and classify user\\nreviews into these categories. For automating the annotation of such reviews,\\nthis paper introduces the annotation model, GRACE (GRU-based Attention with\\nCBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words\\n(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven\\npopular social media apps on Google Play Store, including Instagram, Facebook,\\nWhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were\\nanalyzed. Two annotators manually labelled the reviews, achieving a Cohen's\\nKappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement\\nfor training machine learning models. Among the models tested, GRACE\\ndemonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:\\n0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates\\nsignificant potential to assist developers with extracting and addressing\\nprivacy-related feature requests or bug reports from user reviews, enhancing\\nuser privacy and trust.\", comment='26 pages, 9 figures, 5 tables', journal_ref=None, doi=None, primary_category='cs.SE', categories=['cs.SE', 'cs.LG', 'cs.SI', 'D.2.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.10640v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.10640v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.10194v1', updated=datetime.datetime(2025, 7, 14, 12, 1, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 14, 12, 1, 8, tzinfo=datetime.timezone.utc), title='Learning Private Representations through Entropy-based Adversarial Training', authors=[arxiv.Result.Author('Tassilo Klein'), arxiv.Result.Author('Moin Nabi')], summary='How can we learn a representation with high predictive power while preserving\\nuser privacy? We present an adversarial representation learning method for\\nsanitizing sensitive content from the learned representation. Specifically, we\\nintroduce a variant of entropy - focal entropy, which mitigates the potential\\ninformation leakage of the existing entropy-based approaches. We showcase\\nfeasibility on multiple benchmarks. The results suggest high target utility at\\nmoderate privacy leakage.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.10194v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.10194v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.10029v2', updated=datetime.datetime(2025, 9, 1, 5, 19, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 14, 8, 8, 55, tzinfo=datetime.timezone.utc), title='Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies', authors=[arxiv.Result.Author('Seokeon Choi'), arxiv.Result.Author('Sunghyun Park'), arxiv.Result.Author('Hyoungwoo Park'), arxiv.Result.Author('Jeongho Kim'), arxiv.Result.Author('Sungrack Yun')], summary='Memory-efficient personalization is critical for adapting text-to-image\\ndiffusion models while preserving user privacy and operating within the limited\\ncomputational resources of edge devices. To this end, we propose a selective\\noptimization framework that adaptively chooses between backpropagation on\\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\\nimages (ZO-high), guided by the characteristics of the diffusion process. As\\nobserved in our experiments, BP-low efficiently adapts the model to\\ntarget-specific features, but suffers from structural distortions due to\\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\\nminimal memory overhead but faces slow convergence when applied without prior\\nadaptation. By complementing both methods, our framework leverages BP-low for\\neffective personalization while using ZO-high to maintain structural\\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\\nprobabilistic function that dynamically selects the appropriate optimization\\nstrategy based on diffusion timesteps. This function mitigates the overfitting\\nfrom BP-low at high timesteps, where structural information is critical, while\\nensuring ZO-high is applied more effectively as training progresses.\\nExperimental results demonstrate that our method achieves competitive\\nperformance while significantly reducing memory consumption, enabling scalable,\\nhigh-quality on-device personalization without increasing inference latency.', comment='Accepted to ICCV 2025 LIMIT Workshop (4-page short paper). Extended\\n  version in preparation', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.10029v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.10029v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.14181v1', updated=datetime.datetime(2025, 7, 12, 10, 54, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 12, 10, 54, 23, tzinfo=datetime.timezone.utc), title='Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis', authors=[arxiv.Result.Author('Yajiao Dai'), arxiv.Result.Author('Jun Li'), arxiv.Result.Author('Zhen Mei'), arxiv.Result.Author('Yiyang Ni'), arxiv.Result.Author('Shi Jin'), arxiv.Result.Author('Zengxiang Li'), arxiv.Result.Author('Sheng Guo'), arxiv.Result.Author('Wei Xiang')], summary=\"Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe\\noperation of industrial machinery and improving production efficiency. However,\\ntraditional supervised deep learning methods require a large amount of training\\ndata and labels, which are often located in different clients. Additionally,\\nthe cost of data labeling is high, making labels difficult to acquire.\\nMeanwhile, differences in data distribution among clients may also hinder the\\nmodel's performance. To tackle these challenges, this paper proposes a\\nsemi-supervised federated learning framework, SSFL-DCSL, which integrates dual\\ncontrastive loss and soft labeling to address data and label scarcity for\\ndistributed clients with few labeled samples while safeguarding user privacy.\\nIt enables representation learning using unlabeled data on the client side and\\nfacilitates joint learning among clients through prototypes, thereby achieving\\nmutual knowledge sharing and preventing local model divergence. Specifically,\\nfirst, a sample weighting function based on the Laplace distribution is\\ndesigned to alleviate bias caused by low confidence in pseudo labels during the\\nsemi-supervised training process. Second, a dual contrastive loss is introduced\\nto mitigate model divergence caused by different data distributions, comprising\\nlocal contrastive loss and global contrastive loss. Third, local prototypes are\\naggregated on the server with weighted averaging and updated with momentum to\\nshare knowledge among clients. To evaluate the proposed SSFL-DCSL framework,\\nexperiments are conducted on two publicly available datasets and a dataset\\ncollected on motors from the factory. In the most challenging task, where only\\n10\\\\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by\\n1.15% to 7.85% over state-of-the-art methods.\", comment='Accepted to IEEE Internet of Things Journal, Early Access. 14 pages,\\n  5 figures', journal_ref='IEEE Internet of Things Journal, Early Access, 2025', doi='10.1109/JIOT.2025.3586718', primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/JIOT.2025.3586718', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2507.14181v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.14181v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.09231v1', updated=datetime.datetime(2025, 7, 12, 10, 0, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 12, 10, 0, 50, tzinfo=datetime.timezone.utc), title='Confidential Wrapped Ethereum', authors=[arxiv.Result.Author('Artem Chystiakov'), arxiv.Result.Author('Mariia Zhvanko')], summary=\"Transparency is one of the key benefits of public blockchains. However, the\\npublic visibility of transactions potentially compromises users' privacy. The\\nfundamental challenge is to balance the intrinsic benefits of blockchain\\nopenness with the vital need for individual confidentiality. The proposal\\nsuggests creating a confidential version of wrapped Ethereum (cWETH) fully\\nwithin the application layer. The solution combines the Elliptic Curve (EC)\\nTwisted ElGamal-based commitment scheme to preserve confidentiality and the EC\\nDiffie-Hellman (DH) protocol to introduce accessibility limited by the\\ncommitment scheme. To enforce the correct generation of commitments,\\nencryption, and decryption, zk-SNARKs are utilized.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.09231v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.09231v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.08982v1', updated=datetime.datetime(2025, 7, 11, 19, 34, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 11, 19, 34, 1, tzinfo=datetime.timezone.utc), title='VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models', authors=[arxiv.Result.Author('Hanene F. Z. Brachemi Meftah'), arxiv.Result.Author('Wassim Hamidouche'), arxiv.Result.Author('Sid Ahmed Fezza'), arxiv.Result.Author('Olivier Déforges')], summary='Recent years have witnessed remarkable progress in developing Vision-Language\\nModels (VLMs) capable of processing both textual and visual inputs. These\\nmodels have demonstrated impressive performance, leading to their widespread\\nadoption in various applications. However, this widespread raises serious\\nconcerns regarding user privacy, particularly when models inadvertently process\\nor expose private visual information. In this work, we frame the preservation\\nof privacy in VLMs as an adversarial attack problem. We propose a novel attack\\nstrategy that selectively conceals information within designated Region Of\\nInterests (ROIs) in an image, effectively preventing VLMs from accessing\\nsensitive content while preserving the semantic integrity of the remaining\\nimage. Unlike conventional adversarial attacks that often disrupt the entire\\nimage, our method maintains high coherence in unmasked areas. Experimental\\nresults across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and\\nBLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while\\nmaintaining global image semantics intact, as confirmed by high similarity\\nscores between clean and adversarial outputs. We believe that this work\\ncontributes to a more privacy conscious use of multimodal models and offers a\\npractical tool for further research, with the source code publicly available\\nat: https://github.com/hbrachemi/Vlm_defense-attack.', comment=None, journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.CV', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.08982v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.08982v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.08331v1', updated=datetime.datetime(2025, 7, 11, 6, 10, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 11, 6, 10, 15, tzinfo=datetime.timezone.utc), title='Qualcomm Trusted Application Emulation for Fuzzing Testing', authors=[arxiv.Result.Author('Chun-I Fan'), arxiv.Result.Author('Li-En Chang'), arxiv.Result.Author('Cheng-Han Shie')], summary='In recent years, the increasing awareness of cybersecurity has led to a\\nheightened focus on information security within hardware devices and products.\\nIncorporating Trusted Execution Environments (TEEs) into product designs has\\nbecome a standard practice for safeguarding sensitive user information.\\nHowever, vulnerabilities within these components present significant risks, if\\nexploited by attackers, these vulnerabilities could lead to the leakage of\\nsensitive data, thereby compromising user privacy and security. This research\\ncenters on trusted applications (TAs) within the Qualcomm TEE and introduces a\\nnovel emulator specifically designed for these applications. Through reverse\\nengineering techniques, we thoroughly analyze Qualcomm TAs and develop a\\npartial emulation environment that accurately emulates their behavior.\\nAdditionally, we integrate fuzzing testing techniques into the emulator to\\nsystematically uncover potential vulnerabilities within Qualcomm TAs,\\ndemonstrating its practical effectiveness in identifying real-world security\\nflaws. This research makes a significant contribution by being the first to\\nprovide both the implementation methods and source codes for a Qualcomm TAs\\nemulator, offering a valuable reference for future research efforts. Unlike\\nprevious approaches that relied on complex and resource-intensive full-system\\nsimulations, our approach is lightweight and effective, making security testing\\nof TA more convenient.', comment='This work is currently under review for presentation at the USENIX\\n  Security 2025 poster session', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.08331v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.08331v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.08878v1', updated=datetime.datetime(2025, 7, 10, 5, 36, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 10, 5, 36, 32, tzinfo=datetime.timezone.utc), title='Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models', authors=[arxiv.Result.Author('Xinyu Huang'), arxiv.Result.Author('Leming Shen'), arxiv.Result.Author('Zijing Ma'), arxiv.Result.Author('Yuanqing Zheng')], summary='Large Language Models (LLMs) have showcased remarkable generalizability in\\nlanguage comprehension and hold significant potential to revolutionize\\nhuman-computer interaction in smart homes. Existing LLM-based smart home\\nassistants typically transmit user commands, along with user profiles and home\\nconfigurations, to remote servers to obtain personalized services. However,\\nusers are increasingly concerned about the potential privacy leaks to the\\nremote servers. To address this issue, we develop HomeLLaMA, an on-device\\nassistant for privacy-preserving and personalized smart home serving with a\\ntailored small language model (SLM). HomeLLaMA learns from cloud LLMs to\\ndeliver satisfactory responses and enable user-friendly interactions. Once\\ndeployed, HomeLLaMA facilitates proactive interactions by continuously updating\\nlocal SLMs and user profiles. To further enhance user experience while\\nprotecting their privacy, we develop PrivShield to offer an optional\\nprivacy-preserving LLM-based smart home serving for those users, who are\\nunsatisfied with local responses and willing to send less-sensitive queries to\\nremote servers. For evaluation, we build a comprehensive benchmark DevFinder to\\nassess the service quality. Extensive experiments and user studies (M=100)\\ndemonstrate that HomeLLaMA can provide personalized services while\\nsignificantly enhancing user privacy.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.08878v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.08878v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.06258v1', updated=datetime.datetime(2025, 7, 7, 9, 40, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 7, 9, 40, 16, tzinfo=datetime.timezone.utc), title='Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems', authors=[arxiv.Result.Author('Bo Yan'), arxiv.Result.Author('Yurong Hao'), arxiv.Result.Author('Dingqi Liu'), arxiv.Result.Author('Huabin Sun'), arxiv.Result.Author('Pengpeng Qiao'), arxiv.Result.Author('Wei Yang Bryan Lim'), arxiv.Result.Author('Yang Cao'), arxiv.Result.Author('Chuan Shi')], summary=\"Federated recommender systems (FedRec) have emerged as a promising solution\\nfor delivering personalized recommendations while safeguarding user privacy.\\nHowever, recent studies have demonstrated their vulnerability to poisoning\\nattacks. Existing attacks typically target the entire user group, which\\ncompromises stealth and increases the risk of detection. In contrast,\\nreal-world adversaries may prefer to prompt target items to specific user\\nsubgroups, such as recommending health supplements to elderly users. Motivated\\nby this gap, we introduce Spattack, the first targeted poisoning attack\\ndesigned to manipulate recommendations for specific user subgroups in the\\nfederated setting. Specifically, Spattack adopts a two-stage\\napproximation-and-promotion strategy, which first simulates user embeddings of\\ntarget/non-target subgroups and then prompts target items to the target\\nsubgroups. To enhance the approximation stage, we push the inter-group\\nembeddings away based on contrastive learning and augment the target group's\\nrelevant item set based on clustering. To enhance the promotion stage, we\\nfurther propose to adaptively tune the optimization weights between target and\\nnon-target subgroups. Besides, an embedding alignment strategy is proposed to\\nalign the embeddings between the target items and the relevant items. We\\nconduct comprehensive experiments on three real-world datasets, comparing\\nSpattack against seven state-of-the-art poisoning attacks and seven\\nrepresentative defense mechanisms. Experimental results demonstrate that\\nSpattack consistently achieves strong manipulation performance on the specific\\nuser subgroup, while incurring minimal impact on non-target users, even when\\nonly 0.1\\\\% of users are malicious. Moreover, Spattack maintains competitive\\noverall recommendation performance and exhibits strong resilience against\\nexisting mainstream defenses.\", comment='13 pages', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.DC', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.06258v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.06258v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.05308v1', updated=datetime.datetime(2025, 7, 7, 9, 28, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 7, 9, 28, 49, tzinfo=datetime.timezone.utc), title='High Order Collaboration-Oriented Federated Graph Neural Network for Accurate QoS Prediction', authors=[arxiv.Result.Author('Zehuan Chen'), arxiv.Result.Author('Xiangwei Lai')], summary='Predicting Quality of Service (QoS) data crucial for cloud service selection,\\nwhere user privacy is a critical concern. Federated Graph Neural Networks\\n(FGNNs) can perform QoS data prediction as well as maintaining user privacy.\\nHowever, existing FGNN-based QoS predictors commonly implement on-device\\ntraining on scattered explicit user-service graphs, thereby failing to utilize\\nthe implicit user-user interactions. To address this issue, this study proposes\\na high order collaboration-oriented federated graph neural network (HC-FGNN) to\\nobtain accurate QoS prediction with privacy preservation. Concretely, it\\nmagnifies the explicit user-service graphs following the principle of attention\\nmechanism to obtain the high order collaboration, which reflects the implicit\\nuser-user interactions. Moreover, it utilizes a lightweight-based message\\naggregation way to improve the computational efficiency. The extensive\\nexperiments on two QoS datasets from real application indicate that the\\nproposed HC-FGNN possesses the advantages of high prediction accurate and\\nprivacy protection.', comment=None, journal_ref=None, doi=None, primary_category='cs.DC', categories=['cs.DC', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.05308v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.05308v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.01635v1', updated=datetime.datetime(2025, 7, 2, 12, 7, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 7, 2, 12, 7, 3, tzinfo=datetime.timezone.utc), title='EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity Leakage in Blockchain', authors=[arxiv.Result.Author('Chenyu Li'), arxiv.Result.Author('Xueping Liang'), arxiv.Result.Author('Xiaorui Gong'), arxiv.Result.Author('Xiu Zhang')], summary=\"While Ethereum's discovery protocols (Discv4/ Discv5) incorporate robust\\ncryptographic designs to protect user privacy, real-world deployment reveals\\ncritical vulnerabilities when users deviate from security guidelines. In this\\npaper, we design a system called EGNInfoLeaker. Our study is the first work\\nthat uncovers widespread public key reuse across Ethereum's peer-to-peer\\nnetworks - a practice that fundamentally undermines the protocol's privacy\\nguarantees. Through systematic analysis of 300 real-world network snapshots, we\\nidentify 83 users controlling 483 service nodes via public key reuse, enabling\\nprecise de-anonymization through IP correlation. Using evidence collected by\\nEGNInfoLeaker, our Graph-Based Identity Association Algorithm links users to\\nnetwork entities and generates comprehensive user profiles. For User27, it\\nexposes the public key, IP, network ID, location (country/region/city), and\\nISP/ORG details. The EGNInfoLeaker system demonstrates how such cryptographic\\nmisuse transforms theoretical anonymity into practical identity leakage,\\nexposing users to surveillance and targeted attacks. These findings establish\\nthat protocol security depends not only on sound design but also on strict user\\ncompliance. Going forward, our detection framework provides a foundation for\\nenhancing real-world privacy preservation in decentralized networks.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.01635v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.01635v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2507.02966v2', updated=datetime.datetime(2025, 7, 9, 8, 2, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 30, 14, 42, 49, tzinfo=datetime.timezone.utc), title='PBa-LLM: Privacy- and Bias-aware NLP using Named-Entity Recognition (NER)', authors=[arxiv.Result.Author('Gonzalo Mancera'), arxiv.Result.Author('Aythami Morales'), arxiv.Result.Author('Julian Fierrez'), arxiv.Result.Author('Ruben Tolosana'), arxiv.Result.Author('Alejandro Penna'), arxiv.Result.Author('Miguel Lopez-Duran'), arxiv.Result.Author('Francisco Jurado'), arxiv.Result.Author('Alvaro Ortigosa')], summary='The use of Natural Language Processing (NLP) in highstakes AI-based\\napplications has increased significantly in recent years, especially since the\\nemergence of Large Language Models (LLMs). However, despite their strong\\nperformance, LLMs introduce important legal/ ethical concerns, particularly\\nregarding privacy, data protection, and transparency. Due to these concerns,\\nthis work explores the use of Named- Entity Recognition (NER) to facilitate the\\nprivacy-preserving training (or adaptation) of LLMs. We propose a framework\\nthat uses NER technologies to anonymize sensitive information in text data,\\nsuch as personal identities or geographic locations. An evaluation of the\\nproposed privacy-preserving learning framework was conducted to measure its\\nimpact on user privacy and system performance in a particular high-stakes and\\nsensitive setup: AI-based resume scoring for recruitment processes. The study\\ninvolved two language models (BERT and RoBERTa) and six anonymization\\nalgorithms (based on Presidio, FLAIR, BERT, and different versions of GPT)\\napplied to a database of 24,000 candidate profiles. The findings indicate that\\nthe proposed privacy preservation techniques effectively maintain system\\nperformance while playing a critical role in safeguarding candidate\\nconfidentiality, thus promoting trust in the experimented scenario. On top of\\nthe proposed privacy-preserving approach, we also experiment applying an\\nexisting approach that reduces the gender bias in LLMs, thus finally obtaining\\nour proposed Privacyand Bias-aware LLMs (PBa-LLMs). Note that the proposed\\nPBa-LLMs have been evaluated in a particular setup (resume scoring), but are\\ngenerally applicable to any other LLM-based AI application.', comment='Presented at AAAI Workshop on Privacy-Preserving Artificial\\n  Intelligence (PPAI) 2025, Philadelphia, PA, USA, March 2025', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.CR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2507.02966v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2507.02966v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.23622v2', updated=datetime.datetime(2025, 7, 30, 12, 10, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 30, 8, 39, 1, tzinfo=datetime.timezone.utc), title='Privacy-Preserving Federated Learning Scheme with Mitigating Model Poisoning Attacks: Vulnerabilities and Countermeasures', authors=[arxiv.Result.Author('Jiahui Wu'), arxiv.Result.Author('Fucai Luo'), arxiv.Result.Author('Tiecheng Sun'), arxiv.Result.Author('Haiyan Wang'), arxiv.Result.Author('Weizhe Zhang')], summary=\"The privacy-preserving federated learning schemes based on the setting of two\\nhonest-but-curious and non-colluding servers offer promising solutions in terms\\nof security and efficiency. However, our investigation reveals that these\\nschemes still suffer from privacy leakage when considering model poisoning\\nattacks from malicious users. Specifically, we demonstrate that the\\nprivacy-preserving computation process for defending against model poisoning\\nattacks inadvertently leaks privacy to one of the honest-but-curious servers,\\nenabling it to access users' gradients in plaintext. To address both privacy\\nleakage and model poisoning attacks, we propose an enhanced privacy-preserving\\nand Byzantine-robust federated learning (PBFL) scheme, comprising three\\ncomponents: (1) a two-trapdoor fully homomorphic encryption (FHE) scheme to\\nbolster users' privacy protection; (2) a novel secure normalization judgment\\nmethod to preemptively thwart gradient poisoning; and (3) an innovative secure\\ncosine similarity measurement method for detecting model poisoning attacks\\nwithout compromising data privacy. Our scheme guarantees privacy preservation\\nand resilience against model poisoning attacks, even in scenarios with\\nheterogeneous, non-IID (Independently and Identically Distributed) datasets.\\nTheoretical analyses substantiate the security and efficiency of our scheme,\\nand extensive experiments corroborate the efficacy of our private attacks.\\nFurthermore, the experimental results demonstrate that our scheme accelerates\\ntraining speed while reducing communication overhead compared to the\\nstate-of-the-art PBFL schemes.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.23622v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.23622v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.23210v2', updated=datetime.datetime(2025, 7, 16, 11, 6, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 29, 12, 41, 11, tzinfo=datetime.timezone.utc), title='FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model', authors=[arxiv.Result.Author('Taehwan Yoon'), arxiv.Result.Author('Bongjun Choi')], summary=\"Federated learning(FL) is used for distributed scenarios to train artificial\\nintelligence(AI) models while ensuring users' privacy. In federated learning\\nscenario, the server generally never knows about users' data. This type of\\nconcept makes the AI training process efficient in terms of data privacy.\\nHowever, regarding model performance, federated AI models may not sufficiently\\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\\ndifferent needs. It is not easy to satisfy the whole users needs. These types\\nof issues can be addressed through AI model optimization, fine-tuning, or\\npersonalization to achieve optimal model performance. To address model\\noptimization challenges, we propose reference model-based federated learning\\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\\nThis method is derived from Bayesian parameter-efficient transfer learning,\\nwhich includes an optimal proximal term and utilizes a reference model that\\nincorporates previous model parameters. As a result, this method achieves both\\nhigh model performance and clients' low computing cost.\", comment='6 pages,14 equation, 4 figure, 1table', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.23210v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.23210v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.22750v1', updated=datetime.datetime(2025, 6, 28, 4, 36, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 28, 4, 36, 31, tzinfo=datetime.timezone.utc), title='Enhancing Android Malware Detection with Retrieval-Augmented Generation', authors=[arxiv.Result.Author('Saraga S.'), arxiv.Result.Author('Anagha M. S.'), arxiv.Result.Author('Dincy R. Arikkat'), arxiv.Result.Author('Rafidha Rehiman K. A.'), arxiv.Result.Author('Serena Nicolazzo'), arxiv.Result.Author('Antonino Nocera'), arxiv.Result.Author('Vinod P')], summary='The widespread use of Android applications has made them a prime target for\\ncyberattacks, significantly increasing the risk of malware that threatens user\\nprivacy, security, and device functionality. Effective malware detection is\\nthus critical, with static analysis, dynamic analysis, and Machine Learning\\nbeing widely used approaches. In this work, we focus on a Machine\\nLearning-based method utilizing static features. We first compiled a dataset of\\nbenign and malicious APKs and performed static analysis to extract features\\nsuch as code structure, permissions, and manifest file content, without\\nexecuting the apps. Instead of relying solely on raw static features, our\\nsystem uses an LLM to generate high-level functional descriptions of APKs. To\\nmitigate hallucinations, which are a known vulnerability of LLM, we integrated\\nRetrieval-Augmented Generation (RAG), enabling the LLM to ground its output in\\nrelevant context. Using carefully designed prompts, we guide the LLM to produce\\ncoherent function summaries, which are then analyzed using a transformer-based\\nmodel, improving detection accuracy over conventional feature-based methods for\\nmalware detection.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.22750v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.22750v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.22606v2', updated=datetime.datetime(2025, 9, 11, 10, 59, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 27, 20, 5, 46, tzinfo=datetime.timezone.utc), title='A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization', authors=[arxiv.Result.Author('Osama Zafar'), arxiv.Result.Author('Mina Namazi'), arxiv.Result.Author('Yuqiao Xu'), arxiv.Result.Author('Youngjin Yoo'), arxiv.Result.Author('Erman Ayday')], summary=\"In the current paradigm of digital personalized services, the centralized\\nmanagement of personal data raises significant privacy concerns, security\\nvulnerabilities, and diminished individual autonomy over sensitive information.\\nDespite their efficiency, traditional centralized architectures frequently fail\\nto satisfy rigorous privacy requirements and expose users to data breaches and\\nunauthorized access risks. This pressing challenge calls for a fundamental\\nparadigm shift in methodologies for collecting, storing, and utilizing personal\\ndata across diverse sectors, including education, healthcare, and finance.\\n  This paper introduces a novel decentralized, privacy-preserving architecture\\nthat handles heterogeneous personal information, ranging from educational\\ncredentials to health records and financial data. Unlike traditional models,\\nour system grants users complete data ownership and control, allowing them to\\nselectively share information without compromising privacy. The architecture's\\nfoundation comprises advanced privacy-enhancing technologies, including secure\\nenclaves and federated learning, enabling secure computation, verification, and\\ndata sharing. The system supports diverse functionalities, including local\\ncomputation, model training, and privacy-preserving data sharing, while\\nensuring data credibility and robust user privacy.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.22606v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.22606v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.22224v1', updated=datetime.datetime(2025, 6, 27, 13, 40, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 27, 13, 40, 20, tzinfo=datetime.timezone.utc), title='A Decade of News Forum Interactions: Threaded Conversations, Signed Votes, and Topical Tags', authors=[arxiv.Result.Author('Emma Fraxanet'), arxiv.Result.Author('Vicenç Gómez'), arxiv.Result.Author('Andreas Kaltenbrunner'), arxiv.Result.Author('Max Pellert')], summary='We present a large-scale, longitudinal dataset capturing user activity on the\\nonline platform of DerStandard, a major Austrian newspaper. The dataset spans\\nten years (2013-2022) and includes over 75 million user comments, more than 400\\nmillion votes, and detailed metadata on articles and user interactions. It\\nprovides structured conversation threads, explicit up- and downvotes of user\\ncomments and editorial topic labels, enabling rich analyses of online discourse\\nwhile preserving user privacy. To ensure this privacy, all persistent\\nidentifiers are anonymized using salted hash functions, and the raw comment\\ntexts are not publicly shared. Instead, we release pre-computed vector\\nrepresentations derived from a state-of-the-art embedding model. The dataset\\nsupports research on discussion dynamics, network structures, and semantic\\nanalyses in the mid-resourced language German, offering a reusable resource\\nacross computational social science and related fields.', comment=None, journal_ref=None, doi=None, primary_category='cs.SI', categories=['cs.SI', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.22224v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.22224v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.22521v1', updated=datetime.datetime(2025, 6, 26, 22, 2, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 26, 22, 2, 1, tzinfo=datetime.timezone.utc), title='A Survey on Model Extraction Attacks and Defenses for Large Language Models', authors=[arxiv.Result.Author('Kaixiang Zhao'), arxiv.Result.Author('Lincan Li'), arxiv.Result.Author('Kaize Ding'), arxiv.Result.Author('Neil Zhenqiang Gong'), arxiv.Result.Author('Yue Zhao'), arxiv.Result.Author('Yushun Dong')], summary='Model extraction attacks pose significant security threats to deployed\\nlanguage models, potentially compromising intellectual property and user\\nprivacy. This survey provides a comprehensive taxonomy of LLM-specific\\nextraction attacks and defenses, categorizing attacks into functionality\\nextraction, training data extraction, and prompt-targeted attacks. We analyze\\nvarious attack methodologies including API-based knowledge distillation, direct\\nquerying, parameter recovery, and prompt stealing techniques that exploit\\ntransformer architectures. We then examine defense mechanisms organized into\\nmodel protection, data privacy protection, and prompt-targeted strategies,\\nevaluating their effectiveness across different deployment scenarios. We\\npropose specialized metrics for evaluating both attack effectiveness and\\ndefense performance, addressing the specific challenges of generative language\\nmodels. Through our analysis, we identify critical limitations in current\\napproaches and propose promising research directions, including integrated\\nattack methodologies and adaptive defense mechanisms that balance security with\\nmodel utility. This work serves NLP researchers, ML engineers, and security\\nprofessionals seeking to protect language models in production environments.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.22521v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.22521v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.20212v1', updated=datetime.datetime(2025, 6, 25, 7, 55, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 25, 7, 55, 59, tzinfo=datetime.timezone.utc), title='Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning', authors=[arxiv.Result.Author('Andrea Bussolan'), arxiv.Result.Author('Oliver Avram'), arxiv.Result.Author('Andrea Pignata'), arxiv.Result.Author('Gianvito Urgese'), arxiv.Result.Author('Stefano Baraldo'), arxiv.Result.Author('Anna Valente')], summary=\"With the advent of Industry 5.0, manufacturers are increasingly prioritizing\\nworker well-being alongside mass customization. Stress-aware Human-Robot\\nCollaboration (HRC) plays a crucial role in this paradigm, where robots must\\nadapt their behavior to human mental states to improve collaboration fluency\\nand safety. This paper presents a novel framework that integrates Federated\\nLearning (FL) to enable personalized mental state evaluation while preserving\\nuser privacy. By leveraging physiological signals, including EEG, ECG, EDA,\\nEMG, and respiration, a multimodal model predicts an operator's stress level,\\nfacilitating real-time robot adaptation. The FL-based approach allows\\ndistributed on-device training, ensuring data confidentiality while improving\\nmodel generalization and individual customization. Results demonstrate that the\\ndeployment of an FL approach results in a global model with performance in\\nstress prediction accuracy comparable to a centralized training approach.\\nMoreover, FL allows for enhancing personalization, thereby optimizing\\nhuman-robot interaction in industrial settings, while preserving data privacy.\\nThe proposed framework advances privacy-preserving, adaptive robotics to\\nenhance workforce well-being in smart manufacturing.\", comment=None, journal_ref=None, doi=None, primary_category='cs.RO', categories=['cs.RO', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.20212v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.20212v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.19268v2', updated=datetime.datetime(2025, 6, 26, 15, 23, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 24, 2, 59, 14, tzinfo=datetime.timezone.utc), title=\"HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps\", authors=[arxiv.Result.Author('Timoteo Kelly'), arxiv.Result.Author('Abdulkadir Korkmaz'), arxiv.Result.Author('Samuel Mallet'), arxiv.Result.Author('Connor Souders'), arxiv.Result.Author('Sadra Aliakbarpour'), arxiv.Result.Author('Praveen Rao')], summary='We present HARPT, a large-scale annotated corpus of mobile health app store\\nreviews aimed at advancing research in user privacy and trust. The dataset\\ncomprises over 480,000 user reviews labeled into seven categories that capture\\ncritical aspects of trust in applications, trust in providers and privacy\\nconcerns. Creating HARPT required addressing multiple complexities, such as\\ndefining a nuanced label schema, isolating relevant content from large volumes\\nof noisy data, and designing an annotation strategy that balanced scalability\\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\\nlabeling with review, targeted data augmentation, and weak supervision using\\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\\ncurated subset of 7,000 reviews was manually annotated to support model\\ndevelopment and evaluation. We benchmark a broad range of classification\\nmodels, demonstrating that strong performance is achievable and providing a\\nbaseline for future research. HARPT is released as a public resource to support\\nwork in health informatics, cybersecurity, and natural language processing.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CR', 'cs.ET', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.19268v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.19268v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.16891v1', updated=datetime.datetime(2025, 6, 20, 10, 29, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 20, 10, 29, 13, tzinfo=datetime.timezone.utc), title='Tracker Installations Are Not Created Equal: Understanding Tracker Configuration of Form Data Collection', authors=[arxiv.Result.Author('Julia B. Kieserman'), arxiv.Result.Author('Athanasios Andreou'), arxiv.Result.Author('Chris Geeng'), arxiv.Result.Author('Tobias Lauinger'), arxiv.Result.Author('Damon McCoy')], summary=\"Targeted advertising is fueled by the comprehensive tracking of users' online\\nactivity. As a result, advertising companies, such as Google and Meta,\\nencourage website administrators to not only install tracking scripts on their\\nwebsites but configure them to automatically collect users' Personally\\nIdentifying Information (PII). In this study, we aim to characterize how Google\\nand Meta's trackers can be configured to collect PII data from web forms. We\\nfirst perform a qualitative analysis of how third parties present form data\\ncollection to website administrators in the documentation and user interface.\\nWe then perform a measurement study of 40,150 websites to quantify the\\nprevalence and configuration of Google and Meta trackers.\\n  Our results reveal that both Meta and Google encourage the use of form data\\ncollection and include inaccurate statements about hashing PII as a\\nprivacy-preserving method. Additionally, we find that Meta includes configuring\\nform data collection as part of the basic setup flow. Our large-scale\\nmeasurement study reveals that while Google trackers are more prevalent than\\nMeta trackers (72.6% vs. 28.2% of websites), Meta trackers are configured to\\ncollect form data more frequently (11.6% vs. 62.3%). Finally, we identify\\nsensitive finance and health websites that have installed trackers that are\\nlikely configured to collect form data PII in violation of Meta and Google\\npolicies. Our study highlights how tracker documentation and interfaces can\\npotentially play a role in users' privacy through the configuration choices\\nmade by the website administrators who install trackers.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.16891v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.16891v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.16812v1', updated=datetime.datetime(2025, 6, 20, 8, 0, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 20, 8, 0, 32, tzinfo=datetime.timezone.utc), title='Zero-Knowledge Proof-of-Location Protocols for Vehicle Subsidies and Taxation Compliance', authors=[arxiv.Result.Author('Dan Bogdanov'), arxiv.Result.Author('Eduardo Brito'), arxiv.Result.Author('Annika Jaakson'), arxiv.Result.Author('Peeter Laud'), arxiv.Result.Author('Raul-Martin Rebane')], summary=\"This paper introduces a new set of privacy-preserving mechanisms for\\nverifying compliance with location-based policies for vehicle taxation, or for\\n(electric) vehicle (EV) subsidies, using Zero-Knowledge Proofs (ZKPs). We\\npresent the design and evaluation of a Zero-Knowledge Proof-of-Location\\n(ZK-PoL) system that ensures a vehicle's adherence to territorial driving\\nrequirements without disclosing specific location data, hence maintaining user\\nprivacy. Our findings suggest a promising approach to apply ZK-PoL protocols in\\nlarge-scale governmental subsidy or taxation programs.\", comment='This is the extended version of the paper to appear in the\\n  Proceedings of the 5th International Workshop on Security and Privacy in\\n  Intelligent Infrastructures (SP2I 2025), held in conjunction with the 20th\\n  International Conference on Availability, Reliability and Security (ARES\\n  2025)', journal_ref=None, doi='10.1007/978-3-032-00642-4_20', primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-032-00642-4_20', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2506.16812v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.16812v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.17349v1', updated=datetime.datetime(2025, 6, 19, 17, 42, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 19, 17, 42, 2, tzinfo=datetime.timezone.utc), title='AndroIDS : Android-based Intrusion Detection System using Federated Learning', authors=[arxiv.Result.Author('Akarsh K Nair'), arxiv.Result.Author('Shanik Hubert Satheesh Kumar.'), arxiv.Result.Author('Deepti Gupta')], summary='The exponential growth of android-based mobile IoT systems has significantly\\nincreased the susceptibility of devices to cyberattacks, particularly in smart\\nhomes, UAVs, and other connected mobile environments. This article presents a\\nfederated learning-based intrusion detection framework called AndroIDS that\\nleverages system call traces as a personalized and privacy-preserving data\\nsource. Unlike conventional centralized approaches, the proposed method enables\\ncollaborative anomaly detection without sharing raw data, thus preserving user\\nprivacy across distributed nodes. A generalized system call dataset was\\ngenerated to reflect realistic android system behavior and serves as the\\nfoundation for experimentation. Extensive evaluation demonstrates the\\neffectiveness of the FL model under both IID and non-IID conditions, achieving\\nan accuracy of 96.46 % and 92.87 %, and F1-scores of 89 % and 86 %,\\nrespectively. These results highlight the models robustness to data\\nheterogeneity, with only a minor performance drop in the non-IID case. Further,\\na detailed comparison with centralized deep learning further illustrates\\ntrade-offs in detection performance and deployment feasibility. Overall, the\\nresults validate the practical applicability of the proposed approach for\\nsecure and scalable intrusion detection in real-world mobile IoT scenarios.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.17349v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.17349v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.17342v1', updated=datetime.datetime(2025, 6, 19, 13, 33, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 19, 13, 33, 43, tzinfo=datetime.timezone.utc), title='Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning', authors=[arxiv.Result.Author('Zijian Long'), arxiv.Result.Author('Haopeng Wang'), arxiv.Result.Author('Haiwei Dong'), arxiv.Result.Author('Abdulmotaleb El Saddik')], summary='The social metaverse is a growing digital ecosystem that blends virtual and\\nphysical worlds. It allows users to interact socially, work, shop, and enjoy\\nentertainment. However, privacy remains a major challenge, as immersive\\ninteractions require continuous collection of biometric and behavioral data. At\\nthe same time, ensuring high-quality, low-latency streaming is difficult due to\\nthe demands of real-time interaction, immersive rendering, and bandwidth\\noptimization. To address these issues, we propose ASMS (Adaptive Social\\nMetaverse Streaming), a novel streaming system based on Federated Multi-Agent\\nProximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which\\nintegrates federated learning (FL) and deep reinforcement learning (DRL) to\\ndynamically adjust streaming bit rates while preserving user privacy.\\nExperimental results show that ASMS improves user experience by at least 14%\\ncompared to existing streaming methods across various network conditions.\\nTherefore, ASMS enhances the social metaverse experience by providing seamless\\nand immersive streaming, even in dynamic and resource-constrained networks,\\nwhile ensuring that sensitive user data remains on local devices.', comment='Accepted by IEEE Transactions on Computational Social Systems', journal_ref=None, doi='10.1109/TCSS.2025.3555419', primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.MM', 'cs.NI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TCSS.2025.3555419', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2506.17342v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.17342v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.17336v2', updated=datetime.datetime(2025, 7, 1, 16, 41, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 19, 7, 13, 30, tzinfo=datetime.timezone.utc), title='Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases', authors=[arxiv.Result.Author('Yubeen Bae'), arxiv.Result.Author('Minchan Kim'), arxiv.Result.Author('Jaejin Lee'), arxiv.Result.Author('Sangbum Kim'), arxiv.Result.Author('Jaehyung Kim'), arxiv.Result.Author('Yejin Choi'), arxiv.Result.Author('Niloofar Mireshghallah')], summary=\"Large language models (LLMs) are increasingly used as personal agents,\\naccessing sensitive user data such as calendars, emails, and medical records.\\nUsers currently face a trade-off: They can send private records, many of which\\nare stored in remote databases, to powerful but untrusted LLM providers,\\nincreasing their exposure risk. Alternatively, they can run less powerful\\nmodels locally on trusted devices. We bridge this gap. Our Socratic\\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\\ndetailed sub-queries without accessing user data. Next, we embed these\\nsub-queries and perform encrypted sub-second semantic search using our\\nHomomorphically Encrypted Vector Database across one million entries of a\\nsingle user's private data. This represents a realistic scale of personal\\ndocuments, emails, and records accumulated over years of digital activity.\\nFinally, we feed the CoT prompt and the decrypted records to a local language\\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\\ndemonstrates a first step toward systems where tasks are decomposed and split\\nbetween untrusted strong LLMs and weak local ones, preserving user privacy.\", comment='29 pages', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.17336v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.17336v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.15924v1', updated=datetime.datetime(2025, 6, 18, 23, 58, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 18, 23, 58, 29, tzinfo=datetime.timezone.utc), title=\"FARFETCH'D: A Side-Channel Analysis Framework for Privacy Applications on Confidential Virtual Machines\", authors=[arxiv.Result.Author('Ruiyi Zhang'), arxiv.Result.Author('Albert Cheu'), arxiv.Result.Author('Adria Gascon'), arxiv.Result.Author('Daniel Moghimi'), arxiv.Result.Author('Phillipp Schoppmann'), arxiv.Result.Author('Michael Schwarz'), arxiv.Result.Author('Octavian Suciu')], summary=\"Confidential virtual machines (CVMs) based on trusted execution environments\\n(TEEs) enable new privacy-preserving solutions. Yet, they leave side-channel\\nleakage outside their threat model, shifting the responsibility of mitigating\\nsuch attacks to developers. However, mitigations are either not generic or too\\nslow for practical use, and developers currently lack a systematic, efficient\\nway to measure and compare leakage across real-world deployments. In this\\npaper, we present FARFETCH'D, an open-source toolkit that offers configurable\\nside-channel tracing primitives on production AMD SEV-SNP hardware and couples\\nthem with statistical and machine-learning-based analysis pipelines for\\nautomated leakage estimation. We apply FARFETCH'D to three representative\\nworkloads that are deployed on CVMs to enhance user privacy - private\\ninformation retrieval, private heavy hitters, and Wasm user-defined functions -\\nand uncover previously unnoticed leaks, including a covert channel that\\nexfiltrated data at 497 kbit/s. The results show that FARFETCH'D pinpoints\\nvulnerabilities and guides low-overhead mitigations based on oblivious memory\\nand differential privacy, giving practitioners a practical path to deploy CVMs\\nwith meaningful confidentiality guarantees.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.15924v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.15924v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.15224v1', updated=datetime.datetime(2025, 6, 18, 8, 8, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 18, 8, 8, 12, tzinfo=datetime.timezone.utc), title='Facility Location Problem under Local Differential Privacy without Super-set Assumption', authors=[arxiv.Result.Author('Kevin Pfisterer'), arxiv.Result.Author('Quentin Hillebrand'), arxiv.Result.Author('Vorapong Suppakitpaisarn')], summary='In this paper, we introduce an adaptation of the facility location problem\\nand analyze it within the framework of local differential privacy (LDP). Under\\nthis model, we ensure the privacy of client presence at specific locations.\\nWhen n is the number of points, Gupta et al. established a lower bound of\\n$\\\\Omega(\\\\sqrt{n})$ on the approximation ratio for any differentially private\\nalgorithm applied to the original facility location problem. As a result,\\nsubsequent works have adopted the super-set assumption, which may, however,\\ncompromise user privacy. We show that this lower bound does not apply to our\\nadaptation by presenting an LDP algorithm that achieves a constant\\napproximation ratio with a relatively small additive factor. Additionally, we\\nprovide experimental results demonstrating that our algorithm outperforms the\\nstraightforward approach on both synthetically generated and real-world\\ndatasets.', comment='accepted at DBSec 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.15224v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.15224v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.13170v1', updated=datetime.datetime(2025, 6, 16, 7, 33, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 16, 7, 33, 12, tzinfo=datetime.timezone.utc), title='Dual Protection Ring: User Profiling Via Differential Privacy and Service Dissemination Through Private Information Retrieval', authors=[arxiv.Result.Author('Imdad Ullah'), arxiv.Result.Author('Najm Hassan'), arxiv.Result.Author('Tariq Ahamed Ahangar'), arxiv.Result.Author('Zawar Hussain Shah'), arxiv.Result.Author('Mehregan Mahdavi'), arxiv.Result.Author('Andrew Levula')], summary='User profiling is crucial in providing personalised services, as it relies on\\nanalysing user behaviour and preferences to deliver targeted services. This\\napproach enhances user experience and promotes heightened engagement.\\nNevertheless, user profiling also gives rise to noteworthy privacy\\nconsiderations due to the extensive tracking and monitoring of personal data,\\npotentially leading to surveillance or identity theft. We propose a dual-ring\\nprotection mechanism to protect user privacy by examining various threats to\\nuser privacy, such as behavioural attacks, profiling fingerprinting and\\nmonitoring, profile perturbation, etc., both on the user and service provider\\nsides. We develop user profiles that contain sensitive private attributes and\\nan equivalent profile based on differential privacy for evaluating personalised\\nservices. We determine the entropy of the resultant profiles during each update\\nto protect profiling attributes and invoke various processes, such as data\\nevaporation, to artificially increase entropy or destroy private profiling\\nattributes. Furthermore, we use different variants of private information\\nretrieval (PIR) to retrieve personalised services against differentially\\nprivate profiles. We implement critical components of the proposed model via a\\nproof-of-concept mobile app to demonstrate its applicability over a specific\\ncase study of advertising services, which can be generalised to other services.\\nOur experimental results show that the observed processing delays with\\ndifferent PIR schemes are similar to the current advertising systems.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.13170v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.13170v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.13052v2', updated=datetime.datetime(2025, 6, 22, 2, 48, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 16, 2, 42, 14, tzinfo=datetime.timezone.utc), title='Buy it Now, Track Me Later: Attacking User Privacy via Wi-Fi AP Online Auctions', authors=[arxiv.Result.Author('Steven Su'), arxiv.Result.Author('Erik Rye'), arxiv.Result.Author('Dave Levin'), arxiv.Result.Author('Robert Beverly')], summary=\"Static and hard-coded layer-two network identifiers are well known to present\\nsecurity vulnerabilities and endanger user privacy. In this work, we introduce\\na new privacy attack against Wi-Fi access points listed on secondhand\\nmarketplaces. Specifically, we demonstrate the ability to remotely gather a\\nlarge quantity of layer-two Wi-Fi identifiers by programmatically querying the\\neBay marketplace and applying state-of-the-art computer vision techniques to\\nextract IEEE 802.11 BSSIDs from the seller's posted images of the hardware. By\\nleveraging data from a global Wi-Fi Positioning System (WPS) that geolocates\\nBSSIDs, we obtain the physical locations of these devices both pre- and\\npost-sale. In addition to validating the degree to which a seller's location\\nmatches the location of the device, we examine cases of device movement -- once\\nthe device is sold and then subsequently re-used in a new environment. Our work\\nhighlights a previously unrecognized privacy vulnerability and suggests, yet\\nagain, the strong need to protect layer-two network identifiers.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.13052v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.13052v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.12761v1', updated=datetime.datetime(2025, 6, 15, 8, 1, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 15, 8, 1, 35, tzinfo=datetime.timezone.utc), title='Versatile and Fast Location-Based Private Information Retrieval with Fully Homomorphic Encryption over the Torus', authors=[arxiv.Result.Author('Joon Soo Yoo'), arxiv.Result.Author('Taeho Kim'), arxiv.Result.Author('Ji Won Yoon')], summary=\"Location-based services often require users to share sensitive locational\\ndata, raising privacy concerns due to potential misuse or exploitation by\\nuntrusted servers. In response, we present VeLoPIR, a versatile location-based\\nprivate information retrieval (PIR) system designed to preserve user privacy\\nwhile enabling efficient and scalable query processing. VeLoPIR introduces\\nthree operational modes-interval validation, coordinate validation, and\\nidentifier matching-that support a broad range of real-world applications,\\nincluding information and emergency alerts. To enhance performance, VeLoPIR\\nincorporates multi-level algorithmic optimizations with parallel structures,\\nachieving significant scalability across both CPU and GPU platforms. We also\\nprovide formal security and privacy proofs, confirming the system's robustness\\nunder standard cryptographic assumptions. Extensive experiments on real-world\\ndatasets demonstrate that VeLoPIR achieves up to 11.55 times speed-up over a\\nprior baseline. The implementation of VeLoPIR is publicly available at\\nhttps://github.com/PrivStatBool/VeLoPIR.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.12761v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.12761v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.17279v1', updated=datetime.datetime(2025, 6, 14, 4, 22, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 14, 4, 22, 17, tzinfo=datetime.timezone.utc), title=\"Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models\", authors=[arxiv.Result.Author('Yash Sinha'), arxiv.Result.Author('Manit Baser'), arxiv.Result.Author('Murari Mandal'), arxiv.Result.Author('Dinil Mon Divakaran'), arxiv.Result.Author('Mohan Kankanhalli')], summary='Knowledge erasure in large language models (LLMs) is important for ensuring\\ncompliance with data and AI regulations, safeguarding user privacy, mitigating\\nbias, and misinformation. Existing unlearning methods aim to make the process\\nof knowledge erasure more efficient and effective by removing specific\\nknowledge while preserving overall model performance, especially for retained\\ninformation. However, it has been observed that the unlearning techniques tend\\nto suppress and leave the knowledge beneath the surface, thus making it\\nretrievable with the right prompts. In this work, we demonstrate that\\n\\\\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden\\ninformation. We introduce a step-by-step reasoning-based black-box attack,\\nSleek, that systematically exposes unlearning failures. We employ a structured\\nattack framework with three core components: (1) an adversarial prompt\\ngeneration strategy leveraging step-by-step reasoning built from LLM-generated\\nqueries, (2) an attack mechanism that successfully recalls erased content, and\\nexposes unfair suppression of knowledge intended for retention and (3) a\\ncategorization of prompts as direct, indirect, and implied, to identify which\\nquery types most effectively exploit unlearning weaknesses. Through extensive\\nevaluations on four state-of-the-art unlearning techniques and two widely used\\nLLMs, we show that existing approaches fail to ensure reliable knowledge\\nremoval. Of the generated adversarial prompts, 62.5% successfully retrieved\\nforgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair\\nsuppression of retained knowledge. Our work highlights the persistent risks of\\ninformation leakage, emphasizing the need for more robust unlearning strategies\\nfor erasure.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.17279v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.17279v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.11212v1', updated=datetime.datetime(2025, 6, 12, 18, 19, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 12, 18, 19, 50, tzinfo=datetime.timezone.utc), title='User Perceptions and Attitudes Toward Untraceability in Messaging Platforms', authors=[arxiv.Result.Author('Carla F. Griggio'), arxiv.Result.Author('Boel Nelson'), arxiv.Result.Author('Zefan Sramek'), arxiv.Result.Author('Aslan Askarov')], summary='Mainstream messaging platforms offer a variety of features designed to\\nenhance user privacy, such as disappearing messages, password-protected chats,\\nand end-to-end encryption (E2EE), which primarily protect message contents.\\nBeyond contents, the transmission of messages generates metadata that can\\nreveal who communicates with whom, when and how often. In this paper, we study\\nuser perceptions of \"untraceability\", i.e., preventing third parties from\\ntracing who communicates with whom, with the goal of informing the design of\\nprivacy-enhancing features in messaging platforms and untraceable communication\\nprotocols that depend on large anonymity sets and widespread user adoption. We\\nexplore this from a broad conceptual standpoint: rather than studying mental\\nmodels of a particular solution, we analyze how users reason about what\\nfeatures should be incorporated by two fictitious platforms, Texty and Chatty,\\nto prevent third parties from knowing who communicates with whom. Through a\\nvignette-based survey with 189 participants, we found that users associate the\\nconcept of untraceability with a wide range of privacy enhancing technologies,\\nimplying a diverse set of threat models. Overall, the features suggested by\\nparticipants show awareness of privacy threats stemming from forms of\\nsurveillance and unauthorized access to message contents. Many participants\\nalso associated untraceability with the notion of anonymity, but interpreted it\\nas senders and receivers concealing their identity from each other rather than\\nonly from third parties. We discuss the gap between users\\' perceptions of\\nuntraceability and the threat models addressed by untraceable communication\\nprotocols, as well as how different privacy attitudes point to challenges and\\nopportunities for the adoption of untraceable communication tools in messaging\\nplatforms.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.11212v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.11212v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.09525v1', updated=datetime.datetime(2025, 6, 11, 8, 51, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 11, 8, 51, 19, tzinfo=datetime.timezone.utc), title='Beyond Personalization: Federated Recommendation with Calibration via Low-rank Decomposition', authors=[arxiv.Result.Author('Jundong Chen'), arxiv.Result.Author('Honglei Zhang'), arxiv.Result.Author('Haoxuan Li'), arxiv.Result.Author('Chunxu Zhang'), arxiv.Result.Author('Zhiwei Li'), arxiv.Result.Author('Yidong Li')], summary='Federated recommendation (FR) is a promising paradigm to protect user privacy\\nin recommender systems. Distinct from general federated scenarios, FR\\ninherently needs to preserve client-specific parameters, i.e., user embeddings,\\nfor privacy and personalization. However, we empirically find that globally\\naggregated item embeddings can induce skew in user embeddings, resulting in\\nsuboptimal performance. To this end, we theoretically analyze the user\\nembedding skew issue and propose Personalized Federated recommendation with\\nCalibration via Low-Rank decomposition (PFedCLR). Specifically, PFedCLR\\nintroduces an integrated dual-function mechanism, implemented with a buffer\\nmatrix, to jointly calibrate local user embedding and personalize global item\\nembeddings. To ensure efficiency, we employ a low-rank decomposition of the\\nbuffer matrix to reduce the model overhead. Furthermore, for privacy, we train\\nand upload the local model before personalization, preventing the server from\\naccessing sensitive information. Extensive experiments demonstrate that PFedCLR\\neffectively mitigates user embedding skew and achieves a desirable trade-off\\namong performance, efficiency, and privacy, outperforming state-of-the-art\\n(SOTA) methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.09525v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.09525v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.08169v1', updated=datetime.datetime(2025, 6, 9, 19, 28, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 9, 19, 28, 39, tzinfo=datetime.timezone.utc), title='Federated Learning on Stochastic Neural Networks', authors=[arxiv.Result.Author('Jingqiao Tang'), arxiv.Result.Author('Ryan Bausback'), arxiv.Result.Author('Feng Bao'), arxiv.Result.Author('Richard Archibald')], summary='Federated learning is a machine learning paradigm that leverages edge\\ncomputing on client devices to optimize models while maintaining user privacy\\nby ensuring that local data remains on the device. However, since all data is\\ncollected by clients, federated learning is susceptible to latent noise in\\nlocal datasets. Factors such as limited measurement capabilities or human\\nerrors may introduce inaccuracies in client data. To address this challenge, we\\npropose the use of a stochastic neural network as the local model within the\\nfederated learning framework. Stochastic neural networks not only facilitate\\nthe estimation of the true underlying states of the data but also enable the\\nquantification of latent noise. We refer to our federated learning approach,\\nwhich incorporates stochastic neural networks as local models, as Federated\\nstochastic neural networks. We will present numerical experiments demonstrating\\nthe performance and effectiveness of our method, particularly in handling\\nnon-independent and identically distributed data.', comment='25 pages, 19 figures, Submitted to Journal of Machine Learning for\\n  Modeling and Computing', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.08169v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.08169v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.09065v1', updated=datetime.datetime(2025, 6, 7, 19, 0, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 7, 19, 0, 17, tzinfo=datetime.timezone.utc), title='Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis', authors=[arxiv.Result.Author('Abigail Copiaco'), arxiv.Result.Author('Christian Ritz'), arxiv.Result.Author('Yassine Himeur'), arxiv.Result.Author('Valsamma Eapen'), arxiv.Result.Author('Ammar Albanna'), arxiv.Result.Author('Wathiq Mansoor')], summary=\"The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\\npast decade, posing significant challenges in communication, behavior, and\\nfocus for affected individuals. Current diagnostic techniques, though\\neffective, are time-intensive, leading to high social and economic costs. This\\nwork introduces an AI-powered assistive technology designed to streamline ASD\\ndiagnosis and management, enhancing convenience for individuals with ASD and\\nefficiency for caregivers and therapists. The system integrates transfer\\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\\nThis facilitates and opens opportunities for in-home periodical diagnosis,\\nreducing stress for individuals and caregivers, while also preserving user\\nprivacy through the use of image transforms. The accessibility of the proposed\\nmethod also offers opportunities for improved communication between guardians\\nand therapists, ensuring regular updates on progress and evolving support\\nneeds. Overall, the approach proposed in this work ensures timely, accessible\\ndiagnosis while protecting the subjects' privacy, improving outcomes for\\nindividuals with ASD.\", comment='6 pages, 8 figures, and 1 table', journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.AI', 'cs.CV', 'cs.HC', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.09065v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.09065v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.06591v1', updated=datetime.datetime(2025, 6, 6, 23, 49, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 6, 23, 49, 48, tzinfo=datetime.timezone.utc), title='Privacy Perspectives and Practices of Chinese Smart Home Product Teams', authors=[arxiv.Result.Author('Shijing He'), arxiv.Result.Author('Yaxiong Lei'), arxiv.Result.Author('Xiao Zhan'), arxiv.Result.Author('Chi Zhang'), arxiv.Result.Author('Juan Ye'), arxiv.Result.Author('Ruba Abu-Salma'), arxiv.Result.Author('Jose Such')], summary=\"Previous research has explored the privacy needs and concerns of device\\nowners, primary users, and different bystander groups with regard to smart home\\ndevices like security cameras, smart speakers, and hubs, but little is known\\nabout the privacy views and practices of smart home product teams, particularly\\nthose in non-Western contexts. This paper presents findings from 27\\nsemi-structured interviews with Chinese smart home product team members,\\nincluding product/project managers, software/hardware engineers, user\\nexperience (UX) designers, legal/privacy experts, and marketers/operation\\nspecialists. We examine their privacy perspectives, practices, and risk\\nmitigation strategies. Our results show that participants emphasized compliance\\nwith Chinese data privacy laws, which typically prioritized national security\\nover individual privacy rights. China-specific cultural, social, and legal\\nfactors also influenced participants' ethical considerations and attitudes\\ntoward balancing user privacy and security with convenience. Drawing on our\\nfindings, we propose a set of recommendations for smart home product teams,\\nalong with socio-technical and legal interventions to address smart home\\nprivacy issues-especially those belonging to at-risk groups-in Chinese\\nmulti-user smart homes.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.06591v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.06591v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.05708v1', updated=datetime.datetime(2025, 6, 6, 3, 14, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 6, 3, 14, 5, tzinfo=datetime.timezone.utc), title='Hybrid Stabilization Protocol for Cross-Chain Digital Assets Using Adaptor Signatures and AI-Driven Arbitrage', authors=[arxiv.Result.Author('Shengwei You'), arxiv.Result.Author('Andrey Kuehlkamp'), arxiv.Result.Author('Jarek Nabrzyski')], summary=\"Stablecoins face an unresolved trilemma of balancing decentralization,\\nstability, and regulatory compliance. We present a hybrid stabilization\\nprotocol that combines crypto-collateralized reserves, algorithmic futures\\ncontracts, and cross-chain liquidity pools to achieve robust price adherence\\nwhile preserving user privacy. At its core, the protocol introduces\\nstabilization futures contracts (SFCs), non-collateralized derivatives that\\nprogrammatically incentivize third-party arbitrageurs to counteract price\\ndeviations via adaptor signature atomic swaps. Autonomous AI agents optimize\\ndelta hedging across decentralized exchanges (DEXs), while zkSNARKs prove\\ncompliance with anti-money laundering (AML) regulations without exposing\\nidentities or transaction details. Our cryptographic design reduces cross-chain\\nliquidity concentration (Herfindahl-Hirschman Index: 2,400 vs. 4,900 in\\nsingle-chain systems) and ensures atomicity under standard cryptographic\\nassumptions. The protocol's layered architecture encompassing\\nincentive-compatible SFCs, AI-driven market making, and zero-knowledge\\nregulatory proofs. It provides a blueprint for next-generation decentralized\\nfinancial infrastructure.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.CE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.05708v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.05708v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.05138v2', updated=datetime.datetime(2025, 9, 4, 15, 8, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 5, 15, 22, 4, tzinfo=datetime.timezone.utc), title='Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems', authors=[arxiv.Result.Author('Pavle Vasiljevic'), arxiv.Result.Author('Milica Matic'), arxiv.Result.Author('Miroslav Popovic')], summary='Recently, federated learning frameworks such as Python TestBed for Federated\\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\\nhave emerged to tackle user privacy concerns and efficiency in embedded\\nsystems. Even more recently, an efficient federated anomaly detection\\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\\nlow-resource, unsupervised method well-suited for edge deployment and\\ncontinuous learning. In this paper, we present an application of Isolation\\nForest-based temperature anomaly detection, developed using the previously\\nmentioned federated learning frameworks, aimed at small edge devices and IoT\\nsystems running MicroPython. The system has been experimentally evaluated,\\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\\nabove 78% precision in detecting anomalies across all tested configurations,\\nwhile maintaining a memory usage below 160 KB during model training. These\\nresults highlight its suitability for resource-constrained environments and\\nedge systems, while upholding federated learning principles of data privacy and\\ncollaborative learning.', comment='6 pages, 4 algorithms, 5 figures, 2 tables', journal_ref='Published by IEEE Xplore', doi='10.1109/ZINC65316.2025.11103552', primary_category='cs.LG', categories=['cs.LG', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ZINC65316.2025.11103552', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2506.05138v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.05138v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.04681v1', updated=datetime.datetime(2025, 6, 5, 7, 0, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 5, 7, 0, 31, tzinfo=datetime.timezone.utc), title='Urania: Differentially Private Insights into AI Use', authors=[arxiv.Result.Author('Daogao Liu'), arxiv.Result.Author('Edith Cohen'), arxiv.Result.Author('Badih Ghazi'), arxiv.Result.Author('Peter Kairouz'), arxiv.Result.Author('Pritish Kamath'), arxiv.Result.Author('Alexander Knop'), arxiv.Result.Author('Ravi Kumar'), arxiv.Result.Author('Pasin Manurangsi'), arxiv.Result.Author('Adam Sealfon'), arxiv.Result.Author('Da Yu'), arxiv.Result.Author('Chiyuan Zhang')], summary=\"We introduce $Urania$, a novel framework for generating insights about LLM\\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\\nframework employs a private clustering mechanism and innovative keyword\\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\\napproaches. By leveraging DP tools such as clustering, partition selection, and\\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\\nOur evaluation assesses lexical and semantic content preservation, pair\\nsimilarity, and LLM-based metrics, benchmarking against a non-private\\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\\nempirical privacy evaluation that demonstrates the enhanced robustness of our\\nDP pipeline. The results show the framework's ability to extract meaningful\\nconversational insights while maintaining stringent user privacy, effectively\\nbalancing data utility with privacy preservation.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.CL', 'cs.CR', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.04681v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.04681v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.05421v2', updated=datetime.datetime(2025, 6, 12, 1, 37, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 5, 2, 38, 2, tzinfo=datetime.timezone.utc), title='TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in Mobile Networks using Transformers, Adversarial Learning, and Differential Privacy', authors=[arxiv.Result.Author('Al Nahian Bin Emran'), arxiv.Result.Author('Dhiman Goswami'), arxiv.Result.Author('Md Hasan Ullah Sadi'), arxiv.Result.Author('Sanchari Das')], summary=\"The proliferation of propaganda on mobile platforms raises critical concerns\\naround detection accuracy and user privacy. To address this, we propose TRIDENT\\n- a three-tier propaganda detection model implementing transformers,\\nadversarial learning, and differential privacy which integrates syntactic\\nobfuscation and label perturbation to mitigate privacy leakage while\\nmaintaining propaganda detection accuracy. TRIDENT leverages multilingual\\nback-translation to introduce semantic variance, character-level noise, and\\nentity obfuscation for differential privacy enforcement, and combines these\\ntechniques into a unified defense mechanism. Using a binary propaganda\\nclassification dataset, baseline transformer models (BERT, GPT-2) we achieved\\nF1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a\\nreduced but effective cumulative F1 of 0.83, demonstrating strong privacy\\nprotection across mobile ML deployments with minimal degradation.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.05421v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.05421v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.03870v1', updated=datetime.datetime(2025, 6, 4, 12, 1, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 4, 12, 1, 17, tzinfo=datetime.timezone.utc), title=\"Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets\", authors=[arxiv.Result.Author('Mohd. Farhan Israk Soumik'), arxiv.Result.Author('Syed Mhamudul Hasan'), arxiv.Result.Author('Abdur R. Shahid')], summary=\"The misuse of Large Language Models (LLMs) to infer emotions from text for\\nmalicious purposes, known as emotion inference attacks, poses a significant\\nthreat to user privacy. In this paper, we investigate the potential of Apple\\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\\nmitigate these risks through text modifications such as rewriting and tone\\nadjustment. By developing early novel datasets specifically for this purpose,\\nwe empirically assess how different text modifications influence LLM-based\\ndetection. This capability suggests strong potential for Apple Intelligence's\\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\\nfor future adaptive rewriting systems capable of dynamically neutralizing\\nsensitive emotional content to enhance user privacy. To the best of our\\nknowledge, this research provides the first empirical analysis of Apple\\nIntelligence's text-modification tools within a privacy-preservation context\\nwith the broader goal of developing on-device, user-centric privacy-preserving\\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\\nsystems.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.03870v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.03870v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.01954v1', updated=datetime.datetime(2025, 6, 2, 17, 59, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 6, 2, 17, 59, 51, tzinfo=datetime.timezone.utc), title='DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation', authors=[arxiv.Result.Author('Jennifer Chen'), arxiv.Result.Author('Aidar Myrzakhan'), arxiv.Result.Author('Yaxin Luo'), arxiv.Result.Author('Hassaan Muhammad Khan'), arxiv.Result.Author('Sondos Mahmoud Bsharat'), arxiv.Result.Author('Zhiqiang Shen')], summary=\"Retrieval-Augmented Generation (RAG) methods have proven highly effective for\\ntasks requiring factual consistency and robust knowledge retrieval. However,\\nlarge-scale RAG systems consume significant computational resources and are\\nprone to generating hallucinated content from Humans. In this work, we\\nintroduce $\\\\texttt{DRAG}$, a novel framework for distilling RAG knowledge from\\nlarge-scale Language Models (LLMs) into small LMs (SLMs). Our approach\\nleverages evidence- and knowledge graph-based distillation, ensuring that the\\ndistilled model retains critical factual knowledge while significantly reducing\\nmodel size and computational cost. By aligning the smaller model's predictions\\nwith a structured knowledge graph and ranked evidence, $\\\\texttt{DRAG}$\\neffectively mitigates hallucinations and improves factual accuracy. We further\\npresent a case demonstrating how our framework mitigates user privacy risks and\\nintroduce a corresponding benchmark. Experimental evaluations on multiple\\nbenchmarks demonstrate that our method outperforms the prior competitive RAG\\nmethods like MiniRAG for SLMs by up to 27.7% using the same models, preserving\\nhigh-level efficiency and reliability. With $\\\\texttt{DRAG}$, we provide a\\npractical and resource-efficient roadmap to deploying enhanced retrieval and\\ngeneration capabilities in small-sized LLMs.\", comment='ACL 2025 Main. Code is available at https://github.com/VILA-Lab/DRAG', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.01954v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.01954v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2506.00719v1', updated=datetime.datetime(2025, 5, 31, 21, 39, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 31, 21, 39, 17, tzinfo=datetime.timezone.utc), title='Browser Fingerprinting Using WebAssembly', authors=[arxiv.Result.Author('Mordechai Guri'), arxiv.Result.Author('Dor Fibert')], summary=\"Web client fingerprinting has become a widely used technique for uniquely\\nidentifying users, browsers, operating systems, and devices with high accuracy.\\nWhile it is beneficial for applications such as fraud detection and\\npersonalized experiences, it also raises privacy concerns by enabling\\npersistent tracking and detailed user profiling. This paper introduces an\\nadvanced fingerprinting method using WebAssembly (Wasm) - a low-level\\nprogramming language that offers near-native execution speed in modern web\\nbrowsers. With broad support across major browsers and growing adoption,\\nWebAssembly provides a strong foundation for developing more effective\\nfingerprinting methods.\\n  In this work, we present a new approach that leverages WebAssembly's\\ncomputational capabilities to identify returning devices-such as smartphones,\\ntablets, laptops, and desktops across different browsing sessions. Our method\\nuses subtle differences in the WebAssembly JavaScript API implementation to\\ndistinguish between Chromium-based browsers like Google Chrome and Microsoft\\nEdge, even when identifiers such as the User-Agent are completely spoofed,\\nachieving a false-positive rate of less than 1%. The fingerprint is generated\\nusing a combination of CPU-bound operations, memory tasks, and I/O activities\\nto capture unique browser behaviors. We validate this approach on a variety of\\nplatforms, including Intel, AMD, and ARM CPUs, operating systems such as\\nWindows, macOS, Android, and iOS, and in environments like VMWare, KVM, and\\nVirtualBox. Extensive evaluation shows that WebAssembly-based fingerprinting\\nsignificantly improves identification accuracy. We also propose mitigation\\nstrategies to reduce the privacy risks associated with this method, which could\\nbe integrated into future browser designs to better protect user privacy.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2506.00719v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2506.00719v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.24115v1', updated=datetime.datetime(2025, 5, 30, 1, 26, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 30, 1, 26, 31, tzinfo=datetime.timezone.utc), title='FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing System', authors=[arxiv.Result.Author('Bhawana Chhaglani'), arxiv.Result.Author('Sarmistha Sarna Gomasta'), arxiv.Result.Author('Yuvraj Agarwal'), arxiv.Result.Author('Jeremy Gummeson'), arxiv.Result.Author('Prashant Shenoy')], summary='Audio is a rich sensing modality that is useful for a variety of human\\nactivity recognition tasks. However, the ubiquitous nature of smartphones and\\nsmart speakers with always-on microphones has led to numerous privacy concerns\\nand a lack of trust in deploying these audio-based sensing systems. This paper\\naddresses this critical challenge of preserving user privacy when using audio\\nfor sensing applications while maintaining utility. While prior work focuses\\nprimarily on protecting recoverable speech content, we show that sensitive\\nspeaker-specific attributes such as age and gender can still be inferred after\\nmasking speech and propose a comprehensive privacy evaluation framework to\\nassess this speaker attribute leakage. We design and implement FeatureSense, an\\nopen-source library that provides a set of generalizable privacy-aware audio\\nfeatures that can be used for wide range of sensing applications. We present an\\nadaptive task-specific feature selection algorithm that optimizes the\\nprivacy-utility-cost trade-off based on the application requirements. Through\\nour extensive evaluation, we demonstrate the high utility of FeatureSense\\nacross a diverse set of sensing tasks. Our system outperforms existing privacy\\ntechniques by 60.6% in preserving user-specific privacy. This work provides a\\nfoundational framework for ensuring trust in audio sensing by enabling\\neffective privacy-aware audio classification systems.', comment=None, journal_ref=None, doi=None, primary_category='cs.SD', categories=['cs.SD', 'cs.HC', 'eess.AS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.24115v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.24115v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.24019v1', updated=datetime.datetime(2025, 5, 29, 21, 39, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 29, 21, 39, 8, tzinfo=datetime.timezone.utc), title='LLM Agents Should Employ Security Principles', authors=[arxiv.Result.Author('Kaiyuan Zhang'), arxiv.Result.Author('Zian Su'), arxiv.Result.Author('Pin-Yu Chen'), arxiv.Result.Author('Elisa Bertino'), arxiv.Result.Author('Xiangyu Zhang'), arxiv.Result.Author('Ninghui Li')], summary=\"Large Language Model (LLM) agents show considerable promise for automating\\ncomplex tasks using contextual reasoning; however, interactions involving\\nmultiple agents and the system's susceptibility to prompt injection and other\\nforms of context manipulation introduce new vulnerabilities related to privacy\\nleakage and system exploitation. This position paper argues that the\\nwell-established design principles in information security, which are commonly\\nreferred to as security principles, should be employed when deploying LLM\\nagents at scale. Design principles such as defense-in-depth, least privilege,\\ncomplete mediation, and psychological acceptability have helped guide the\\ndesign of mechanisms for securing information systems over the last five\\ndecades, and we argue that their explicit and conscientious adoption will help\\nsecure agentic systems. To illustrate this approach, we introduce AgentSandbox,\\na conceptual framework embedding these security principles to provide\\nsafeguards throughout an agent's life-cycle. We evaluate with state-of-the-art\\nLLMs along three dimensions: benign utility, attack utility, and attack success\\nrate. AgentSandbox maintains high utility for its intended functions under both\\nbenign and adversarial evaluations while substantially mitigating privacy\\nrisks. By embedding secure design principles as foundational elements within\\nemerging LLM agent protocols, we aim to promote trustworthy agent ecosystems\\naligned with user privacy expectations and evolving regulatory requirements.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.24019v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.24019v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.21008v1', updated=datetime.datetime(2025, 5, 27, 10, 42, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 27, 10, 42, 28, tzinfo=datetime.timezone.utc), title=\"A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on Anonymity, Confidentiality, and Auditability\", authors=[arxiv.Result.Author('Matteo Nardelli'), arxiv.Result.Author('Francesco De Sclavis'), arxiv.Result.Author('Michela Iezzi')], summary='Cryptocurrencies and central bank digital currencies (CBDCs) are reshaping\\nthe monetary landscape, offering transparency and efficiency while raising\\ncritical concerns about user privacy and regulatory compliance. This survey\\nprovides a comprehensive and technically grounded overview of\\nprivacy-preserving digital currencies, covering both cryptocurrencies and\\nCBDCs. We propose a taxonomy of privacy goals -- including anonymity,\\nconfidentiality, unlinkability, and auditability -- and map them to underlying\\ncryptographic primitives, protocol mechanisms, and system architectures. Unlike\\nprevious surveys, our work adopts a design-oriented perspective, linking\\nhigh-level privacy objectives to concrete implementations. We also trace the\\nevolution of privacy-preserving currencies through three generations,\\nhighlighting shifts from basic anonymity guarantees toward more nuanced\\nprivacy-accountability trade-offs. Finally, we identify open challenges at the\\nintersection of cryptography, distributed systems, and policy definition, which\\nmotivate further investigation into the primitives and design of digital\\ncurrencies that balance real-world privacy and auditability needs.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.21008v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.21008v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.18518v1', updated=datetime.datetime(2025, 5, 24, 5, 29, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 24, 5, 29, 59, tzinfo=datetime.timezone.utc), title='A Study of Semi-Fungible Token based Wi-Fi Access Control', authors=[arxiv.Result.Author('Litao Ye'), arxiv.Result.Author('Bin Chen'), arxiv.Result.Author('Chen Sun'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Peichang Zhang'), arxiv.Result.Author('Shengli Zhang')], summary=\"Current Wi-Fi authentication methods face issues such as insufficient\\nsecurity, user privacy leakage, high management costs, and difficulty in\\nbilling. To address these challenges, a Wi-Fi access control solution based on\\nblockchain smart contracts is proposed. Firstly, semi-fungible Wi-Fi tokens\\n(SFWTs) are designed using the ERC1155 token standard as credentials for users\\nto access Wi-Fi. Secondly, a Wi-Fi access control system based on SFWTs is\\ndeveloped to securely verify and manage the access rights of Wi-Fi users.\\nExperimental results demonstrate that SFWTs, designed based on the ERC1155\\nstandard, along with the SFWT access right verification process, can\\nsignificantly reduce Wi-Fi operating costs and authentication time, effectively\\nmeeting users' needs for safe and convenient Wi-Fi access.\", comment=None, journal_ref=None, doi='10.1109/ICCT62411.2024.10946664', primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICCT62411.2024.10946664', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2505.18518v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.18518v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.18323v1', updated=datetime.datetime(2025, 5, 23, 19, 28, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 23, 19, 28, 45, tzinfo=datetime.timezone.utc), title='Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation', authors=[arxiv.Result.Author('Nicolas Küchler'), arxiv.Result.Author('Ivan Petrov'), arxiv.Result.Author('Conrad Grobler'), arxiv.Result.Author('Ilia Shumailov')], summary='For nearly a decade the academic community has investigated backdoors in\\nneural networks, primarily focusing on classification tasks where adversaries\\nmanipulate the model prediction. While demonstrably malicious, the immediate\\nreal-world impact of such prediction-altering attacks has remained unclear. In\\nthis paper we introduce a novel and significantly more potent class of\\nbackdoors that builds upon recent advancements in architectural backdoors. We\\ndemonstrate how these backdoors can be specifically engineered to exploit\\nbatched inference, a common technique for hardware utilization, enabling\\nlarge-scale user data manipulation and theft. By targeting the batching\\nprocess, these architectural backdoors facilitate information leakage between\\nconcurrent user requests and allow attackers to fully control model responses\\ndirected at other users within the same batch. In other words, an attacker who\\ncan change the model architecture can set and steal model inputs and outputs of\\nother users within the same batch. We show that such attacks are not only\\nfeasible but also alarmingly effective, can be readily injected into prevalent\\nmodel architectures, and represent a truly malicious threat to user privacy and\\nsystem integrity. Critically, to counteract this new class of vulnerabilities,\\nwe propose a deterministic mitigation strategy that provides formal guarantees\\nagainst this new attack vector, unlike prior work that relied on Large Language\\nModels to find the backdoors. Our mitigation strategy employs a novel\\nInformation Flow Control mechanism that analyzes the model graph and proves\\nnon-interference between different user inputs within the same batch. Using our\\nmitigation strategy we perform a large scale analysis of models hosted through\\nHugging Face and find over 200 models that introduce (unintended) information\\nleakage between batch entries due to the use of dynamic quantization.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.18323v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.18323v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.15156v1', updated=datetime.datetime(2025, 5, 21, 6, 21, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 21, 6, 21, 21, tzinfo=datetime.timezone.utc), title='Privacy-Preserving Socialized Recommendation based on Multi-View Clustering in a Cloud Environment', authors=[arxiv.Result.Author('Cheng Guo'), arxiv.Result.Author('Jing Jia'), arxiv.Result.Author('Peng Wang'), arxiv.Result.Author('Jing Zhang')], summary=\"Recommendation as a service has improved the quality of our lives and plays a\\nsignificant role in variant aspects. However, the preference of users may\\nreveal some sensitive information, so that the protection of privacy is\\nrequired. In this paper, we propose a privacy-preserving, socialized,\\nrecommendation protocol that introduces information collected from online\\nsocial networks to enhance the quality of the recommendation. The proposed\\nscheme can calculate the similarity between users to determine their potential\\nrelationships and interests, and it also can protect the users' privacy from\\nleaking to an untrusted third party. The security analysis and experimental\\nresults showed that our proposed scheme provides excellent performance and is\\nfeasible for real-world applications.\", comment='6 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.15156v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.15156v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.14959v1', updated=datetime.datetime(2025, 5, 20, 22, 38, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 20, 22, 38, 50, tzinfo=datetime.timezone.utc), title='Privacy Preserving Conversion Modeling in Data Clean Room', authors=[arxiv.Result.Author('Kungang Li'), arxiv.Result.Author('Xiangyi Chen'), arxiv.Result.Author('Ling Leng'), arxiv.Result.Author('Jiajing Xu'), arxiv.Result.Author('Jiankai Sun'), arxiv.Result.Author('Behnam Rezaei')], summary='In the realm of online advertising, accurately predicting the conversion rate\\n(CVR) is crucial for enhancing advertising efficiency and user satisfaction.\\nThis paper addresses the challenge of CVR prediction while adhering to user\\nprivacy preferences and advertiser requirements. Traditional methods face\\nobstacles such as the reluctance of advertisers to share sensitive conversion\\ndata and the limitations of model training in secure environments like data\\nclean rooms. We propose a novel model training framework that enables\\ncollaborative model training without sharing sample-level gradients with the\\nadvertising platform. Our approach introduces several innovative components:\\n(1) utilizing batch-level aggregated gradients instead of sample-level\\ngradients to minimize privacy risks; (2) applying adapter-based\\nparameter-efficient fine-tuning and gradient compression to reduce\\ncommunication costs; and (3) employing de-biasing techniques to train the model\\nunder label differential privacy, thereby maintaining accuracy despite\\nprivacy-enhanced label perturbations. Our experimental results, conducted on\\nindustrial datasets, demonstrate that our method achieves competitive ROCAUC\\nperformance while significantly decreasing communication overhead and complying\\nwith both advertiser privacy requirements and user privacy choices. This\\nframework establishes a new standard for privacy-preserving, high-performance\\nCVR prediction in the digital advertising landscape.', comment=\"Published in Proceedings of the 18th ACM Conference on Recommender\\n  Systems. 2024 (RecSys '24)\", journal_ref=None, doi='10.1145/3640457.3688054', primary_category='cs.LG', categories=['cs.LG', 'cs.IR', 'H.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3640457.3688054', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2505.14959v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.14959v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.14549v1', updated=datetime.datetime(2025, 5, 20, 16, 5, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 20, 16, 5, 5, tzinfo=datetime.timezone.utc), title='Can Large Language Models Really Recognize Your Name?', authors=[arxiv.Result.Author('Dzung Pham'), arxiv.Result.Author('Peter Kairouz'), arxiv.Result.Author('Niloofar Mireshghallah'), arxiv.Result.Author('Eugene Bagdasarian'), arxiv.Result.Author('Chau Minh Pham'), arxiv.Result.Author('Amir Houmansadr')], summary='Large language models (LLMs) are increasingly being used to protect sensitive\\nuser data. However, current LLM-based privacy solutions assume that these\\nmodels can reliably detect personally identifiable information (PII),\\nparticularly named entities. In this paper, we challenge that assumption by\\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\\nthat modern LLMs regularly overlook human names even in short text snippets due\\nto ambiguous contexts, which cause the names to be misinterpreted or\\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\\nhuman names, leveraging the name regularity bias phenomenon, embedded within\\nconcise text snippets along with benign prompt injections. Our experiments on\\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\\nof ambiguous names drops by 20--40% compared to more recognizable names.\\nFurthermore, ambiguous human names are four times more likely to be ignored in\\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\\ninjections are present. These findings highlight the underexplored risks of\\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\\nmore systematic investigation into their privacy failure modes.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.14549v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.14549v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.12791v1', updated=datetime.datetime(2025, 5, 19, 7, 23, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 19, 7, 23, 46, tzinfo=datetime.timezone.utc), title='Unlearning for Federated Online Learning to Rank: A Reproducibility Study', authors=[arxiv.Result.Author('Yiling Tao'), arxiv.Result.Author('Shuyi Wang'), arxiv.Result.Author('Jiaxi Yang'), arxiv.Result.Author('Guido Zuccon')], summary='This paper reports on findings from a comparative study on the effectiveness\\nand efficiency of federated unlearning strategies within Federated Online\\nLearning to Rank (FOLTR), with specific attention to systematically analysing\\nthe unlearning capabilities of methods in a verifiable manner.\\n  Federated approaches to ranking of search results have recently garnered\\nattention to address users privacy concerns. In FOLTR, privacy is safeguarded\\nby collaboratively training ranking models across decentralized data sources,\\npreserving individual user data while optimizing search results based on\\nimplicit feedback, such as clicks.\\n  Recent legislation introduced across numerous countries is establishing the\\nso called \"the right to be forgotten\", according to which services based on\\nmachine learning models like those in FOLTR should provide capabilities that\\nallow users to remove their own data from those used to train models. This has\\nsparked the development of unlearning methods, along with evaluation practices\\nto measure whether unlearning of a user data successfully occurred. Current\\nevaluation practices are however often controversial, necessitating the use of\\nmultiple metrics for a more comprehensive assessment -- but previous proposals\\nof unlearning methods only used single evaluation metrics.\\n  This paper addresses this limitation: our study rigorously assesses the\\neffectiveness of unlearning strategies in managing both under-unlearning and\\nover-unlearning scenarios using adapted, and newly proposed evaluation metrics.\\nThanks to our detailed analysis, we uncover the strengths and limitations of\\nfive unlearning strategies, offering valuable insights into optimizing\\nfederated unlearning to balance data privacy and system performance within\\nFOLTR. We publicly release our code and complete results at\\nhttps://github.com/Iris1026/Unlearning-for-FOLTR.git.', comment='Accepted at SIGIR2025', journal_ref=None, doi=None, primary_category='cs.IR', categories=['cs.IR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.12791v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.12791v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.11417v1', updated=datetime.datetime(2025, 5, 16, 16, 29, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 16, 16, 29, 21, tzinfo=datetime.timezone.utc), title='EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions', authors=[arxiv.Result.Author('Patryk Bartkowiak'), arxiv.Result.Author('Michal Podstawski')], summary='This paper introduces a novel dataset and evaluation benchmark designed to\\nassess and improve small language models deployable on edge devices, with a\\nfocus on user profiling from multi-session natural language interactions in\\nsmart home environments. At the core of the dataset are structured user\\nprofiles, each defined by a set of routines - context-triggered, repeatable\\npatterns of behavior that govern how users interact with their home systems.\\nUsing these profiles as input, a large language model (LLM) generates\\ncorresponding interaction sessions that simulate realistic, diverse, and\\ncontext-aware dialogues between users and their devices.\\n  The primary task supported by this dataset is profile reconstruction:\\ninferring user routines and preferences solely from interactions history. To\\nassess how well current models can perform this task under realistic\\nconditions, we benchmarked several state-of-the-art compact language models and\\ncompared their performance against large foundation models. Our results show\\nthat while small models demonstrate some capability in reconstructing profiles,\\nthey still fall significantly short of large models in accurately capturing\\nuser behavior. This performance gap poses a major challenge - particularly\\nbecause on-device processing offers critical advantages, such as preserving\\nuser privacy, minimizing latency, and enabling personalized experiences without\\nreliance on the cloud. By providing a realistic, structured testbed for\\ndeveloping and evaluating behavioral modeling under these constraints, our\\ndataset represents a key step toward enabling intelligent, privacy-respecting\\nAI systems that learn and adapt directly on user-owned devices.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.11417v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.11417v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.10903v1', updated=datetime.datetime(2025, 5, 16, 6, 15, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 16, 6, 15, 31, tzinfo=datetime.timezone.utc), title='On the Security Risks of ML-based Malware Detection Systems: A Survey', authors=[arxiv.Result.Author('Ping He'), arxiv.Result.Author('Yuhao Mao'), arxiv.Result.Author('Changjiang Li'), arxiv.Result.Author('Lorenzo Cavallaro'), arxiv.Result.Author('Ting Wang'), arxiv.Result.Author('Shouling Ji')], summary='Malware presents a persistent threat to user privacy and data integrity. To\\ncombat this, machine learning-based (ML-based) malware detection (MD) systems\\nhave been developed. However, these systems have increasingly been attacked in\\nrecent years, undermining their effectiveness in practice. While the security\\nrisks associated with ML-based MD systems have garnered considerable attention,\\nthe majority of prior works is limited to adversarial malware examples, lacking\\na comprehensive analysis of practical security risks. This paper addresses this\\ngap by utilizing the CIA principles to define the scope of security risks. We\\nthen deconstruct ML-based MD systems into distinct operational stages, thus\\ndeveloping a stage-based taxonomy. Utilizing this taxonomy, we summarize the\\ntechnical progress and discuss the gaps in the attack and defense proposals\\nrelated to the ML-based MD systems within each stage. Subsequently, we conduct\\ntwo case studies, using both inter-stage and intra-stage analyses according to\\nthe stage-based taxonomy to provide new empirical insights. Based on these\\nanalyses and insights, we suggest potential future directions from both\\ninter-stage and intra-stage perspectives.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.10903v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.10903v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.10500v1', updated=datetime.datetime(2025, 5, 15, 17, 1, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 15, 17, 1, 52, tzinfo=datetime.timezone.utc), title='Quantized Approximate Signal Processing (QASP): Towards Homomorphic Encryption for audio', authors=[arxiv.Result.Author('Tu Duyen Nguyen'), arxiv.Result.Author('Adrien Lesage'), arxiv.Result.Author('Clotilde Cantini'), arxiv.Result.Author('Rachid Riad')], summary='Audio and speech data are increasingly used in machine learning applications\\nsuch as speech recognition, speaker identification, and mental health\\nmonitoring. However, the passive collection of this data by audio listening\\ndevices raises significant privacy concerns. Fully homomorphic encryption (FHE)\\noffers a promising solution by enabling computations on encrypted data and\\npreserving user privacy. Despite its potential, prior attempts to apply FHE to\\naudio processing have faced challenges, particularly in securely computing time\\nfrequency representations, a critical step in many audio tasks.\\n  Here, we addressed this gap by introducing a fully secure pipeline that\\ncomputes, with FHE and quantized neural network operations, four fundamental\\ntime-frequency representations: Short-Time Fourier Transform (STFT), Mel\\nfilterbanks, Mel-frequency cepstral coefficients (MFCCs), and gammatone\\nfilters. Our methods also support the private computation of audio descriptors\\nand convolutional neural network (CNN) classifiers. Besides, we proposed\\napproximate STFT algorithms that lighten computation and bit use for\\nstatistical and machine learning analyses.\\n  We ran experiments on the VocalSet and OxVoc datasets demonstrating the fully\\nprivate computation of our approach. We showed significant performance\\nimprovements with STFT approximation in private statistical analysis of audio\\nmarkers, and for vocal exercise classification with CNNs. Our results reveal\\nthat our approximations substantially reduce error rates compared to\\nconventional STFT implementations in FHE. We also demonstrated a fully private\\nclassification based on the raw audio for gender and vocal exercise\\nclassification. Finally, we provided a practical heuristic for parameter\\nselection, making quantized approximate signal processing accessible to\\nresearchers and practitioners aiming to protect sensitive audio data.', comment='34 pages, 5 figures', journal_ref=None, doi=None, primary_category='eess.AS', categories=['eess.AS', 'cs.CR', 'cs.SD'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.10500v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.10500v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.09702v1', updated=datetime.datetime(2025, 5, 14, 18, 4, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 14, 18, 4, 2, tzinfo=datetime.timezone.utc), title='Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing', authors=[arxiv.Result.Author('Yezi Liu'), arxiv.Result.Author('Prathyush Poduval'), arxiv.Result.Author('Wenjun Huang'), arxiv.Result.Author('Yang Ni'), arxiv.Result.Author('Hanning Chen'), arxiv.Result.Author('Mohsen Imani')], summary='Graph unlearning is a crucial approach for protecting user privacy by erasing\\nthe influence of user data on trained graph models. Recent developments in\\ngraph unlearning methods have primarily focused on maintaining model prediction\\nperformance while removing user information. However, we have observed that\\nwhen user information is deleted from the model, the prediction distribution\\nacross different sensitive groups often changes. Furthermore, graph models are\\nshown to be prone to amplifying biases, making the study of fairness in graph\\nunlearning particularly important. This raises the question: Does graph\\nunlearning actually introduce bias? Our findings indicate that the predictions\\nof post-unlearning models become highly correlated with sensitive attributes,\\nconfirming the introduction of bias in the graph unlearning process. To address\\nthis issue, we propose a fair graph unlearning method, FGU. To guarantee\\nprivacy, FGU trains shard models on partitioned subgraphs, unlearns the\\nrequested data from the corresponding subgraphs, and retrains the shard models\\non the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing\\nprocess: it first enables shard-level fairness by incorporating a fairness\\nregularizer in the shard model retraining, and then achieves global-level\\nfairness by aligning all shard models to minimize global disparity. Our\\nexperiments demonstrate that FGU achieves superior fairness while maintaining\\nprivacy and accuracy. Additionally, FGU is robust to diverse unlearning\\nrequests, ensuring fairness and utility performance across various data\\ndistributions.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.09702v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.09702v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.06738v3', updated=datetime.datetime(2025, 6, 15, 8, 41, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 10, 19, 6, 37, tzinfo=datetime.timezone.utc), title='I Know What You Said: Unveiling Hardware Cache Side-Channels in Local Large Language Model Inference', authors=[arxiv.Result.Author('Zibo Gao'), arxiv.Result.Author('Junjie Hu'), arxiv.Result.Author('Feng Guo'), arxiv.Result.Author('Yixin Zhang'), arxiv.Result.Author('Yinglong Han'), arxiv.Result.Author('Siyuan Liu'), arxiv.Result.Author('Haiyang Li'), arxiv.Result.Author('Zhiqiang Lv')], summary=\"Large Language Models (LLMs) that can be deployed locally have recently\\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\\nGoogle, and Intel playing significant roles in their development. However, the\\nsecurity of local LLMs through the lens of hardware cache side-channels remains\\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\\nlocal LLM inference: token value and token position leakage, which can expose\\nboth the victim's input and output text, thereby compromising user privacy.\\nSpecifically, we found that adversaries can infer the token values from the\\ncache access patterns of the token embedding operation, and deduce the token\\npositions from the timing of autoregressive decoding phases. To demonstrate the\\npotential of these leaks, we design a novel eavesdropping attack framework\\ntargeting both open-source and proprietary LLM inference systems. The attack\\nframework does not directly interact with the victim's LLM and can be executed\\nwithout privilege.\\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\\nLlama, Falcon, and Gemma), and the results show that our attack achieves\\npromising accuracy. The restored output and input text have an average edit\\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\\nand 98.0% (output).\", comment='Submitted for review in January 22, 2025, revised under shepherding', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'K.6.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.06738v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.06738v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.06305v1', updated=datetime.datetime(2025, 5, 8, 4, 42, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 8, 4, 42, 17, tzinfo=datetime.timezone.utc), title='User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data', authors=[arxiv.Result.Author('Haowei Yang'), arxiv.Result.Author('Qingyi Lu'), arxiv.Result.Author('Yang Wang'), arxiv.Result.Author('Sibei Liu'), arxiv.Result.Author('Jiayun Zheng'), arxiv.Result.Author('Ao Xiang')], summary='With the widespread application of large language models (LLMs), user privacy\\nprotection has become a significant research topic. Existing privacy preference\\nmodeling methods often rely on large-scale user data, making effective privacy\\npreference analysis challenging in data-limited environments. This study\\nexplores how LLMs can analyze user behavior related to privacy protection in\\nscenarios with limited data and proposes a method that integrates Few-shot\\nLearning and Privacy Computing to model user privacy preferences. The research\\nutilizes anonymized user privacy settings data, survey responses, and simulated\\ndata, comparing the performance of traditional modeling approaches with\\nLLM-based methods. Experimental results demonstrate that, even with limited\\ndata, LLMs significantly improve the accuracy of privacy preference modeling.\\nAdditionally, incorporating Differential Privacy and Federated Learning further\\nreduces the risk of user data exposure. The findings provide new insights into\\nthe application of LLMs in privacy protection and offer theoretical support for\\nadvancing privacy computing and user behavior analysis.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.06305v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.06305v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.01292v1', updated=datetime.datetime(2025, 5, 2, 14, 9, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 5, 2, 14, 9, 56, tzinfo=datetime.timezone.utc), title='Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams', authors=[arxiv.Result.Author('Xinyu Li'), arxiv.Result.Author('Xuebin Ren'), arxiv.Result.Author('Shusen Yang'), arxiv.Result.Author('Liang Shi'), arxiv.Result.Author('Chia-Mu Yu')], summary=\"Local Differential Privacy (LDP) enables massive data collection and analysis\\nwhile protecting end users' privacy against untrusted aggregators. It has been\\napplied to various data types (e.g., categorical, numerical, and graph data)\\nand application settings (e.g., static and streaming). Recent findings indicate\\nthat LDP protocols can be easily disrupted by poisoning or manipulation\\nattacks, which leverage injected/corrupted fake users to send crafted data\\nconforming to the LDP reports. However, current attacks primarily target static\\nprotocols, neglecting the security of LDP protocols in the streaming settings.\\nOur research fills the gap by developing novel fine-grained manipulation\\nattacks to LDP protocols for data streams. By reviewing the attack surfaces in\\nexisting algorithms, We introduce a unified attack framework with composable\\nmodules, which can manipulate the LDP estimated stream toward a target stream.\\nOur attack framework can adapt to state-of-the-art streaming LDP algorithms\\nwith different analytic tasks (e.g., frequency and mean) and LDP models\\n(event-level, user-level, w-event level). We validate our attacks theoretically\\nand through extensive experiments on real-world datasets, and finally explore a\\npossible defense mechanism for mitigating these attacks.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.01292v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.01292v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.20926v1', updated=datetime.datetime(2025, 4, 29, 16, 39, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 29, 16, 39, 50, tzinfo=datetime.timezone.utc), title='Bipartite Randomized Response Mechanism for Local Differential Privacy', authors=[arxiv.Result.Author('Shun Zhang'), arxiv.Result.Author('Hai Zhu'), arxiv.Result.Author('Zhili Chen'), arxiv.Result.Author('Neal N. Xiong')], summary=\"With the increasing importance of data privacy, Local Differential Privacy\\n(LDP) has recently become a strong measure of privacy for protecting each\\nuser's privacy from data analysts without relying on a trusted third party. In\\nmany cases, both data providers and data analysts hope to maximize the utility\\nof released data. In this paper, we study the fundamental trade-off formulated\\nas a constrained optimization problem: maximizing data utility subject to the\\nconstraint of LDP budgets. In particular, the Generalized Randomized Response\\n(GRR) treats all discrete data equally except for the true data. For this, we\\nintroduce an adaptive LDP mechanism called Bipartite Randomized Response (BRR),\\nwhich solves the above privacy-utility maximization problem from the global\\nstandpoint. We prove that for any utility function and any privacy level,\\nsolving the maximization problem is equivalent to confirming how many\\nhigh-utility data to be treated equally as the true data on release\\nprobability, the outcome of which gives the optimal randomized response.\\nFurther, solving this linear program can be computationally cheap in theory.\\nSeveral examples of utility functions defined by distance metrics and\\napplications in decision trees and deep learning are presented. The results of\\nvarious experiments show that our BRR significantly outperforms the\\nstate-of-the-art LDP mechanisms of both continuous and distributed types.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.20926v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.20926v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.20296v1', updated=datetime.datetime(2025, 4, 28, 22, 36, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 28, 22, 36, 6, tzinfo=datetime.timezone.utc), title='SoK: A Survey of Mixing Techniques and Mixers for Cryptocurrencies', authors=[arxiv.Result.Author('Juraj Mariani'), arxiv.Result.Author('Ivan Homoliak')], summary='Blockchain technologies have overturned the digital finance industry by\\nintroducing a decentralized pseudonymous means of monetary transfer. The\\npseudonymous nature introduced privacy concerns, enabling various\\ndeanonymization techniques, which in turn spurred development of stronger\\nanonymity-preserving measures. The purpose of this paper is to create a\\ncomprehensive survey of mixing techniques and implementations within the vast\\necosystem surrounding anonymization tools and mechanisms available in\\nblockchain cryptocurrencies. First, we begin by reviewing classifications used\\nin the field. Then, we survey various obfuscation techniques, helping to delve\\ninto actual implementations and combinations of these techniques. Next, we\\nidentify the positive and negative attributes of the approaches and\\nimplementations included. Moreover, we examine the implications of\\nanonymization tools for user privacy, including their effectiveness in\\npreserving anonymity and susceptibility to attacks and vulnerabilities.\\nFinally, we discuss the challenges and innovations for extending mixing\\nservices into the realm of smart contracts or cross-chain space.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.20296v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.20296v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.20260v1', updated=datetime.datetime(2025, 4, 28, 21, 5, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 28, 21, 5, 3, tzinfo=datetime.timezone.utc), title='SA2FE: A Secure, Anonymous, Auditable, and Fair Edge Computing Service Offloading Framework', authors=[arxiv.Result.Author('Xiaojian Wang'), arxiv.Result.Author('Huayue Gu'), arxiv.Result.Author('Zhouyu Li'), arxiv.Result.Author('Fangtong Zhou'), arxiv.Result.Author('Ruozhou Yu'), arxiv.Result.Author('Dejun Yang'), arxiv.Result.Author('Guoliang Xue')], summary='The inclusion of pervasive computing devices in a democratized edge computing\\necosystem can significantly expand the capability and coverage of near-end\\ncomputing for large-scale applications. However, offloading user tasks to\\nheterogeneous and decentralized edge devices comes with the dual risk of both\\nendangered user data security and privacy due to the curious base station or\\nmalicious edge servers, and unfair offloading and malicious attacks targeting\\nedge servers from other edge servers and/or users. Existing solutions to edge\\naccess control and offloading either rely on \"always-on\" cloud servers with\\nreduced edge benefits or fail to protect sensitive user service information. To\\naddress these challenges, this paper presents SA2FE, a novel framework for edge\\naccess control, offloading and accounting. We design a rerandomizable puzzle\\nprimitive and a corresponding scheme to protect sensitive service information\\nfrom eavesdroppers and ensure fair offloading decisions, while a blind\\ntoken-based scheme safeguards user privacy, prevents double spending, and\\nensures usage accountability. The security of SA2FE is proved under the\\nUniversal Composability framework, and its performance and scalability are\\ndemonstrated with implementation on commodity mobile devices and edge servers.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.20260v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.20260v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2505.11502v1', updated=datetime.datetime(2025, 4, 28, 7, 23, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 28, 7, 23, 8, tzinfo=datetime.timezone.utc), title='Hybrid Privacy Policy-Code Consistency Check using Knowledge Graphs and LLMs', authors=[arxiv.Result.Author('Zhenyu Mao'), arxiv.Result.Author('Xinxin Fan'), arxiv.Result.Author('Yifei Wang'), arxiv.Result.Author('Jacky Keung'), arxiv.Result.Author('Jialong Li')], summary=\"The increasing concern in user privacy misuse has accelerated research into\\nchecking consistencies between smartphone apps' declared privacy policies and\\ntheir actual behaviors. Recent advances in Large Language Models (LLMs) have\\nintroduced promising techniques for semantic comparison, but these methods\\noften suffer from low accuracies and expensive computational costs. To address\\nthis problem, this paper proposes a novel hybrid approach that integrates 1)\\nknowledge graph-based deterministic checking to ensure higher accuracy, and 2)\\nLLMs exclusively used for preliminary semantic analysis to save computational\\ncosts. Preliminary evaluation indicates this hybrid approach not only achieves\\n37.63% increase in precision and 23.13% increase F1-score but also consumes\\n93.5% less tokens and 87.3% shorter time.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2505.11502v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2505.11502v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.17677v2', updated=datetime.datetime(2025, 6, 30, 17, 30, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 24, 15, 47, 20, tzinfo=datetime.timezone.utc), title='INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models', authors=[arxiv.Result.Author('Jarne Thys'), arxiv.Result.Author('Sebe Vanbrabant'), arxiv.Result.Author('Davy Vanacken'), arxiv.Result.Author('Gustavo Rovelo Ruiz')], summary=\"The rise of AI, especially Large Language Models, presents challenges and\\nopportunities to integrate such technology into the classroom. AI has the\\npotential to revolutionize education by helping teaching staff with various\\ntasks, such as personalizing their teaching methods, but it also raises\\nconcerns, for example, about the degradation of student-teacher interactions\\nand user privacy. Based on interviews with teaching staff, this paper\\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\\nteaching staff and students in the process of solving exercises. INSIGHT has a\\nmodular design that allows it to be integrated into various higher education\\ncourses. We analyze students' questions to an LLM by extracting keywords, which\\nwe use to dynamically build an FAQ from students' questions and provide new\\ninsights for the teaching staff to use for more personalized face-to-face\\nsupport. Future work could build upon INSIGHT by using the collected data to\\nprovide adaptive learning and adjust content based on student progress and\\nlearning styles to offer a more interactive and inclusive learning experience.\", comment='Accepted author version for the D-SAIL Workshop - Transformative\\n  Curriculum Design: Digitalisation, Sustainability, and AI Literacy for 21st\\n  Century Learning, July 22, 2025, Palermo, Italy', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.17677v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.17677v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.16423v1', updated=datetime.datetime(2025, 4, 23, 5, 15, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 23, 5, 15, 43, tzinfo=datetime.timezone.utc), title='Advancing Radar Hand Gesture Recognition: A Hybrid Spectrum Synthetic Framework Merging Simulation with Neural Networks', authors=[arxiv.Result.Author('Jiaqi Tang'), arxiv.Result.Author('Xinbo Xu'), arxiv.Result.Author('Yinsong Xu'), arxiv.Result.Author('Qingchao Chen')], summary='Millimeter wave (mmWave) radar sensors play a vital role in hand gesture\\nrecognition (HGR) by detecting subtle motions while preserving user privacy.\\nHowever, the limited scale of radar datasets hinders the performance. Existing\\nsynthetic data generation methods fall short in two key areas. On the one hand,\\nmodeling-based approaches fail to accurately simulate the wave propagation and\\nreflection at the hand-gesture level, facing unique complexities such as\\ndiffraction and occlusion. On the other hand, generative model-based methods\\nare hard to converge while radar data is limited, lacking interpretability, and\\nsometimes fail to produce kinematically plausible results. To overcome these\\nlimitations, we propose a novel hybrid spectrum synthetic framework leveraging\\nvisual hand gesture data. It combines a cylinder mesh-based hand reflection\\nmodel with a small-scale neural network called RadarWeightNet, which focuses on\\nassigning weights to simulated signals. Our framework addresses two key\\nchallenges: achieving accurate simulation of complex hand geometry and bridging\\nthe simulation-to-real gap in a data-driven manner while preserving\\ninterpretability, which balances physical accuracy with machine learning\\nadaptability. We tested our framework under extreme scenarios where radar data\\nis scarce. The results demonstrate the effectiveness of our hybrid framework,\\nachieving up to 63% SSIM in synthetic performance and up to 30% improvement in\\nclassification performance in few-shot learning.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.16423v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.16423v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.15300v1', updated=datetime.datetime(2025, 4, 17, 6, 41, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 17, 6, 41, 30, tzinfo=datetime.timezone.utc), title='Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions', authors=[arxiv.Result.Author('Chaoyue Niu'), arxiv.Result.Author('Yucheng Ding'), arxiv.Result.Author('Junhui Lu'), arxiv.Result.Author('Zhengxiang Huang'), arxiv.Result.Author('Hang Zeng'), arxiv.Result.Author('Yutong Dai'), arxiv.Result.Author('Xuezhen Tu'), arxiv.Result.Author('Chengfei Lv'), arxiv.Result.Author('Fan Wu'), arxiv.Result.Author('Guihai Chen')], summary='The conventional cloud-based large model learning framework is increasingly\\nconstrained by latency, cost, personalization, and privacy concerns. In this\\nsurvey, we explore an emerging paradigm: collaborative learning between\\non-device small model and cloud-based large model, which promises low-latency,\\ncost-efficient, and personalized intelligent services while preserving user\\nprivacy. We provide a comprehensive review across hardware, system, algorithm,\\nand application layers. At each layer, we summarize key problems and recent\\nadvances from both academia and industry. In particular, we categorize\\ncollaboration algorithms into data-based, feature-based, and parameter-based\\nframeworks. We also review publicly available datasets and evaluation metrics\\nwith user-level or device-level consideration tailored to collaborative\\nlearning settings. We further highlight real-world deployments, ranging from\\nrecommender systems and mobile livestreaming to personal intelligent\\nassistants. We finally point out open research directions to guide future\\ndevelopment in this rapidly evolving field.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.DC', 'cs.MA'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.15300v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.15300v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.13212v1', updated=datetime.datetime(2025, 4, 16, 9, 52, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 16, 9, 52, 4, tzinfo=datetime.timezone.utc), title='I Know What You Bought Last Summer: Investigating User Data Leakage in E-Commerce Platforms', authors=[arxiv.Result.Author('Ioannis Vlachogiannakis'), arxiv.Result.Author('Emmanouil Papadogiannakis'), arxiv.Result.Author('Panagiotis Papadopoulos'), arxiv.Result.Author('Nicolas Kourtellis'), arxiv.Result.Author('Evangelos Markatos')], summary='In the digital age, e-commerce has transformed the way consumers shop,\\noffering convenience and accessibility. Nevertheless, concerns about the\\nprivacy and security of personal information shared on these platforms have\\nrisen. In this work, we investigate user privacy violations, noting the risks\\nof data leakage to third-party entities. Utilizing a semi-automated data\\ncollection approach, we examine a selection of popular online e-shops,\\nrevealing that nearly 30% of them violate user privacy by disclosing personal\\ninformation to third parties. We unveil how minimal user interaction across\\nmultiple e-commerce websites can result in a comprehensive privacy breach. We\\nobserve significant data-sharing patterns with platforms like Facebook, which\\nuse personal information to build user profiles and link them to social media\\naccounts.', comment='SECRYPT 2025 - 22nd International Conference on Security and\\n  Cryptography, 8 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.13212v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.13212v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.13880v2', updated=datetime.datetime(2025, 6, 2, 18, 33, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 2, 2, 24, 30, tzinfo=datetime.timezone.utc), title='An AI-powered Public Health Automated Kiosk System for Personalized Care: An Experimental Pilot Study', authors=[arxiv.Result.Author('Sonya Falahati'), arxiv.Result.Author('Morteza Alizadeh'), arxiv.Result.Author('Fatemeh Ghazipour'), arxiv.Result.Author('Zhino Safahi'), arxiv.Result.Author('Navid Khaledian'), arxiv.Result.Author('Mohammad R. Salmanpour')], summary='Background: The HERMES Kiosk (Healthcare Enhanced Recommendations through\\nArtificial Intelligence & Expertise System) is designed to provide personalized\\nOver-the-Counter (OTC) medication recommendations, addressing the limitations\\nof traditional health kiosks. It integrates an advanced GAMENet model enhanced\\nwith Graph Attention Networks (GAT) and Multi-Head Cross-Attention (MHCA) while\\nensuring user privacy through federated learning. This paper outlines the\\nconceptual design and architecture of HERMES, with a focus on deployment in\\nhigh-traffic public areas. Methods: HERMES analyzes self-reported symptoms and\\nanonymized medical histories using AI algorithms to generate context-aware OTC\\nmedication recommendations. The system was initially trained using Electronic\\nHealth Records (EHR) from the MIMIC-III dataset (6,350 patients) and Drug-Drug\\nInteraction (DDI) data from the TWOSIDES database, incorporating the top 90\\nseverity DDI types. Real-time DDI checks and ATC-mapped drug codes further\\nimprove safety. The kiosk is designed for accessibility, offering multilingual\\nsupport, large fonts, voice commands, and Braille compatibility. A built-in\\nhealth education library promotes preventive care and health literacy. A survey\\nwas conducted among 10 medical professionals to evaluate its potential\\napplications in medicine. Results: Preliminary results show that the enhanced\\nGAMENet model achieved a Precision-Recall AUC (PRAUC) of 0.74, outperforming\\nthe original model. These findings suggest a strong potential for delivering\\naccurate and secure healthcare recommendations in public settings. Conclusion:\\nHERMES demonstrates how AI-driven, privacy-preserving kiosks can enhance public\\nhealth access, empower users, and alleviate burdens on healthcare systems.\\nFuture work will focus on real-world deployment, usability testing, and\\nscalability for broader adoption.', comment='16 pages, 3 figures, 2 tables', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'F.2.2; I.2.7'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.13880v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.13880v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.00858v1', updated=datetime.datetime(2025, 4, 1, 14, 49, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 4, 1, 14, 49, 39, tzinfo=datetime.timezone.utc), title='Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems', authors=[arxiv.Result.Author('Weifei Jin'), arxiv.Result.Author('Yuxin Cao'), arxiv.Result.Author('Junjie Su'), arxiv.Result.Author('Derui Wang'), arxiv.Result.Author('Yedi Zhang'), arxiv.Result.Author('Minhui Xue'), arxiv.Result.Author('Jie Hao'), arxiv.Result.Author('Jin Song Dong'), arxiv.Result.Author('Yixian Yang')], summary=\"The widespread application of automatic speech recognition (ASR) supports\\nlarge-scale voice surveillance, raising concerns about privacy among users. In\\nthis paper, we concentrate on using adversarial examples to mitigate\\nunauthorized disclosure of speech privacy thwarted by potential eavesdroppers\\nin speech communications. While audio adversarial examples have demonstrated\\nthe capability to mislead ASR models or evade ASR surveillance, they are\\ntypically constructed through time-intensive offline optimization, restricting\\ntheir practicality in real-time voice communication. Recent work overcame this\\nlimitation by generating universal adversarial perturbations (UAPs) and\\nenhancing their transferability for black-box scenarios. However, they\\nintroduced excessive noise that significantly degrades audio quality and\\naffects human perception, thereby limiting their effectiveness in practical\\nscenarios. To address this limitation and protect live users' speech against\\nASR systems, we propose a novel framework, AudioShield. Central to this\\nframework is the concept of Transferable Universal Adversarial Perturbations in\\nthe Latent Space (LS-TUAP). By transferring the perturbations to the latent\\nspace, the audio quality is preserved to a large extent. Additionally, we\\npropose target feature adaptation to enhance the transferability of UAPs by\\nembedding target text features into the perturbations. Comprehensive evaluation\\non four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice\\nassistants, two LLM-powered ASR and one NN-based ASR demonstrates the\\nprotection superiority of AudioShield over existing competitors, and both\\nobjective and subjective evaluations indicate that AudioShield significantly\\nimproves the audio quality. Moreover, AudioShield also shows high effectiveness\\nin real-time end-to-end scenarios, and demonstrates strong resilience against\\nadaptive countermeasures.\", comment='Accept to USENIX Security 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.LG', 'cs.SD'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.00858v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.00858v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.23748v1', updated=datetime.datetime(2025, 3, 31, 5, 58, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 31, 5, 58, 57, tzinfo=datetime.timezone.utc), title='THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models', authors=[arxiv.Result.Author('Yujin Huang'), arxiv.Result.Author('Zhi Zhang'), arxiv.Result.Author('Qingchuan Zhao'), arxiv.Result.Author('Xingliang Yuan'), arxiv.Result.Author('Chunyang Chen')], summary=\"On-device deep learning (DL) has rapidly gained adoption in mobile apps,\\noffering the benefits of offline model inference and user privacy preservation\\nover cloud-based approaches. However, it inevitably stores models on user\\ndevices, introducing new vulnerabilities, particularly model-stealing attacks\\nand intellectual property infringement. While system-level protections like\\nTrusted Execution Environments (TEEs) provide a robust solution, practical\\nchallenges remain in achieving scalable on-device DL model protection,\\nincluding complexities in supporting third-party models and limited adoption in\\ncurrent mobile solutions. Advancements in TEE-enabled hardware, such as\\nNVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently,\\nwatermarking serves as a common defense against model theft but also faces\\nchallenges here as many mobile app developers lack corresponding machine\\nlearning expertise and the inherent read-only and inference-only nature of\\non-device DL models prevents third parties like app stores from implementing\\nexisting watermarking techniques in post-deployment models.\\n  To protect the intellectual property of on-device DL models, in this paper,\\nwe propose THEMIS, an automatic tool that lifts the read-only restriction of\\non-device DL models by reconstructing their writable counterparts and leverages\\nthe untrainable nature of on-device DL models to solve watermark parameters and\\nprotect the model owner's intellectual property. Extensive experimental results\\nacross various datasets and model structures show the superiority of THEMIS in\\nterms of different metrics. Further, an empirical investigation of 403\\nreal-world DL mobile apps from Google Play is performed with a success rate of\\n81.14%, showing the practicality of THEMIS.\", comment='To Appear in the 34th USENIX Security Symposium, August 13-15, 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.LG', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.23748v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.23748v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.23001v3', updated=datetime.datetime(2025, 4, 15, 9, 43, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 29, 7, 44, 34, tzinfo=datetime.timezone.utc), title='Buyer-Initiated Auction Mechanism for Data Redemption in Machine Unlearning', authors=[arxiv.Result.Author('Bin Han'), arxiv.Result.Author('Di Feng'), arxiv.Result.Author('Jie Wang'), arxiv.Result.Author('Hans D. Schotten')], summary=\"The rapid growth of artificial intelligence (AI) has raised privacy concerns\\nover user data, leading to regulations like the General Data Protection\\nRegulation (GDPR) and the California Consumer Privacy Act (CCPA). With the\\nessential toolbox provided by machine unlearning, AI service providers are now\\nable to remove user data from their trained models as well as the training\\ndatasets, so as to comply with such regulations. However, extensive data\\nredemption can be costly and degrade model accuracy. To balance the cost of\\nunlearning and the privacy protection, we propose a buyer-initiated auction\\nmechanism for data redemption, enabling the service provider to purchase data\\nfrom willing users with appropriate compensation. This approach does not\\nrequire the server to have any a priori knowledge about the users' privacy\\npreference, and provides an efficient solution for maximizing the social\\nwelfare in the investigated problem.\", comment='Submitted to IEEE GLOBECOM 2025', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.GT'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.23001v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.23001v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.08751v1', updated=datetime.datetime(2025, 3, 27, 22, 56, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 27, 22, 56, 41, tzinfo=datetime.timezone.utc), title='Research on the Design of a Short Video Recommendation System Based on Multimodal Information and Differential Privacy', authors=[arxiv.Result.Author('Haowei Yang'), arxiv.Result.Author('Lei Fu'), arxiv.Result.Author('Qingyi Lu'), arxiv.Result.Author('Yue Fan'), arxiv.Result.Author('Tianle Zhang'), arxiv.Result.Author('Ruohan Wang')], summary='With the rapid development of short video platforms, recommendation systems\\nhave become key technologies for improving user experience and enhancing\\nplatform engagement. However, while short video recommendation systems leverage\\nmultimodal information (such as images, text, and audio) to improve\\nrecommendation effectiveness, they also face the severe challenge of user\\nprivacy leakage. This paper proposes a short video recommendation system based\\non multimodal information and differential privacy protection. First, deep\\nlearning models are used for feature extraction and fusion of multimodal data,\\neffectively improving recommendation accuracy. Then, a differential privacy\\nprotection mechanism suitable for recommendation scenarios is designed to\\nensure user data privacy while maintaining system performance. Experimental\\nresults show that the proposed method outperforms existing mainstream\\napproaches in terms of recommendation accuracy, multimodal fusion\\neffectiveness, and privacy protection performance, providing important insights\\nfor the design of recommendation systems for short video platforms.', comment=None, journal_ref=None, doi=None, primary_category='cs.IR', categories=['cs.IR', 'cs.AI', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.08751v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.08751v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.21010v1', updated=datetime.datetime(2025, 3, 26, 21, 58, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 26, 21, 58, 19, tzinfo=datetime.timezone.utc), title='Privacy in Immersive Extended Reality: Exploring User Perceptions, Concerns, and Coping Strategies', authors=[arxiv.Result.Author('Hilda Hadan'), arxiv.Result.Author('Derrick M. Wang'), arxiv.Result.Author('Lennart E. Nacke'), arxiv.Result.Author('Leah Zhang-Kennedy')], summary=\"Extended Reality (XR) technology is changing online interactions, but its\\ngranular data collection sensors may be more invasive to user privacy than web,\\nmobile, and the Internet of Things technologies. Despite an increased interest\\nin studying developers' concerns about XR device privacy, user perceptions have\\nrarely been addressed. We surveyed 464 XR users to assess their awareness,\\nconcerns, and coping strategies around XR data in 18 scenarios. Our findings\\ndemonstrate that many factors, such as data types and sensitivity, affect\\nusers' perceptions of privacy in XR. However, users' limited awareness of XR\\nsensors' granular data collection capabilities, such as involuntary body\\nsignals of emotional responses, restricted the range of privacy-protective\\nstrategies they used. Our results highlight a need to enhance users' awareness\\nof data privacy threats in XR, design privacy-choice interfaces tailored to XR\\nenvironments, and develop transparent XR data practices.\", comment=\"25 pages, 4 figures, 8 tables. the 2024 CHI Conference on Human\\n  Factors in Computing Systems (CHI'24)\", journal_ref=None, doi='10.1145/3613904.3642104', primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3613904.3642104', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2503.21010v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.21010v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.19142v1', updated=datetime.datetime(2025, 3, 24, 20, 55, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 24, 20, 55, 18, tzinfo=datetime.timezone.utc), title='Activation Functions Considered Harmful: Recovering Neural Network Weights through Controlled Channels', authors=[arxiv.Result.Author('Jesse Spielman'), arxiv.Result.Author('David Oswald'), arxiv.Result.Author('Mark Ryan'), arxiv.Result.Author('Jo Van Bulck')], summary='With high-stakes machine learning applications increasingly moving to\\nuntrusted end-user or cloud environments, safeguarding pre-trained model\\nparameters becomes essential for protecting intellectual property and user\\nprivacy. Recent advancements in hardware-isolated enclaves, notably Intel SGX,\\nhold the promise to secure the internal state of machine learning applications\\neven against compromised operating systems. However, we show that privileged\\nsoftware adversaries can exploit input-dependent memory access patterns in\\ncommon neural network activation functions to extract secret weights and biases\\nfrom an SGX enclave.\\n  Our attack leverages the SGX-Step framework to obtain a noise-free,\\ninstruction-granular page-access trace. In a case study of an 11-input\\nregression network using the Tensorflow Microlite library, we demonstrate\\ncomplete recovery of all first-layer weights and biases, as well as partial\\nrecovery of parameters from deeper layers under specific conditions. Our novel\\nattack technique requires only 20 queries per input per weight to obtain all\\nfirst-layer weights and biases with an average absolute error of less than 1%,\\nimproving over prior model stealing attacks.\\n  Additionally, a broader ecosystem analysis reveals the widespread use of\\nactivation functions with input-dependent memory access patterns in popular\\nmachine learning frameworks (either directly or via underlying math libraries).\\nOur findings highlight the limitations of deploying confidential models in SGX\\nenclaves and emphasise the need for stricter side-channel validation of machine\\nlearning implementations, akin to the vetting efforts applied to secure\\ncryptographic libraries.', comment='17 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.19142v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.19142v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2504.07107v1', updated=datetime.datetime(2025, 3, 17, 10, 56, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 17, 10, 56, 46, tzinfo=datetime.timezone.utc), title='Guarding Digital Privacy: Exploring User Profiling and Security Enhancements', authors=[arxiv.Result.Author('Rishika Kohli'), arxiv.Result.Author('Shaifu Gupta'), arxiv.Result.Author('Manoj Singh Gaur')], summary=\"User profiling, the practice of collecting user information for personalized\\nrecommendations, has become widespread, driving progress in technology.\\nHowever, this growth poses a threat to user privacy, as devices often collect\\nsensitive data without their owners' awareness. This article aims to\\nconsolidate knowledge on user profiling, exploring various approaches and\\nassociated challenges. Through the lens of two companies sharing user data and\\nan analysis of 18 popular Android applications in India across various\\ncategories, including $\\\\textit{Social, Education, Entertainment, Travel,\\nShopping and Others}$, the article unveils privacy vulnerabilities. Further,\\nthe article propose an enhanced machine learning framework, employing decision\\ntrees and neural networks, that improves state-of-the-art classifiers in\\ndetecting personal information exposure. Leveraging the XAI (explainable\\nartificial intelligence) algorithm LIME (Local Interpretable Model-agnostic\\nExplanations), it enhances interpretability, crucial for reliably identifying\\nsensitive data. Results demonstrate a noteworthy performance boost, achieving a\\n$75.01\\\\%$ accuracy with a reduced training time of $3.62$ seconds for neural\\nnetworks. Concluding, the paper suggests research directions to strengthen\\ndigital security measures.\", comment='46 Pages, 8 tables, 9 figures', journal_ref=None, doi=None, primary_category='cs.IR', categories=['cs.IR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2504.07107v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.07107v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.12896v1', updated=datetime.datetime(2025, 3, 17, 7, 58, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 17, 7, 58, 5, tzinfo=datetime.timezone.utc), title='Safeguarding LLM Embeddings in End-Cloud Collaboration via Entropy-Driven Perturbation', authors=[arxiv.Result.Author('Shuaifan Jin'), arxiv.Result.Author('Xiaoyi Pang'), arxiv.Result.Author('Zhibo Wang'), arxiv.Result.Author('He Wang'), arxiv.Result.Author('Jiacheng Du'), arxiv.Result.Author('Jiahui Hu'), arxiv.Result.Author('Kui Ren')], summary='Recent studies improve on-device language model (LM) inference through\\nend-cloud collaboration, where the end device retrieves useful information from\\ncloud databases to enhance local processing, known as Retrieval-Augmented\\nGeneration (RAG). Typically, to retrieve information from the cloud while\\nsafeguarding privacy, the end device transforms original data into embeddings\\nwith a local embedding model. However, the recently emerging Embedding\\nInversion Attacks (EIAs) can still recover the original data from text\\nembeddings (e.g., training a recovery model to map embeddings back to original\\ntexts), posing a significant threat to user privacy. To address this risk, we\\npropose EntroGuard, an entropy-driven perturbation-based embedding privacy\\nprotection method, which can protect the privacy of text embeddings while\\nmaintaining retrieval accuracy during the end-cloud collaboration.\\nSpecifically, to defeat various EIAs, we perturb the embeddings to increase the\\nentropy of the recovered text in the common structure of recovery models, thus\\nsteering the embeddings toward meaningless texts rather than original sensitive\\ntexts during the recovery process. To maintain retrieval performance in the\\ncloud, we constrain the perturbations within a bound, applying the strategy of\\nreducing them where redundant and increasing them where sparse. Moreover,\\nEntroGuard can be directly integrated into end devices without requiring any\\nmodifications to the embedding model. Extensive experimental results\\ndemonstrate that EntroGuard can reduce the risk of privacy leakage by up to 8\\ntimes at most with negligible loss of retrieval performance compared to\\nexisting privacy-preserving methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.12896v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.12896v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.12733v2', updated=datetime.datetime(2025, 3, 18, 1, 46, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 17, 1, 57, 6, tzinfo=datetime.timezone.utc), title='A Linearized Alternating Direction Multiplier Method for Federated Matrix Completion Problems', authors=[arxiv.Result.Author('Patrick Hytla'), arxiv.Result.Author('Tran T. A. Nghia'), arxiv.Result.Author('Duy Nhat Phan'), arxiv.Result.Author('Andrew Rice')], summary='Matrix completion is fundamental for predicting missing data with a wide\\nrange of applications in personalized healthcare, e-commerce, recommendation\\nsystems, and social network analysis. Traditional matrix completion approaches\\ntypically assume centralized data storage, which raises challenges in terms of\\ncomputational efficiency, scalability, and user privacy. In this paper, we\\naddress the problem of federated matrix completion, focusing on scenarios where\\nuser-specific data is distributed across multiple clients, and privacy\\nconstraints are uncompromising. Federated learning provides a promising\\nframework to address these challenges by enabling collaborative learning across\\ndistributed datasets without sharing raw data. We propose \\\\texttt{FedMC-ADMM}\\nfor solving federated matrix completion problems, a novel algorithmic framework\\nthat combines the Alternating Direction Method of Multipliers with a randomized\\nblock-coordinate strategy and alternating proximal gradient steps. Unlike\\nexisting federated approaches, \\\\texttt{FedMC-ADMM} effectively handles\\nmulti-block nonconvex and nonsmooth optimization problems, allowing efficient\\ncomputation while preserving user privacy. We analyze the theoretical\\nproperties of our algorithm, demonstrating subsequential convergence and\\nestablishing a convergence rate of $\\\\mathcal{O}(K^{-1/2})$, leading to a\\ncommunication complexity of $\\\\mathcal{O}(\\\\epsilon^{-2})$ for reaching an\\n$\\\\epsilon$-stationary point. This work is the first to establish these\\ntheoretical guarantees for federated matrix completion in the presence of\\nmulti-block variables. To validate our approach, we conduct extensive\\nexperiments on real-world datasets, including MovieLens 1M, 10M, and Netflix.\\nThe results demonstrate that \\\\texttt{FedMC-ADMM} outperforms existing methods\\nin terms of convergence speed and testing accuracy.', comment='29 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.12733v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.12733v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.09726v1', updated=datetime.datetime(2025, 3, 12, 18, 16, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 12, 18, 16, 37, tzinfo=datetime.timezone.utc), title='How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy against Link Stealing Attacks?', authors=[arxiv.Result.Author('Mir Imtiaz Mostafiz'), arxiv.Result.Author('Imtiaz Karim'), arxiv.Result.Author('Elisa Bertino')], summary='Graph Neural Networks (GNNs) are widely used and deployed for graph-based\\nprediction tasks. However, as good as GNNs are for learning graph data, they\\nalso come with the risk of privacy leakage. For instance, an attacker can run\\ncarefully crafted queries on the GNNs and, from the responses, can infer the\\nexistence of an edge between a pair of nodes. This attack, dubbed as a\\n\"link-stealing\" attack, can jeopardize the user\\'s privacy by leaking\\npotentially sensitive information. To protect against this attack, we propose\\nan approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs\\nfrom $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility.\\n$NARGIS$ is focused on reshaping the graph embedding space so that the\\nposterior from the GNN model will still provide utility for the prediction task\\nbut will introduce ambiguity for the link-stealing attackers. To this end,\\n$NARGIS$ applies spectral clustering on the given graph to facilitate it being\\naugmented with new nodes -- that have learned features instead of fixed ones.\\nIt utilizes tri-level optimization for learning parameters for the GNN model,\\nsurrogate attacker model, and our defense model (i.e. learnable node features).\\nWe extensively evaluate $NARGIS$ on three benchmark citation datasets over\\neight knowledge availability settings for the attackers. We also evaluate the\\nmodel fidelity and defense performance on influence-based link inference\\nattacks. Through our studies, we have figured out the best feature of $NARGIS$\\n-- its superior fidelity-privacy performance trade-off in a significant number\\nof cases. We also have discovered in which cases the model needs to be\\nimproved, and proposed ways to integrate different schemes to make the model\\nmore robust against link stealing attacks.', comment=\"Preprint for the Accepted Work in The 15th ACM Conference on Data and\\n  Application Security and Privacy (CODASPY'25)}, 14 pages\", journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.09726v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.09726v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.08956v1', updated=datetime.datetime(2025, 3, 11, 23, 18, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 11, 23, 18, 26, tzinfo=datetime.timezone.utc), title='Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles', authors=[arxiv.Result.Author('Francesco Marchiori'), arxiv.Result.Author('Mauro Conti')], summary=\"Advancements in battery technology have accelerated the adoption of Electric\\nVehicles (EVs) due to their environmental benefits. However, their growing\\nsophistication introduces security and privacy challenges. Often seen as mere\\noperational data, battery consumption patterns can unintentionally reveal\\ncritical information exploitable for malicious purposes. These risks go beyond\\nprivacy, impacting vehicle security and regulatory compliance. Despite these\\nconcerns, current research has largely overlooked the broader implications of\\nbattery consumption data exposure. As EVs integrate further into smart\\ntransportation networks, addressing these gaps is crucial to ensure their\\nsafety, reliability, and resilience. In this work, we introduce a novel class\\nof side-channel attacks that exploit EV battery data to extract sensitive user\\ninformation. Leveraging only battery consumption patterns, we demonstrate a\\nmethodology to accurately identify the EV driver and their driving style,\\ndetermine the number of occupants, and infer the vehicle's start and end\\nlocations when user habits are known. We utilize several machine learning\\nmodels and feature extraction techniques to analyze EV power consumption\\npatterns, validating our approach on simulated and real-world datasets\\ncollected from actual drivers. Our attacks achieve an average success rate of\\n95.4% across all attack objectives. Our findings highlight the privacy risks\\nassociated with EV battery data, emphasizing the need for stronger protections\\nto safeguard user privacy and vehicle security.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.08956v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.08956v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.06455v1', updated=datetime.datetime(2025, 3, 9, 5, 29, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 9, 5, 29, 29, tzinfo=datetime.timezone.utc), title='Privacy Protection in Prosumer Energy Management Based on Federated Learning', authors=[arxiv.Result.Author('Yunfeng Li'), arxiv.Result.Author('Xiaolin Li Zhitao Li'), arxiv.Result.Author('Gangqiang Li')], summary=\"With the booming development of prosumers, there is an urgent need for a\\nprosumer energy management system to take full advantage of the flexibility of\\nprosumers and take into account the interests of other parties. However,\\nbuilding such a system will undoubtedly reveal users' privacy. In this paper,\\nby solving the non-independent and identical distribution of data (Non-IID)\\nproblem in federated learning with federated cluster average(FedClusAvg)\\nalgorithm, prosumers' information can efficiently participate in the\\nintelligent decision making of the system without revealing privacy. In the\\nproposed FedClusAvg algorithm, each client performs cluster stratified sampling\\nand multiple iterations. Then, the average weight of the parameters of the\\nsub-server is determined according to the degree of deviation of the parameter\\nfrom the average parameter. Finally, the sub-server multiple local iterations\\nand updates, and then upload to the main server. The advantages of FedClusAvg\\nalgorithm are the following two parts. First, the accuracy of the model in the\\ncase of Non-IID is improved through the method of clustering and parameter\\nweighted average. Second, local multiple iterations and three-tier framework\\ncan effectively reduce communication rounds.\", comment='9 pages, 8 figures', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.06455v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.06455v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.03980v2', updated=datetime.datetime(2025, 3, 25, 14, 22, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 6, 0, 6, 2, tzinfo=datetime.timezone.utc), title='USBSnoop -- Revealing Device Activities via USB Congestions', authors=[arxiv.Result.Author('Davis Ranney'), arxiv.Result.Author('Yufei Wang'), arxiv.Result.Author('A. Adam Ding'), arxiv.Result.Author('Yunsi Fei')], summary='The USB protocol has become a ubiquitous standard for connecting peripherals\\nto computers, making its security a critical concern. A recent research study\\ndemonstrated the potential to exploit weaknesses in well-established protocols,\\nsuch as PCIe, and created a side-channel for leaking sensitive information by\\nleveraging congestion within shared interfaces. Drawing inspiration from that,\\nthis project introduces an innovative approach to USB side-channel attacks via\\ncongestion. We evaluated the susceptibility of USB devices and hubs to remote\\nprofiling and side-channel attacks, identified potential weaknesses within the\\nUSB standard, and highlighted the critical need for heightened security and\\nprivacy in USB technology. Our findings discover vulnerabilities within the USB\\nstandard, which are difficult to effectively mitigate and underscore the need\\nfor enhanced security measures to protect user privacy in an era increasingly\\ndependent on USB-connected devices.', comment='10 Pages, HOST 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.03980v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.03980v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.03454v2', updated=datetime.datetime(2025, 3, 6, 14, 25, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 5, 12, 40, 34, tzinfo=datetime.timezone.utc), title='Data Poisoning Attacks to Locally Differentially Private Range Query Protocols', authors=[arxiv.Result.Author('Ting-Wei Liao'), arxiv.Result.Author('Chih-Hsun Lin'), arxiv.Result.Author('Yu-Lin Tsai'), arxiv.Result.Author('Takao Murakami'), arxiv.Result.Author('Chia-Mu Yu'), arxiv.Result.Author('Jun Sakuma'), arxiv.Result.Author('Chun-Ying Huang'), arxiv.Result.Author('Hiroaki Kikuchi')], summary='Local Differential Privacy (LDP) has been widely adopted to protect user\\nprivacy in decentralized data collection. However, recent studies have revealed\\nthat LDP protocols are vulnerable to data poisoning attacks, where malicious\\nusers manipulate their reported data to distort aggregated results. In this\\nwork, we present the first study on data poisoning attacks targeting LDP range\\nquery protocols, focusing on both tree-based and grid-based approaches. We\\nidentify three key challenges in executing such attacks, including crafting\\nconsistent and effective fake data, maintaining data consistency across levels\\nor grids, and preventing server detection. To address the first two challenges,\\nwe propose novel attack methods that are provably optimal, including a\\ntree-based attack and a grid-based attack, designed to manipulate range query\\nresults with high effectiveness. \\\\textbf{Our key finding is that the common\\npost-processing procedure, Norm-Sub, in LDP range query protocols can help the\\nattacker massively amplify their attack effectiveness.} In addition, we study a\\npotential countermeasure, but also propose an adaptive attack capable of\\nevading this defense to address the third challenge. We evaluate our methods\\nthrough theoretical analysis and extensive experiments on synthetic and\\nreal-world datasets. Our results show that the proposed attacks can\\nsignificantly amplify estimations for arbitrary range queries by manipulating a\\nsmall fraction of users, providing 5-10x more influence than a normal user to\\nthe estimation.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.03454v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.03454v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.03160v2', updated=datetime.datetime(2025, 4, 7, 5, 7, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 5, 4, 5, 9, tzinfo=datetime.timezone.utc), title='SpinML: Customized Synthetic Data Generation for Private Training of Specialized ML Models', authors=[arxiv.Result.Author('Jiang Zhang'), arxiv.Result.Author('Rohan Xavier Sequeira'), arxiv.Result.Author('Konstantinos Psounis')], summary='Specialized machine learning (ML) models tailored to users needs and requests\\nare increasingly being deployed on smart devices with cameras, to provide\\npersonalized intelligent services taking advantage of camera data. However, two\\nprimary challenges hinder the training of such models: the lack of publicly\\navailable labeled data suitable for specialized tasks and the inaccessibility\\nof labeled private data due to concerns about user privacy. To address these\\nchallenges, we propose a novel system SpinML, where the server generates\\ncustomized Synthetic image data to Privately traIN a specialized ML model\\ntailored to the user request, with the usage of only a few sanitized reference\\nimages from the user. SpinML offers users fine-grained, object-level control\\nover the reference images, which allows user to trade between the privacy and\\nutility of the generated synthetic data according to their privacy preferences.\\nThrough experiments on three specialized model training tasks, we demonstrate\\nthat our proposed system can enhance the performance of specialized models\\nwithout compromising users privacy preferences.', comment='17 pages (with appendix), 6 figures, Accepted at The 25th Privacy\\n  Enhancing Technologies Symposium (PETS2025)', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.03160v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.03160v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.03146v2', updated=datetime.datetime(2025, 5, 14, 2, 31, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 5, 3, 41, 57, tzinfo=datetime.timezone.utc), title='PriFFT: Privacy-preserving Federated Fine-tuning of Large Language Models via Hybrid Secret Sharing', authors=[arxiv.Result.Author('Zhichao You'), arxiv.Result.Author('Xuewen Dong'), arxiv.Result.Author('Ke Cheng'), arxiv.Result.Author('Xutong Mu'), arxiv.Result.Author('Jiaxuan Fu'), arxiv.Result.Author('Shiyang Ma'), arxiv.Result.Author('Qiang Qu'), arxiv.Result.Author('Yulong Shen')], summary=\"Fine-tuning large language models (LLMs) raises privacy concerns due to the\\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\\nthis risk by keeping training samples on local devices, while facing the\\nfollowing problems in privacy-preserving federated fine-tuning. (i) Recent\\nstudies show that adversaries can still infer private information in FL. (ii)\\nLLM parameters are shared publicly during federated fine-tuning, while\\ndevelopers are often reluctant to disclose these parameters, posing further\\nsecurity challenges. (iii) Existing works focus on secure inference of LLMs but\\ndo not consider privacy-preserving fine-tuning. Inspired by the above problems,\\nwe propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to\\nprotect both the model parameters and users' privacy. Due to considerable LLM\\nparameters, we present hybrid secret sharing combining arithmetic secret\\nsharing (ASS) and function secret sharing (FSS) to build secure operations and\\nimplement secure layers and activation for privacy-preserving fine-tuning. To\\nimprove the efficiency of privacy-preserving federated fine-tuning of LLMs, we\\noptimize several secure computation protocols based on FSS, including\\nreciprocal calculation, tensor products, natural exponentiation, softmax,\\nsigmoid, hyperbolic tangent, and dropout. The hybrid secret sharing enables\\nPriFFT to apply our optimized FSS protocols while combining ASS protocols to\\nsupport complex computation without extra communication. The optimized\\nprotocols reduce execution time up to 62.5% and communication overhead up to\\n70.7% compared to existing protocols. Besides, PriFFT reduces execution time\\nand communication overhead in privacy-preserving fine-tuning up to 59.1%$ and\\n77.0%$ without accuracy drop compared to the existing secret sharing methods.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.03146v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.03146v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.02150v1', updated=datetime.datetime(2025, 3, 4, 0, 29, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 4, 0, 29, 32, tzinfo=datetime.timezone.utc), title='Trust and Friction: Negotiating How Information Flows Through Decentralized Social Media', authors=[arxiv.Result.Author('Sohyeon Hwang'), arxiv.Result.Author('Priyanka Nanayakkara'), arxiv.Result.Author('Yan Shvartzshnaider')], summary=\"Decentralized social media protocols enable users in independent, user-hosted\\nservers (i.e., instances) to interact with each other while they self-govern.\\nThis community-based model of social media governance opens up new\\nopportunities for tailored decision-making about information flows -- i.e.,\\nwhat user data is shared to whom and when -- and in turn, for protecting user\\nprivacy. To better understand how community governance shapes privacy\\nexpectations on decentralized social media, we conducted a semi-structured\\ninterview with 23 users of the Fediverse, a decentralized social media network.\\nOur findings illustrate important factors that shape a community's\\nunderstandings of information flows, such as rules and proactive efforts from\\nadmins who are perceived as trustworthy. We also highlight ''governance\\nfrictions'' between communities that raise new privacy risks due to\\nincompatibilities in values, security practices, and software. Our findings\\nhighlight the unique challenges of decentralized social media, suggest design\\nopportunities to address frictions, and outline the role of participatory\\ndecision-making to realize the full potential of decentralization.\", comment='Conditionally Accepted at CSCW 2025', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CY', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.02150v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.02150v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.00664v1', updated=datetime.datetime(2025, 3, 1, 23, 34, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 3, 1, 23, 34, 2, tzinfo=datetime.timezone.utc), title='Generative Artificial Intelligence for Academic Research: Evidence from Guidance Issued for Researchers by Higher Education Institutions in the United States', authors=[arxiv.Result.Author('Amrita Ganguly'), arxiv.Result.Author('Aditya Johri'), arxiv.Result.Author('Areej Ali'), arxiv.Result.Author('Nora McDonald')], summary=\"The recent development and use of generative AI (GenAI) has signaled a\\nsignificant shift in research activities such as brainstorming, proposal\\nwriting, dissemination, and even reviewing. This has raised questions about how\\nto balance the seemingly productive uses of GenAI with ethical concerns such as\\nauthorship and copyright issues, use of biased training data, lack of\\ntransparency, and impact on user privacy. To address these concerns, many\\nHigher Education Institutions (HEIs) have released institutional guidance for\\nresearchers. To better understand the guidance that is being provided we report\\nfindings from a thematic analysis of guidelines from thirty HEIs in the United\\nStates that are classified as R1 or 'very high research activity.' We found\\nthat guidance provided to researchers: (1) asks them to refer to external\\nsources of information such as funding agencies and publishers to keep updated\\nand use institutional resources for training and education; (2) asks them to\\nunderstand and learn about specific GenAI attributes that shape research such\\nas predictive modeling, knowledge cutoff date, data provenance, and model\\nlimitations, and educate themselves about ethical concerns such as authorship,\\nattribution, privacy, and intellectual property issues; and (3) includes\\ninstructions on how to acknowledge sources and disclose the use of GenAI, how\\nto communicate effectively about their GenAI use, and alerts researchers to\\nlong term implications such as over reliance on GenAI, legal consequences, and\\nrisks to their institutions from GenAI use. Overall, guidance places the onus\\nof compliance on individual researchers making them accountable for any lapses,\\nthereby increasing their responsibility.\", comment=None, journal_ref=None, doi='10.1007/s43681-025-00688-7', primary_category='cs.CY', categories=['cs.CY', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s43681-025-00688-7', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2503.00664v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.00664v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.20629v1', updated=datetime.datetime(2025, 2, 28, 1, 24, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 28, 1, 24, 33, tzinfo=datetime.timezone.utc), title='Towards Privacy-Preserving Split Learning: Destabilizing Adversarial Inference and Reconstruction Attacks in the Cloud', authors=[arxiv.Result.Author('Griffin Higgins'), arxiv.Result.Author('Roozbeh Razavi-Far'), arxiv.Result.Author('Xichen Zhang'), arxiv.Result.Author('Amir David'), arxiv.Result.Author('Ali Ghorbani'), arxiv.Result.Author('Tongyu Ge')], summary=\"This work aims to provide both privacy and utility within a split learning\\nframework while considering both forward attribute inference and backward\\nreconstruction attacks. To address this, a novel approach has been proposed,\\nwhich makes use of class activation maps and autoencoders as a plug-in strategy\\naiming to increase the user's privacy and destabilize an adversary. The\\nproposed approach is compared with a dimensionality-reduction-based plug-in\\nstrategy, which makes use of principal component analysis to transform the\\nfeature map onto a lower-dimensional feature space. Our work shows that our\\nproposed autoencoder-based approach is preferred as it can provide protection\\nat an earlier split position over the tested architectures in our setting, and,\\nhence, better utility for resource-constrained devices in edge-cloud\\ncollaborative inference (EC) systems.\", comment='15 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.20629v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.20629v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.00062v1', updated=datetime.datetime(2025, 2, 27, 5, 59, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 27, 5, 59, 2, tzinfo=datetime.timezone.utc), title='CRFU: Compressive Representation Forgetting Against Privacy Leakage on Machine Unlearning', authors=[arxiv.Result.Author('Weiqi Wang'), arxiv.Result.Author('Chenhan Zhang'), arxiv.Result.Author('Zhiyi Tian'), arxiv.Result.Author('Shushu Liu'), arxiv.Result.Author('Shui Yu')], summary=\"Machine unlearning allows data owners to erase the impact of their specified\\ndata from trained models. Unfortunately, recent studies have shown that\\nadversaries can recover the erased data, posing serious threats to user\\nprivacy. An effective unlearning method removes the information of the\\nspecified data from the trained model, resulting in different outputs for the\\nsame input before and after unlearning. Adversaries can exploit these output\\ndifferences to conduct privacy leakage attacks, such as reconstruction and\\nmembership inference attacks. However, directly applying traditional defenses\\nto unlearning leads to significant model utility degradation. In this paper, we\\nintroduce a Compressive Representation Forgetting Unlearning scheme (CRFU),\\ndesigned to safeguard against privacy leakage on unlearning. CRFU achieves data\\nerasure by minimizing the mutual information between the trained compressive\\nrepresentation (learned through information bottleneck theory) and the erased\\ndata, thereby maximizing the distortion of data. This ensures that the model's\\noutput contains less information that adversaries can exploit. Furthermore, we\\nintroduce a remembering constraint and an unlearning rate to balance the\\nforgetting of erased data with the preservation of previously learned\\nknowledge, thereby reducing accuracy degradation. Theoretical analysis\\ndemonstrates that CRFU can effectively defend against privacy leakage attacks.\\nOur experimental results show that CRFU significantly increases the\\nreconstruction mean square error (MSE), achieving a defense effect improvement\\nof approximately $200\\\\%$ against privacy reconstruction attacks with only\\n$1.5\\\\%$ accuracy degradation on MNIST.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.00062v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.00062v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.18697v1', updated=datetime.datetime(2025, 2, 25, 23, 20, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 25, 23, 20, 53, tzinfo=datetime.timezone.utc), title='H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction', authors=[arxiv.Result.Author('Robert Marlin'), arxiv.Result.Author('Raja Jurdak'), arxiv.Result.Author('Alsharif Abuadbba')], summary='The widespread adoption of Electric Vehicles (EVs) poses critical challenges\\nfor energy providers, particularly in predicting charging time (temporal\\nprediction), ensuring user privacy, and managing resources efficiently in\\nmobility-driven networks. This paper introduces the Hierarchical Federated\\nLearning Transformer Network (H-FLTN) framework to address these challenges.\\nH-FLTN employs a three-tier hierarchical architecture comprising EVs, community\\nDistributed Energy Resource Management Systems (DERMS), and the Energy Provider\\nData Centre (EPDC) to enable accurate spatio-temporal predictions of EV\\ncharging needs while preserving privacy. Temporal prediction is enhanced using\\nTransformer-based learning, capturing complex dependencies in charging\\nbehavior. Privacy is ensured through Secure Aggregation, Additive Secret\\nSharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only\\nsecret shares of model weights to be exchanged while securing all\\ntransmissions. To improve training efficiency and resource management, H-FLTN\\nintegrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation\\nManagement (CRM), ensuring that training remains both computationally and\\ntemporally efficient as the number of participating EVs increases. DCCM\\noptimises client participation by limiting excessive computational loads, while\\nCRM balances training contributions across epochs, preventing imbalanced\\nparticipation. Our simulation results based on large-scale empirical vehicle\\nmobility data reveal that DCCM and CRM reduce the training time complexity with\\nincreasing EVs from linear to constant. Its integration into real-world smart\\ncity infrastructure enhances energy demand forecasting, resource allocation,\\nand grid stability, ensuring reliability and sustainability in future mobility\\necosystems.', comment='14 pages, 7 tables, 2 figures, Journal Paper', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.CR', 'I.6.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.18697v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.18697v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.15270v1', updated=datetime.datetime(2025, 2, 21, 7, 52, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 21, 7, 52, 46, tzinfo=datetime.timezone.utc), title='On the (In)Security of Non-resettable Device Identifiers in Custom Android Systems', authors=[arxiv.Result.Author('Zikan Dong'), arxiv.Result.Author('Liu Wang'), arxiv.Result.Author('Guoai Xu'), arxiv.Result.Author('Haoyu Wang')], summary='User tracking is critical in the mobile ecosystem, which relies on device\\nidentifiers to build clear user profiles. In earlier ages, Android allowed easy\\naccess to non-resettable device identifiers like device serial numbers and IMEI\\nby third-party apps for user tracking. As privacy concerns grew, Google has\\ntightened restrictions on these identifiers in native Android. Despite this,\\nstakeholders in custom Android systems seek consistent and stable user tracking\\ncapabilities across different system and device models, and they have\\nintroduced covert channels (e.g., system properties and settings) in customized\\nsystems to access identifiers, which undoubtedly increases the risk of user\\nprivacy breaches. This paper examines the introduction of non-resettable\\nidentifiers through system customization and their vulnerability due to poor\\naccess control. We present IDRadar, a scalable and accurate approach for\\nidentifying vulnerable properties and settings on custom Android ROMs. Applying\\nour approach to 1,814 custom ROMs, we have identified 8,192 system properties\\nand 3,620 settings that store non-resettable identifiers, with 3,477 properties\\nand 1,336 settings lacking adequate access control, which can be abused by\\nthird-party apps to track users without permissions. Our large-scale analysis\\ncan identify a large number of security issues which are two orders of\\nmagnitude greater than existing techniques. We further investigate the root\\ncauses of these access control deficiencies. Validation on 32 devices through\\nthe remote testing service confirmed our results. Additionally, we observe that\\nthe vulnerable properties and settings occur in devices of the same OEMs. We\\nhave reported our findings to the vendors and received positive confirmations.\\nOur work underscores the need for greater scrutiny of covert access channels to\\ndevice identifiers and better solutions to safeguard user privacy.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.15270v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.15270v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.13171v1', updated=datetime.datetime(2025, 2, 17, 15, 6, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 17, 15, 6, 56, tzinfo=datetime.timezone.utc), title='Web Phishing Net (WPN): A scalable machine learning approach for real-time phishing campaign detection', authors=[arxiv.Result.Author('Muhammad Fahad Zia'), arxiv.Result.Author('Sri Harish Kalidass')], summary='Phishing is the most prevalent type of cyber-attack today and is recognized\\nas the leading source of data breaches with significant consequences for both\\nindividuals and corporations. Web-based phishing attacks are the most frequent\\nwith vectors such as social media posts and emails containing links to phishing\\nURLs that once clicked on render host systems vulnerable to more sinister\\nattacks. Research efforts to detect phishing URLs have involved the use of\\nsupervised learning techniques that use large amounts of data to train models\\nand have high computational requirements. They also involve analysis of\\nfeatures derived from vectors including email contents thus affecting user\\nprivacy. Additionally, they suffer from a lack of resilience against evolution\\nof threats especially with the advent of generative AI techniques to bypass\\nthese systems as with AI-generated phishing URLs. Unsupervised methods such as\\nclustering techniques have also been used in phishing detection in the past,\\nhowever, they are at times unscalable due to the use of pair-wise comparisons.\\nThey also lack high detection rates while detecting phishing campaigns. In this\\npaper, we propose an unsupervised learning approach that is not only fast but\\nscalable, as it does not involve pair-wise comparisons. It is able to detect\\nentire campaigns at a time with a high detection rate while preserving user\\nprivacy; this includes the recent surge of campaigns with targeted phishing\\nURLs generated by malicious entities using generative AI techniques.', comment='IEEE Intelligent Cybersecurity Conference (ICSC2024)', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.13171v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.13171v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.08889v1', updated=datetime.datetime(2025, 2, 13, 2, 5, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 13, 2, 5, 45, tzinfo=datetime.timezone.utc), title='Linear-Time User-Level DP-SCO via Robust Statistics', authors=[arxiv.Result.Author('Badih Ghazi'), arxiv.Result.Author('Ravi Kumar'), arxiv.Result.Author('Daogao Liu'), arxiv.Result.Author('Pasin Manurangsi')], summary='User-level differentially private stochastic convex optimization (DP-SCO) has\\ngarnered significant attention due to the paramount importance of safeguarding\\nuser privacy in modern large-scale machine learning applications. Current\\nmethods, such as those based on differentially private stochastic gradient\\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\\nutility due to the need to privatize every intermediate iterate. In this work,\\nwe introduce a novel linear-time algorithm that leverages robust statistics,\\nspecifically the median and trimmed mean, to overcome these challenges. Our\\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\\nwith gradient estimation based on robust statistics, thereby significantly\\nreducing the gradient estimation noise for privacy purposes and enhancing the\\nprivacy-utility trade-off. By sidestepping the repeated privatization required\\nby previous methods, our algorithm not only achieves an improved theoretical\\nprivacy-utility trade-off but also maintains computational efficiency. We\\ncomplement our algorithm with an information-theoretic lower bound, showing\\nthat our upper bound is optimal up to logarithmic factors and the dependence on\\n$\\\\epsilon$. This work sets the stage for more robust and efficient\\nprivacy-preserving techniques in machine learning, with implications for future\\nresearch and application in the field.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CR', 'cs.DS', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.08889v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.08889v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.08217v1', updated=datetime.datetime(2025, 2, 12, 8, 54, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 12, 8, 54, 49, tzinfo=datetime.timezone.utc), title='Investigating Vulnerabilities of GPS Trip Data to Trajectory-User Linking Attacks', authors=[arxiv.Result.Author('Benedikt Ströbl'), arxiv.Result.Author('Alexandra Kapp')], summary='Open human mobility data is considered an essential basis for the profound\\nresearch and analysis required for the transition to sustainable mobility and\\nsustainable urban planning. Cycling data has especially been the focus of data\\ncollection endeavors in recent years. Although privacy risks regarding location\\ndata are widely known, practitioners often refrain from advanced privacy\\nmechanisms to prevent utility losses. Removing user identifiers from trips is\\nthereby deemed a major privacy gain, as it supposedly prevents linking single\\ntrips to obtain entire movement patterns. In this paper, we propose a novel\\nattack to reconstruct user identifiers in GPS trip datasets consisting of\\nsingle trips, unlike previous ones that are dedicated to evaluating\\ntrajectory-user linking in the context of check-in data. We evaluate the\\nremaining privacy risk for users in such datasets and our empirical findings\\nfrom two real-world datasets show that the risk of re-identification is\\nsignificant even when personal identifiers have been removed, and that\\ntruncation as a simple additional privacy mechanism may not be effective in\\nprotecting user privacy. Further investigations indicate that users who\\nfrequently visit locations that are only visited by a small number of others,\\ntend to be more vulnerable to re-identification.', comment='32 pages, 15 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.08217v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.08217v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.07942v2', updated=datetime.datetime(2025, 3, 6, 19, 40, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 11, 20, 41, 49, tzinfo=datetime.timezone.utc), title='Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs', authors=[arxiv.Result.Author('Ruichen Zhang'), arxiv.Result.Author('Mufan Qiu'), arxiv.Result.Author('Zhen Tan'), arxiv.Result.Author('Mohan Zhang'), arxiv.Result.Author('Vincent Lu'), arxiv.Result.Author('Jie Peng'), arxiv.Result.Author('Kaidi Xu'), arxiv.Result.Author('Leandro Z. Agudelo'), arxiv.Result.Author('Peter Qian'), arxiv.Result.Author('Tianlong Chen')], summary='Web browsing agents powered by large language models (LLMs) have shown\\ntremendous potential in automating complex web-based tasks. Existing approaches\\ntypically rely on large LLMs (e.g., GPT-4o) to explore web environments and\\ngenerate trajectory data, which is then used either for demonstration retrieval\\n(for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that\\nremains decoupled from the exploration. In this paper, we propose\\nAgentSymbiotic, an iterative framework that couples data synthesis with\\ntask-performance, yielding a \"symbiotic improvement\" for both large and small\\nLLMs. Our study uncovers a complementary dynamic between LLM types: while large\\nLLMs excel at generating high-quality trajectories for distillation, the\\ndistilled small LLMs-owing to their distinct reasoning capabilities-often\\nchoose actions that diverge from those of their larger counterparts. This\\ndivergence drives the exploration of novel trajectories, thereby enriching the\\nsynthesized data. However, we also observe that the performance of small LLMs\\nbecomes a bottleneck in this iterative enhancement process. To address this, we\\npropose two innovations in LLM distillation: a speculative data synthesis\\nstrategy that mitigates off-policy bias, and a multi-task learning approach\\ndesigned to boost the reasoning capabilities of the student LLM. Furthermore,\\nwe introduce a Hybrid Mode for Privacy Preservation to address user privacy\\nconcerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA\\nperformance with both LLM types. Our best Large LLM agent reaches 52%,\\nsurpassing the previous best of 45%, while our 8B distilled model demonstrates\\na competitive 49%, exceeding the prior best of 28%. Code will be released upon\\nacceptance.', comment=None, journal_ref=None, doi=None, primary_category='cs.MA', categories=['cs.MA', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.07942v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.07942v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.04942v2', updated=datetime.datetime(2025, 4, 16, 10, 10, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 7, 14, 3, 46, tzinfo=datetime.timezone.utc), title='WikiReddit: Tracing Information and Attention Flows Between Online Platforms', authors=[arxiv.Result.Author('Patrick Gildersleve'), arxiv.Result.Author('Anna Beers'), arxiv.Result.Author('Viviane Ito'), arxiv.Result.Author('Agustin Orozco'), arxiv.Result.Author('Francesca Tripodi')], summary='The World Wide Web is a complex interconnected digital ecosystem, where\\ninformation and attention flow between platforms and communities throughout the\\nglobe. These interactions co-construct how we understand the world, reflecting\\nand shaping public discourse. Unfortunately, researchers often struggle to\\nunderstand how information circulates and evolves across the web because\\nplatform-specific data is often siloed and restricted by linguistic barriers.\\nTo address this gap, we present a comprehensive, multilingual dataset capturing\\nall Wikipedia mentions and links shared in posts and comments on Reddit\\n2020-2023, excluding those from private and NSFW subreddits. Each linked\\nWikipedia article is enriched with revision history, page view data, article\\nID, redirects, and Wikidata identifiers. Through a research agreement with\\nReddit, our dataset ensures user privacy while providing a query and ID\\nmechanism that integrates with the Reddit and Wikipedia APIs. This enables\\nextended analyses for researchers studying how information flows across\\nplatforms. For example, Reddit discussions use Wikipedia for deliberation and\\nfact-checking which subsequently influences Wikipedia content, by driving\\ntraffic to articles or inspiring edits. By analyzing the relationship between\\ninformation shared and discussed on these platforms, our dataset provides a\\nfoundation for examining the interplay between social media discourse and\\ncollaborative knowledge consumption and production.', comment='Accepted at the 19th International AAAI Conference on Web and Social\\n  Media (ICWSM 2025)', journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.DB', 'cs.HC', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.04942v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.04942v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.04760v2', updated=datetime.datetime(2025, 4, 8, 12, 46, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 7, 8, 48, 6, tzinfo=datetime.timezone.utc), title='Graph Federated Learning Based Proactive Content Caching in Edge Computing', authors=[arxiv.Result.Author('Rui Wang')], summary='With the rapid growth of mobile data traffic and the increasing prevalence of\\nvideo streaming, proactive content caching in edge computing has become crucial\\nfor reducing latency and alleviating network congestion. However, traditional\\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\\nfuture content popularity, while existing proactive caching approaches often\\nrequire users to upload data to a central server, raising concerns regarding\\nprivacy and scalability. To address these challenges, this paper proposes a\\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\\nenhances caching efficiency while preserving user privacy. The proposed\\napproach integrates federated learning and graph neural networks, enabling\\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\\nuser-item relationships and predict content popularity. Instead of sharing raw\\ndata, only the trained model parameters are transmitted to the central server,\\nwhere a federated averaging algorithm aggregates updates, refines the global\\nmodel, and selects the most popular files for proactive caching. Experimental\\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\\noutperforms baseline caching algorithms by achieving higher cache efficiency\\nthrough more accurate content popularity predictions. Moreover, the federated\\nlearning framework strengthens privacy protection while maintaining efficient\\nmodel training; however, scalability remains a challenge in large-scale\\nnetworks with dynamic user preferences.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.04760v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.04760v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.03682v2', updated=datetime.datetime(2025, 6, 11, 20, 50, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 6, 0, 7, 8, tzinfo=datetime.timezone.utc), title='Towards Scalable Defenses against Intimate Partner Infiltrations', authors=[arxiv.Result.Author('Weisi Yang'), arxiv.Result.Author('Shinan Liu'), arxiv.Result.Author('Feng Xiao'), arxiv.Result.Author('Nick Feamster'), arxiv.Result.Author('Stephen Xia')], summary=\"Intimate Partner Infiltration (IPI)--a type of Intimate Partner Violence\\n(IPV) that typically requires physical access to a victim's device--is a\\npervasive concern around the world, often manifesting through digital\\nsurveillance, control, and monitoring. Unlike conventional cyberattacks, IPI\\nperpetrators leverage close proximity and personal knowledge to circumvent\\nstandard protections, underscoring the need for targeted interventions. While\\nsecurity clinics and other human-centered approaches effectively tailor\\nsolutions for victims, their scalability remains constrained by resource\\nlimitations and the need for specialized counseling. We present AID, an\\nAutomated IPI Detection system that continuously monitors for unauthorized\\naccess and suspicious behaviors on smartphones. AID employs a unified\\narchitecture to process multimodal signals stealthily and preserve user\\nprivacy. A brief calibration phase upon installation enables AID to adapt to\\neach user's behavioral patterns, achieving high accuracy with minimal false\\nalarms. Our 27-participant user study demonstrates that AID achieves highly\\naccurate detection of non-owner access and fine-grained IPI-related activities,\\nattaining a false positive rate of 1.6%, which is 11x lower than existing\\nmethods, and an end-to-end F1 score of 0.981. These findings suggest that AID\\ncan serve as a forensic tool that security clinics can deploy to scale their\\nability to identify IPI tactics and deliver personalized, far-reaching support\\nto survivors.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.03682v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.03682v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.02749v1', updated=datetime.datetime(2025, 2, 4, 22, 34, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 4, 22, 34, 3, tzinfo=datetime.timezone.utc), title='Unveiling Privacy and Security Gaps in Female Health Apps', authors=[arxiv.Result.Author('Muhammad Hassan'), arxiv.Result.Author('Mahnoor Jameel'), arxiv.Result.Author('Tian Wang'), arxiv.Result.Author('Masooda Bashir')], summary=\"Female Health Applications (FHA), a growing segment of FemTech, aim to\\nprovide affordable and accessible healthcare solutions for women globally.\\nThese applications gather and monitor health and reproductive data from\\nmillions of users. With ongoing debates on women's reproductive rights and\\nprivacy, it's crucial to assess how these apps protect users' privacy. In this\\npaper, we undertake a security and data protection assessment of 45 popular\\nFHAs. Our investigation uncovers harmful permissions, extensive collection of\\nsensitive personal and medical data, and the presence of numerous third-party\\ntracking libraries. Furthermore, our examination of their privacy policies\\nreveals deviations from fundamental data privacy principles. These findings\\nhighlight a significant lack of privacy and security measures for FemTech apps,\\nespecially as women's reproductive rights face growing political challenges.\\nThe results and recommendations provide valuable insights for users, app\\ndevelopers, and policymakers, paving the way for better privacy and security in\\nFemale Health Applications.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.02749v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.02749v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2502.00847v1', updated=datetime.datetime(2025, 2, 2, 16, 40, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 2, 16, 40, 21, tzinfo=datetime.timezone.utc), title='SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models', authors=[arxiv.Result.Author('Jiawen Zhang'), arxiv.Result.Author('Kejia Chen'), arxiv.Result.Author('Zunlei Feng'), arxiv.Result.Author('Jian Lou'), arxiv.Result.Author('Mingli Song'), arxiv.Result.Author('Jian Liu'), arxiv.Result.Author('Xiaohu Yang')], summary=\"With the growing popularity of LLMs among the general public users,\\nprivacy-preserving and adversarial robustness have become two pressing demands\\nfor LLM-based services, which have largely been pursued separately but rarely\\njointly. In this paper, to the best of our knowledge, we are among the first\\nattempts towards robust and private LLM inference by tightly integrating two\\ndisconnected fields: private inference and prompt ensembling. The former\\nprotects users' privacy by encrypting inference data transmitted and processed\\nby LLMs, while the latter enhances adversarial robustness by yielding an\\naggregated output from multiple prompted LLM responses. Although widely\\nrecognized as effective individually, private inference for prompt ensembling\\ntogether entails new challenges that render the naive combination of existing\\ntechniques inefficient. To overcome the hurdles, we propose SecPE, which\\ndesigns efficient fully homomorphic encryption (FHE) counterparts for the core\\nalgorithmic building blocks of prompt ensembling. We conduct extensive\\nexperiments on 8 tasks to evaluate the accuracy, robustness, and efficiency of\\nSecPE. The results show that SecPE maintains high clean accuracy and offers\\nbetter robustness at the expense of merely $2.5\\\\%$ efficiency overhead compared\\nto baseline private inference methods, indicating a satisfactory\\n``accuracy-robustness-efficiency'' tradeoff. For the efficiency of the\\nencrypted Argmax operation that incurs major slowdown for prompt ensembling,\\nSecPE is 35.4x faster than the state-of-the-art peers, which can be of\\nindependent interest beyond this work.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.00847v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.00847v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.17634v2', updated=datetime.datetime(2025, 3, 6, 11, 17, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 29, 13, 11, 21, tzinfo=datetime.timezone.utc), title='Federated Learning With Individualized Privacy Through Client Sampling', authors=[arxiv.Result.Author('Lucas Lange'), arxiv.Result.Author('Ole Borchardt'), arxiv.Result.Author('Erhard Rahm')], summary='With growing concerns about user data collection, individualized privacy has\\nemerged as a promising solution to balance protection and utility by accounting\\nfor diverse user privacy preferences. Instead of enforcing a uniform level of\\nanonymization for all users, this approach allows individuals to choose privacy\\nsettings that align with their comfort levels. Building on this idea, we\\npropose an adapted method for enabling Individualized Differential Privacy\\n(IDP) in Federated Learning (FL) by handling clients according to their\\npersonal privacy preferences. By extending the SAMPLE algorithm from\\ncentralized settings to FL, we calculate client-specific sampling rates based\\non their heterogeneous privacy budgets and integrate them into a modified\\nIDP-FedAvg algorithm. We test this method under realistic privacy distributions\\nand multiple datasets. The experimental results demonstrate that our approach\\nachieves clear improvements over uniform DP baselines, reducing the trade-off\\nbetween privacy and utility. Compared to the alternative SCALE method in\\nrelated work, which assigns differing noise scales to clients, our method\\nperforms notably better. However, challenges remain for complex tasks with\\nnon-i.i.d. data, primarily stemming from the constraints of the decentralized\\nsetting.', comment='Accepted at 10th International Conference on Machine Learning\\n  Technologies (ICMLT 2025)', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CR', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.17634v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.17634v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.16385v2', updated=datetime.datetime(2025, 5, 23, 5, 33, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 25, 6, 4, 7, tzinfo=datetime.timezone.utc), title='FBQuant: FeedBack Quantization for Large Language Models', authors=[arxiv.Result.Author('Yijiang Liu'), arxiv.Result.Author('Hengyu Fang'), arxiv.Result.Author('Liulu He'), arxiv.Result.Author('Rongyu Zhang'), arxiv.Result.Author('Yichuan Bai'), arxiv.Result.Author('Yuan Du'), arxiv.Result.Author('Li Du')], summary='Deploying Large Language Models (LLMs) on edge devices is increasingly\\nimportant, as it eliminates reliance on network connections, reduces expensive\\nAPI calls, and enhances user privacy. However, on-device deployment is\\nchallenging due to the limited computational resources of edge devices. In\\nparticular, the key bottleneck stems from memory bandwidth constraints related\\nto weight loading. Weight-only quantization effectively reduces memory access,\\nyet often induces significant accuracy degradation. Recent efforts to\\nincorporate sub-branches have shown promise for mitigating quantization errors,\\nbut these methods either lack robust optimization strategies or rely on\\nsuboptimal objectives. To address these gaps, we propose FeedBack Quantization\\n(FBQuant), a novel approach inspired by negative feedback mechanisms in\\nautomatic control. FBQuant inherently ensures that the reconstructed weights\\nremain bounded by the quantization process, thereby reducing the risk of\\noverfitting. To further offset the additional latency introduced by\\nsub-branches, we develop an efficient CUDA kernel that decreases 60% of extra\\ninference time. Comprehensive experiments demonstrate the efficiency and\\neffectiveness of FBQuant across various LLMs. Notably, for 3-bit Llama2-7B,\\nFBQuant improves zero-shot accuracy by 1.2%.', comment='Accepted to IJCAI 2025', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.16385v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.16385v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.14098v1', updated=datetime.datetime(2025, 1, 23, 21, 9, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 23, 21, 9, 3, tzinfo=datetime.timezone.utc), title='Exploring User Perspectives on Data Collection, Data Sharing Preferences, and Privacy Concerns with Remote Healthcare Technology', authors=[arxiv.Result.Author('Daniela Napoli'), arxiv.Result.Author('Heather Molyneaux'), arxiv.Result.Author('Helene Fournier'), arxiv.Result.Author('Sonia Chiasson')], summary=\"Remote healthcare technology can help tackle societal issues by improving\\naccess to quality healthcare services and enhancing diagnoses through in-place\\nmonitoring. These services can be implemented through a combination of mobile\\ndevices, applications, wearable sensors, and other smart technology. It is\\nparamount to handle sensitive data that is collected in ways that meet users'\\nprivacy expectations. We surveyed 384 people in Canada aged 20 to 93 years old\\nto explore participants' comfort with data collection, sharing preferences, and\\npotential privacy concerns related to remote healthcare technology. We explore\\nthese topics within the context of various healthcare scenarios including\\nhealth emergencies and managing chronic health conditions.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.14098v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.14098v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.13782v1', updated=datetime.datetime(2025, 1, 23, 15, 59, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 23, 15, 59, 1, tzinfo=datetime.timezone.utc), title='Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems', authors=[arxiv.Result.Author('Ping He'), arxiv.Result.Author('Lorenzo Cavallaro'), arxiv.Result.Author('Shouling Ji')], summary=\"Android malware presents a persistent threat to users' privacy and data\\nintegrity. To combat this, researchers have proposed machine learning-based\\n(ML-based) Android malware detection (AMD) systems. However, adversarial\\nAndroid malware attacks compromise the detection integrity of the ML-based AMD\\nsystems, raising significant concerns. Existing defenses against adversarial\\nAndroid malware provide protections against feature space attacks which\\ngenerate adversarial feature vectors only, leaving protection against realistic\\nthreats from problem space attacks which generate real adversarial malware an\\nopen problem. In this paper, we address this gap by proposing ADD, a practical\\nadversarial Android malware defense framework designed as a plug-in to enhance\\nthe adversarial robustness of the ML-based AMD systems against problem space\\nattacks. Our extensive evaluation across various ML-based AMD systems\\ndemonstrates that ADD is effective against state-of-the-art problem space\\nadversarial Android malware attacks. Additionally, ADD shows the defense\\neffectiveness in enhancing the adversarial robustness of real-world antivirus\\nsolutions.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.LG', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.13782v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.13782v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.12456v1', updated=datetime.datetime(2025, 1, 21, 19, 4, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 21, 19, 4, 53, tzinfo=datetime.timezone.utc), title='Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications', authors=[arxiv.Result.Author('Shubhi Asthana'), arxiv.Result.Author('Bing Zhang'), arxiv.Result.Author('Ruchi Mahindru'), arxiv.Result.Author('Chad DeLuca'), arxiv.Result.Author('Anna Lisa Gentile'), arxiv.Result.Author('Sandeep Gopisetty')], summary=\"The adoption of Large Language Models (LLMs) has revolutionized AI\\napplications but poses significant challenges in safeguarding user privacy.\\nEnsuring compliance with privacy regulations such as GDPR and CCPA while\\naddressing nuanced privacy risks requires robust and scalable frameworks. This\\npaper presents a detailed study of OneShield Privacy Guard, a framework\\ndesigned to mitigate privacy risks in user inputs and LLM outputs across\\nenterprise and open-source settings. We analyze two real-world deployments:(1)\\na multilingual privacy-preserving system integrated with Data and Model\\nFactory, focusing on enterprise-scale data governance; and (2) PR Insights, an\\nopen-source repository emphasizing automated triaging and community-driven\\nrefinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting\\nsensitive entities like dates, names, and phone numbers across 26 languages,\\noutperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\\\%.\\nDeployment 2, with an average F1 score of 0.86, reduced manual effort by over\\n300 hours in three months, accurately flagging 8.25\\\\% of 1,256 pull requests\\nfor privacy risks with enhanced context sensitivity. These results demonstrate\\nOneShield's adaptability and efficacy in diverse environments, offering\\nactionable insights for context-aware entity recognition, automated compliance,\\nand ethical AI adoption. This work advances privacy-preserving frameworks,\\nsupporting user trust and compliance across operational contexts.\", comment='This paper has been accepted at Deployable AI workshop at AAAI 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.LG', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.12456v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.12456v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.12194v1', updated=datetime.datetime(2025, 1, 21, 15, 2, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 21, 15, 2, 31, tzinfo=datetime.timezone.utc), title='An End-to-End Approach for Korean Wakeword Systems with Speaker Authentication', authors=[arxiv.Result.Author('Geonwoo Seo')], summary=\"Wakeword detection plays a critical role in enabling AI assistants to listen\\nto user voices and interact effectively. However, for languages other than\\nEnglish, there is a significant lack of pre-trained wakeword models.\\nAdditionally, systems that merely determine the presence of a wakeword can pose\\nserious privacy concerns. In this paper, we propose an end-to-end approach that\\ntrains wakewords for Non-English languages, particulary Korean, and uses this\\nto develop a Voice Authentication model to protect user privacy. Our\\nimplementation employs an open-source platform OpenWakeWord, which performs\\nwakeword detection using an FCN (Fully-Connected Network) architecture. Once a\\nwakeword is detected, our custom-developed code calculates cosine similarity\\nfor robust user authentication. Experimental results demonstrate the\\neffectiveness of our approach, achieving a 16.79% and a 6.6% Equal Error Rate\\n(EER) each in the Wakeword Detection and the Voice Authentication. These\\nfindings highlight the model's potential in providing secure and accurate\\nwakeword detection and authentication for Korean users.\", comment='19 pages, 10 figures, implementation code available at\\n  https://github.com/gws8820/securewakeword-model,\\n  https://github.com/gws8820/wyoming-securewakeword, demo video at\\n  https://www.youtube.com/watch?v=F3AXUbL-i-o', journal_ref=None, doi=None, primary_category='cs.SD', categories=['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS', 'I.2.7; I.5.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.12194v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.12194v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.12046v1', updated=datetime.datetime(2025, 1, 21, 11, 16, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 21, 11, 16, 5, tzinfo=datetime.timezone.utc), title='Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning', authors=[arxiv.Result.Author('Chih Wei Ling'), arxiv.Result.Author('Youqi Wu'), arxiv.Result.Author('Jiande Sun'), arxiv.Result.Author('Cheuk Ting Li'), arxiv.Result.Author('Linqi Song'), arxiv.Result.Author('Weitao Xu')], summary=\"Training machine learning models on decentralized private data via federated\\nlearning (FL) poses two key challenges: communication efficiency and privacy\\nprotection. In this work, we address these challenges within the trusted\\naggregator model by introducing a novel approach called the\\nCommunication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both\\nobjectives simultaneously. In particular, CEPAM leverages the rejection-sampled\\nuniversal quantizer (RSUQ), a construction of randomized vector quantizer whose\\nresulting distortion is equivalent to a prescribed noise, such as Gaussian or\\nLaplace noise, enabling joint differential privacy and compression. Moreover,\\nwe analyze the trade-offs among user privacy, global utility, and transmission\\nrate of CEPAM by defining appropriate metrics for FL with differential privacy\\nand compression. Our CEPAM provides the additional benefit of privacy\\nadaptability, allowing clients and the server to customize privacy protection\\nbased on required accuracy and protection. We assess CEPAM's utility\\nperformance using MNIST dataset, demonstrating that CEPAM surpasses baseline\\nmodels in terms of learning accuracy.\", comment='18 pages, 3 figures, Submitted to 2025 IEEE International Symposium\\n  on Information Theory', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.12046v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.12046v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.08044v3', updated=datetime.datetime(2025, 7, 1, 6, 56, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 14, 11, 52, 16, tzinfo=datetime.timezone.utc), title='UFGraphFR: Graph Federation Recommendation System based on User Text description features', authors=[arxiv.Result.Author('Xudong Wang'), arxiv.Result.Author('Qingbo Hao'), arxiv.Result.Author('Xu Cheng'), arxiv.Result.Author('Yingyuan Xiao')], summary='Federated learning has emerged as a key paradigm in privacy-preserving\\ncomputing due to its \"data usable but not visible\" property, enabling users to\\ncollaboratively train models without sharing raw data. Motivated by this,\\nfederated recommendation systems offer a promising architecture that balances\\nuser privacy with recommendation accuracy through distributed collaborative\\nlearning. However, existing federated recommendation methods often neglect the\\nunderlying semantic or behavioral relationships between users during parameter\\naggregation, which limits their recommendation effectiveness. To overcome this\\nlimitation, graph-based federated recommendation systems have been proposed to\\nleverage neighborhood information. Yet, conventional graph construction methods\\nusually require access to raw user data or explicit social links, which\\ncontradicts the strict privacy requirements of federated learning. In this\\nwork, we propose UFGraphFR (User Text-feature-based Graph Federated\\nRecommendation), a novel personalized federated recommendation framework that\\nconstructs a user graph based on clients\\' locally embedded text features. Our\\ncore assumption is that users with similar textual feature descriptions exhibit\\nsimilar preferences. Accordingly, UFGraphFR introduces two key components: (1)\\na privacy-preserving user relationship graph constructed from the joint\\nembedding layer\\'s weight matrix without leaking raw user attributes; (2) a\\nTransformer-based architecture to model temporal dependencies in user-item\\ninteraction sequences. Experimental results on benchmark datasets such as\\nMovieLens and HetRec2011 demonstrate that UFGraphFR achieves recommendation\\naccuracy comparable to both centralized and state-of-the-art federated\\nbaselines while preserving user privacy. The code is available at:\\nhttps://github.com/trueWangSyutung/UFGraphFR.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.08044v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.08044v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.09032v1', updated=datetime.datetime(2025, 1, 14, 0, 2, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 14, 0, 2, 2, tzinfo=datetime.timezone.utc), title='Distributed Identity for Zero Trust and Segmented Access Control: A Novel Approach to Securing Network Infrastructure', authors=[arxiv.Result.Author('Sina Ahmadi')], summary='\"Distributed Identity\" refers to the transition from centralized identity\\nsystems using Decentralized Identifiers (DID) and Verifiable Credentials (VC)\\nfor secure and privacy-preserving authentications. With distributed identity,\\ncontrol of identity data is returned to the user, making credential-based\\nattacks impossible due to the lack of a single point of failure. This study\\nassesses the security improvements achieved when distributed identity is\\nemployed with the ZTA principle, particularly concerning lateral movements\\nwithin segmented networks. It also considers areas such as the implementation\\nspecifications of the framework, the advantages and disadvantages of the method\\nto organizations, and the issues of compatibility and generalizability.\\nFurthermore, the study highlights privacy and regulatory compliance, including\\nthe General Data Protection Regulation (GDPR) and California Consumer Data\\nPrivacy Act (CCPA), analyzing potential solutions to these problems. The study\\nimplies that adopting distributed identities can enhance overall security\\npostures by an order of magnitude, providing contextual and least-privilege\\nauthorization and user privacy. The research recommends refining technical\\nstandards, expanding the use of distributed identity in practice, and\\ndiscussing its applications for the contemporary digital security landscape.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.09032v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.09032v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.07536v2', updated=datetime.datetime(2025, 3, 28, 20, 19, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 13, 18, 16, 13, tzinfo=datetime.timezone.utc), title='ML Mule: Mobile-Driven Context-Aware Collaborative Learning', authors=[arxiv.Result.Author('Haoxiang Yu'), arxiv.Result.Author('Javier Berrocal'), arxiv.Result.Author('Christine Julien')], summary=\"Artificial intelligence has been integrated into nearly every aspect of daily\\nlife, powering applications from object detection with computer vision to large\\nlanguage models for writing emails and compact models for use in smart homes.\\nThese machine learning models at times cater to the needs of individual users\\nbut are often detached from them, as they are typically stored and processed in\\ncentralized data centers. This centralized approach raises privacy concerns,\\nincurs high infrastructure costs, and struggles to provide real time,\\npersonalized experiences. Federated and fully decentralized learning methods\\nhave been proposed to address these issues, but they still depend on\\ncentralized servers or face slow convergence due to communication constraints.\\nWe propose ML Mule, an approach that utilizes individual mobile devices as\\n'mules' to train and transport model snapshots as the mules move through\\nphysical spaces, sharing these models with the physical 'spaces' the mules\\ninhabit. This method implicitly forms affinity groups among devices associated\\nwith users who share particular spaces, enabling collaborative model evolution\\nand protecting users' privacy. Our approach addresses several major\\nshortcomings of traditional, federated, and fully decentralized learning\\nsystems. ML Mule represents a new class of machine learning methods that are\\nmore robust, distributed, and personalized, bringing the field closer to\\nrealizing the original vision of intelligent, adaptive, and genuinely\\ncontext-aware smart environments. Our results show that ML Mule converges\\nfaster and achieves higher model accuracy compared to other existing methods.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.07536v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.07536v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.07262v1', updated=datetime.datetime(2025, 1, 13, 12, 23, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 13, 12, 23, 23, tzinfo=datetime.timezone.utc), title='OblivCDN: A Practical Privacy-preserving CDN with Oblivious Content Access', authors=[arxiv.Result.Author('Viet Vo'), arxiv.Result.Author('Shangqi Lai'), arxiv.Result.Author('Xingliang Yuan'), arxiv.Result.Author('Surya Nepal'), arxiv.Result.Author('Qi Li')], summary=\"Content providers increasingly utilise Content Delivery Networks (CDNs) to\\nenhance users' content download experience. However, this deployment scenario\\nraises significant security concerns regarding content confidentiality and user\\nprivacy due to the involvement of third-party providers. Prior proposals using\\nprivate information retrieval (PIR) and oblivious RAM (ORAM) have proven\\nimpractical due to high computation and communication costs, as well as\\nintegration challenges within distributed CDN architectures. In response, we\\npresent \\\\textsf{OblivCDN}, a practical privacy-preserving system meticulously\\ndesigned for seamless integration with the existing real-world Internet-CDN\\ninfrastructure. Our design strategically adapts Range ORAM primitives to\\noptimise memory and disk seeks when accessing contiguous blocks of CDN content,\\nboth at the origin and edge servers, while preserving both content\\nconfidentiality and user access pattern hiding features. Also, we carefully\\ncustomise several oblivious building blocks that integrate the distributed\\ntrust model into the ORAM client, thereby eliminating the computational\\nbottleneck in the origin server and reducing communication costs between the\\norigin server and edge servers. Moreover, the newly-designed ORAM client also\\neliminates the need for trusted hardware on edge servers, and thus\\nsignificantly ameliorates the compatibility towards networks with massive\\nlegacy devices.In real-world streaming evaluations, OblivCDN} demonstrates\\nremarkable performance, downloading a $256$ MB video in just $5.6$ seconds.\\nThis achievement represents a speedup of $90\\\\times$ compared to a strawman\\napproach (direct ORAM adoption) and a $366\\\\times$ improvement over the prior\\nart, OblivP2P.\", comment='The 20th ACM ASIA Conference on Computer and Communications Security\\n  (ACM ASIACCS 2025)', journal_ref=None, doi='10.1145/3708821.3710826', primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3708821.3710826', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2501.07262v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.07262v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.06177v1', updated=datetime.datetime(2025, 1, 10, 18, 58, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 10, 18, 58, 14, tzinfo=datetime.timezone.utc), title='ScooterLab: A Programmable and Participatory Sensing Research Testbed using Micromobility Vehicles', authors=[arxiv.Result.Author('Ubaidullah Khan'), arxiv.Result.Author('Raveen Wijewickrama'), arxiv.Result.Author('Buddhi Ashan M. K.'), arxiv.Result.Author('A. H. M. Nazmus Sakib'), arxiv.Result.Author('Khoi Trinh'), arxiv.Result.Author('Christina Duthie'), arxiv.Result.Author('Nima Najafian'), arxiv.Result.Author('Ahmer Patel'), arxiv.Result.Author('R. N. Molina'), arxiv.Result.Author('Anindya Maiti'), arxiv.Result.Author('Sushil K. Prasad'), arxiv.Result.Author('Greg P. Griffin'), arxiv.Result.Author('Murtuza Jadliwala')], summary='Micromobility vehicles, such as e-scooters, are increasingly popular in urban\\ncommunities but present significant challenges in terms of road safety, user\\nprivacy, infrastructure planning, and civil engineering. Addressing these\\ncritical issues requires a large-scale and easily accessible research\\ninfrastructure to collect diverse mobility and contextual data from\\nmicromobility users in realistic settings. To this end, we present ScooterLab,\\na community research testbed comprising a fleet of customizable battery-powered\\nmicromobility vehicles retrofitted with advanced sensing, communication, and\\ncontrol capabilities. ScooterLab enables interdisciplinary research at the\\nintersection of computing, mobility, and urban planning by providing\\nresearchers with tools to design and deploy customized sensing experiments and\\naccess curated datasets. The testbed will enable advances in machine learning,\\nprivacy, and urban transportation research while promoting sustainable\\nmobility.', comment=None, journal_ref=None, doi=None, primary_category='cs.ET', categories=['cs.ET', 'cs.CY', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.06177v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.06177v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.04882v1', updated=datetime.datetime(2025, 1, 8, 23, 38, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 8, 23, 38, 19, tzinfo=datetime.timezone.utc), title='Reach Measurement, Optimization and Frequency Capping In Targeted Online Advertising Under k-Anonymity', authors=[arxiv.Result.Author('Yuan Gao'), arxiv.Result.Author('Mu Qiao')], summary='The growth in the use of online advertising to foster brand awareness over\\nrecent years is largely attributable to the ubiquity of social media. One\\npivotal technology contributing to the success of online brand advertising is\\nfrequency capping, a mechanism that enables marketers to control the number of\\ntimes an ad is shown to a specific user. However, the very foundation of this\\ntechnology is being scrutinized as the industry gravitates towards advertising\\nsolutions that prioritize user privacy. This paper delves into the issue of\\nreach measurement and optimization within the context of $k$-anonymity, a\\nprivacy-preserving model gaining traction across major online advertising\\nplatforms. We outline how to report reach within this new privacy landscape and\\ndemonstrate how probabilistic discounting, a probabilistic adaptation of\\ntraditional frequency capping, can be employed to optimize campaign\\nperformance. Experiments are performed to assess the trade-off between user\\nprivacy and the efficacy of online brand advertising. Notably, we discern a\\nsignificant dip in performance as long as privacy is introduced, yet this comes\\nwith a limited additional cost for advertising platforms to offer their users\\nmore privacy.', comment=None, journal_ref=None, doi=None, primary_category='cs.GT', categories=['cs.GT', 'cs.AI', 'cs.LG', 'stat.AP', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.04882v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.04882v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.06226v1', updated=datetime.datetime(2025, 1, 7, 12, 47, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 7, 12, 47, 52, tzinfo=datetime.timezone.utc), title='asanAI: In-Browser, No-Code, Offline-First Machine Learning Toolkit', authors=[arxiv.Result.Author('Norman Koch'), arxiv.Result.Author('Siavash Ghiasvand')], summary='Machine learning (ML) has become crucial in modern life, with growing\\ninterest from researchers and the public. Despite its potential, a significant\\nentry barrier prevents widespread adoption, making it challenging for\\nnon-experts to understand and implement ML techniques. The increasing desire to\\nleverage ML is counterbalanced by its technical complexity, creating a gap\\nbetween potential and practical application. This work introduces asanAI, an\\noffline-first, open-source, no-code machine learning toolkit designed for users\\nof all skill levels. It allows individuals to design, debug, train, and test ML\\nmodels directly in a web browser, eliminating the need for software\\ninstallations and coding. The toolkit runs on any device with a modern web\\nbrowser, including smartphones, and ensures user privacy through local\\ncomputations while utilizing WebGL for enhanced GPU performance. Users can\\nquickly experiment with neural networks and train custom models using various\\ndata sources, supported by intuitive visualizations of network structures and\\ndata flows. asanAI simplifies the teaching of ML concepts in educational\\nsettings and is released under an open-source MIT license, encouraging\\nmodifications. It also supports exporting models in industry-ready formats,\\nempowering a diverse range of users to effectively learn and apply machine\\nlearning in their projects. The proposed toolkit is successfully utilized by\\nresearchers of ScaDS.AI to swiftly draft and test machine learning ideas, by\\ntrainers to effectively educate enthusiasts, and by teachers to introduce\\ncontemporary ML topics in classrooms with minimal effort and high clarity.', comment='7 pages, 8 figures', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.06226v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.06226v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.03301v2', updated=datetime.datetime(2025, 1, 8, 11, 47, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 6, 15, 19, 26, tzinfo=datetime.timezone.utc), title='Rethinking Byzantine Robustness in Federated Recommendation from Sparse Aggregation Perspective', authors=[arxiv.Result.Author('Zhongjian Zhang'), arxiv.Result.Author('Mengmei Zhang'), arxiv.Result.Author('Xiao Wang'), arxiv.Result.Author('Lingjuan Lyu'), arxiv.Result.Author('Bo Yan'), arxiv.Result.Author('Junping Du'), arxiv.Result.Author('Chuan Shi')], summary=\"To preserve user privacy in recommender systems, federated recommendation\\n(FR) based on federated learning (FL) emerges, keeping the personal data on the\\nlocal client and updating a model collaboratively. Unlike FL, FR has a unique\\nsparse aggregation mechanism, where the embedding of each item is updated by\\nonly partial clients, instead of full clients in a dense aggregation of general\\nFL. Recently, as an essential principle of FL, model security has received\\nincreasing attention, especially for Byzantine attacks, where malicious clients\\ncan send arbitrary updates. The problem of exploring the Byzantine robustness\\nof FR is particularly critical since in the domains applying FR, e.g.,\\ne-commerce, malicious clients can be injected easily by registering new\\naccounts. However, existing Byzantine works neglect the unique sparse\\naggregation of FR, making them unsuitable for our problem. Thus, we make the\\nfirst effort to investigate Byzantine attacks on FR from the perspective of\\nsparse aggregation, which is non-trivial: it is not clear how to define\\nByzantine robustness under sparse aggregations and design Byzantine attacks\\nunder limited knowledge/capability. In this paper, we reformulate the Byzantine\\nrobustness under sparse aggregation by defining the aggregation for a single\\nitem as the smallest execution unit. Then we propose a family of effective\\nattack strategies, named Spattack, which exploit the vulnerability in sparse\\naggregation and are categorized along the adversary's knowledge and capability.\\nExtensive experimental results demonstrate that Spattack can effectively\\nprevent convergence and even break down defenses under a few malicious clients,\\nraising alarms for securing FR systems.\", comment='accepted by AAAI 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.DC', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.03301v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.03301v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.02737v2', updated=datetime.datetime(2025, 2, 11, 12, 38, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 6, 3, 11, 12, tzinfo=datetime.timezone.utc), title='Holistic Semantic Representation for Navigational Trajectory Generation', authors=[arxiv.Result.Author('Ji Cao'), arxiv.Result.Author('Tongya Zheng'), arxiv.Result.Author('Qinghong Guo'), arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Junshu Dai'), arxiv.Result.Author('Shunyu Liu'), arxiv.Result.Author('Jie Yang'), arxiv.Result.Author('Jie Song'), arxiv.Result.Author('Mingli Song')], summary=\"Trajectory generation has garnered significant attention from researchers in\\nthe field of spatio-temporal analysis, as it can generate substantial\\nsynthesized human mobility trajectories that enhance user privacy and alleviate\\ndata scarcity. However, existing trajectory generation methods often focus on\\nimproving trajectory generation quality from a singular perspective, lacking a\\ncomprehensive semantic understanding across various scales. Consequently, we\\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\\nfor navigational trajectory generation. Given an origin-and-destination (OD)\\npair and the starting time point of a latent trajectory, we first propose a\\nRoad Network Encoder to expand the receptive field of road- and zone-level\\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\\nintegrate the spatio-temporal semantics of the generated trajectory at both the\\npoint and trajectory levels. Finally, we employ a Destination-Oriented\\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\\nexperiments on three real-world datasets demonstrate that HOSER outperforms\\nstate-of-the-art baselines by a significant margin. Moreover, the model's\\nperformance in few-shot learning and zero-shot learning scenarios further\\nverifies the effectiveness of our holistic semantic representation.\", comment='Accepted by AAAI 2025', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.02737v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.02737v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2501.02091v1', updated=datetime.datetime(2025, 1, 3, 20, 29, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 3, 20, 29, 33, tzinfo=datetime.timezone.utc), title='PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in Browsers', authors=[arxiv.Result.Author('Seyed Ali Akhavani'), arxiv.Result.Author('Engin Kirda'), arxiv.Result.Author('Amin Kharraz')], summary='Online tracking is a widespread practice on the web with questionable ethics,\\nsecurity, and privacy concerns. While web tracking can offer personalized and\\ncurated content to Internet users, it operates as a sophisticated surveillance\\nmechanism to gather extensive user information. This paper introduces\\nPriveShield, a light-weight privacy mechanism that disrupts the information\\ngathering cycle while offering more control to Internet users to maintain their\\nprivacy. PriveShield is implemented as a browser extension that offers an\\nadjustable privacy feature to surf the web with multiple identities or accounts\\nsimultaneously without any changes to underlying browser code or services. When\\nnecessary, multiple factors are automatically analyzed on the client side to\\nisolate cookies and other information that are the basis of online tracking.\\nPriveShield creates isolated profiles for clients based on their browsing\\nhistory, interactions with websites, and the amount of time they spend on\\nspecific websites. This allows the users to easily prevent unwanted browsing\\ninformation from being shared with third parties and ad exchanges without the\\nneed for manual configuration. Our evaluation results from 54 real-world\\nscenarios show that our extension is effective in preventing retargeted ads in\\n91% of those scenarios.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2501.02091v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.02091v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2503.15489v1', updated=datetime.datetime(2025, 1, 3, 0, 31, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 1, 3, 0, 31, 28, tzinfo=datetime.timezone.utc), title='PersonaAI: Leveraging Retrieval-Augmented Generation and Personalized Context for AI-Driven Digital Avatars', authors=[arxiv.Result.Author('Elvis Kimara'), arxiv.Result.Author('Kunle S. Oguntoye'), arxiv.Result.Author('Jian Sun')], summary=\"This paper introduces PersonaAI, a cutting-edge application that leverages\\nRetrieval-Augmented Generation (RAG) and the LLAMA model to create highly\\npersonalized digital avatars capable of accurately mimicking individual\\npersonalities. Designed as a cloud-based mobile application, PersonaAI captures\\nuser data seamlessly, storing it in a secure database for retrieval and\\nanalysis. The result is a system that provides context-aware, accurate\\nresponses to user queries, enhancing the potential of AI-driven\\npersonalization.\\n  Why should you care? PersonaAI combines the scalability of RAG with the\\nefficiency of prompt-engineered LLAMA3, offering a lightweight, sustainable\\nalternative to traditional large language model (LLM) training methods. The\\nsystem's novel approach to data collection, utilizing real-time user\\ninteractions via a mobile app, ensures enhanced context relevance while\\nmaintaining user privacy. By open-sourcing our implementation, we aim to foster\\nadaptability and community-driven development.\\n  PersonaAI demonstrates how AI can transform interactions by merging\\nefficiency, scalability, and personalization, making it a significant step\\nforward in the future of digital avatars and personalized AI.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2503.15489v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.15489v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.18716v2', updated=datetime.datetime(2025, 7, 2, 21, 57, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 25, 0, 27, 13, tzinfo=datetime.timezone.utc), title='Design and Evaluation of Privacy-Preserving Protocols for Agent-Facilitated Mobile Money Services in Kenya', authors=[arxiv.Result.Author('Karen Sowon'), arxiv.Result.Author('Collins W. Munyendo'), arxiv.Result.Author('Lily Klucinec'), arxiv.Result.Author('Eunice Maingi'), arxiv.Result.Author('Gerald Suleh'), arxiv.Result.Author('Lorrie Faith Cranor'), arxiv.Result.Author('Giulia Fanti'), arxiv.Result.Author('Conrad Tucker'), arxiv.Result.Author('Assane Gueye')], summary=\"Mobile Money (MoMo), a technology that allows users to complete financial\\ntransactions using a mobile phone without requiring a bank account, is a common\\nmethod for processing financial transactions in Africa and other developing\\nregions. Users can deposit and withdraw money with the help of human agents.\\nDuring deposit and withdraw operations, know-your-customer (KYC) processes\\nrequire agents to access and verify customer information such as name and ID\\nnumber, which can introduce privacy and security risks. In this work, we design\\nalternative protocols for MoMo deposits/withdrawals that protect users' privacy\\nwhile enabling KYC checks by redirecting the flow of sensitive information from\\nthe agent to the MoMo provider. We evaluate the usability and efficiency of our\\nproposed protocols in a role-play and semi-structured interview study with 32\\nusers and 15 agents in Kenya. We find that users and agents prefer the new\\nprotocols, due in part to convenient and efficient verification using\\nbiometrics as well as better data privacy and access control. However, our\\nstudy also surfaced challenges that need to be addressed before these protocols\\ncan be deployed.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.18716v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.18716v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.16916v3', updated=datetime.datetime(2025, 3, 31, 23, 9, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 22, 8, 22, 57, tzinfo=datetime.timezone.utc), title='On the Differential Privacy and Interactivity of Privacy Sandbox Reports', authors=[arxiv.Result.Author('Badih Ghazi'), arxiv.Result.Author('Charlie Harrison'), arxiv.Result.Author('Arpana Hosabettu'), arxiv.Result.Author('Pritish Kamath'), arxiv.Result.Author('Alexander Knop'), arxiv.Result.Author('Ravi Kumar'), arxiv.Result.Author('Ethan Leeman'), arxiv.Result.Author('Pasin Manurangsi'), arxiv.Result.Author('Mariana Raykova'), arxiv.Result.Author('Vikas Sahu'), arxiv.Result.Author('Phillipp Schoppmann')], summary='The Privacy Sandbox initiative from Google includes APIs for enabling\\nprivacy-preserving advertising functionalities as part of the effort around\\nlimiting third-party cookies. In particular, the Private Aggregation API (PAA)\\nand the Attribution Reporting API (ARA) can be used for ad measurement while\\nproviding different guardrails for safeguarding user privacy, including a\\nframework for satisfying differential privacy (DP). In this work, we provide an\\nabstract model for analyzing the privacy of these APIs and show that they\\nsatisfy a formal DP guarantee under certain assumptions. Our analysis handles\\nthe case where both the queries and database can change interactively based on\\nprevious responses from the API.', comment='To appear in Proceedings of Privacy Enhancing Technologies 2025', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.16916v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.16916v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.15696v1', updated=datetime.datetime(2024, 12, 20, 9, 15, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 20, 9, 15, 48, tzinfo=datetime.timezone.utc), title='Revealing the Black Box of Device Search Engine: Scanning Assets, Strategies, and Ethical Consideration', authors=[arxiv.Result.Author('Mengying Wu'), arxiv.Result.Author('Geng Hong'), arxiv.Result.Author('Jinsong Chen'), arxiv.Result.Author('Qi Liu'), arxiv.Result.Author('Shujun Tang'), arxiv.Result.Author('Youhao Li'), arxiv.Result.Author('Baojun Liu'), arxiv.Result.Author('Haixin Duan'), arxiv.Result.Author('Min Yang')], summary=\"In the digital age, device search engines such as Censys and Shodan play\\ncrucial roles by scanning the internet to catalog online devices, aiding in the\\nunderstanding and mitigation of network security risks. While previous research\\nhas used these tools to detect devices and assess vulnerabilities, there\\nremains uncertainty regarding the assets they scan, the strategies they employ,\\nand whether they adhere to ethical guidelines. This study presents the first\\ncomprehensive examination of these engines' operational and ethical dimensions.\\nWe developed a novel framework to trace the IP addresses utilized by these\\nengines and collected 1,407 scanner IPs. By uncovering their IPs, we gain deep\\ninsights into the actions of device search engines for the first time and gain\\noriginal findings. By employing 28 honeypots to monitor their scanning\\nactivities extensively in one year, we demonstrate that users can hardly evade\\nscans by blocklisting scanner IPs or migrating service ports. Our findings\\nreveal significant ethical concerns, including a lack of transparency,\\nharmlessness, and anonymity. Notably, these engines often fail to provide\\ntransparency and do not allow users to opt out of scans. Further, the engines\\nsend malformed requests, attempt to access excessive details without\\nauthorization, and even publish personally identifiable information (PII) and\\nscreenshots on search results. These practices compromise user privacy and\\nexpose devices to further risks by potentially aiding malicious entities. This\\npaper emphasizes the urgent need for stricter ethical standards and enhanced\\ntransparency in the operations of device search engines, offering crucial\\ninsights into safeguarding against invasive scanning practices and protecting\\ndigital infrastructures.\", comment='18 pages, accepted by NDSS 2025', journal_ref=None, doi='10.14722/ndss.2025.241924', primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.14722/ndss.2025.241924', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2412.15696v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.15696v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.15538v2', updated=datetime.datetime(2025, 2, 8, 2, 34, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 20, 3, 56, 31, tzinfo=datetime.timezone.utc), title='FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF', authors=[arxiv.Result.Author('Flint Xiaofeng Fan'), arxiv.Result.Author('Cheston Tan'), arxiv.Result.Author('Yew-Soon Ong'), arxiv.Result.Author('Roger Wattenhofer'), arxiv.Result.Author('Wei-Tsang Ooi')], summary='In the era of increasing privacy concerns and demand for personalized\\nexperiences, traditional Reinforcement Learning with Human Feedback (RLHF)\\nframeworks face significant challenges due to their reliance on centralized\\ndata. We introduce Federated Reinforcement Learning with Human Feedback\\n(FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF\\nenables collaborative policy learning across multiple clients without\\nnecessitating the sharing of raw data or human feedback, thereby ensuring\\nrobust privacy preservation. Leveraging federated reinforcement learning, each\\nclient integrates human feedback locally into their reward functions and\\nupdates their policies through personalized RLHF processes. We establish\\nrigorous theoretical foundations for FedRLHF, providing convergence guarantees,\\nand deriving sample complexity bounds that scale efficiently with the number of\\nclients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate\\nthat FedRLHF not only preserves user privacy but also achieves performance on\\npar with centralized RLHF, while enhancing personalization across diverse\\nclient environments.', comment='Updated for AAMAS 2025 camera-ready. This preprint represents the\\n  full version of the paper, including all proofs, experimental details, and\\n  additional discussions', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.CR', 'I.2.11'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.15538v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.15538v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.14505v1', updated=datetime.datetime(2024, 12, 19, 3, 59, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 19, 3, 59, 26, tzinfo=datetime.timezone.utc), title='A hybrid framework for effective and efficient machine unlearning', authors=[arxiv.Result.Author('Mingxin Li'), arxiv.Result.Author('Yizhen Yu'), arxiv.Result.Author('Ning Wang'), arxiv.Result.Author('Zhigang Wang'), arxiv.Result.Author('Xiaodong Wang'), arxiv.Result.Author('Haipeng Qu'), arxiv.Result.Author('Jia Xu'), arxiv.Result.Author('Shen Su'), arxiv.Result.Author('Zhichao Yin')], summary=\"Recently machine unlearning (MU) is proposed to remove the imprints of\\nrevoked samples from the already trained model parameters, to solve users'\\nprivacy concern. Different from the runtime expensive retraining from scratch,\\nthere exist two research lines, exact MU and approximate MU with different\\nfavorites in terms of accuracy and efficiency. In this paper, we present a\\nnovel hybrid strategy on top of them to achieve an overall success. It\\nimplements the unlearning operation with an acceptable computation cost, while\\nsimultaneously improving the accuracy as much as possible. Specifically, it\\nruns reasonable unlearning techniques by estimating the retraining workloads\\ncaused by revocations. If the workload is lightweight, it performs retraining\\nto derive the model parameters consistent with the accurate ones retrained from\\nscratch. Otherwise, it outputs the unlearned model by directly modifying the\\ncurrent parameters, for better efficiency. In particular, to improve the\\naccuracy in the latter case, we propose an optimized version to amend the\\noutput model with lightweight runtime penalty. We particularly study the\\nboundary of two approaches in our frameworks to adaptively make the smart\\nselection. Extensive experiments on real datasets validate that our proposals\\ncan improve the unlearning efficiency by 1.5$\\\\times$ to 8$\\\\times$ while\\nachieving comparable accuracy.\", comment='14 pages, 5 figures, accepted by CSE2024', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.14505v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.14505v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.13865v3', updated=datetime.datetime(2024, 12, 20, 15, 6, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 18, 14, 1, 31, tzinfo=datetime.timezone.utc), title='Towards an identity management solution on Arweave', authors=[arxiv.Result.Author('Andreea Elena Dragnoiu'), arxiv.Result.Author('Ruxandra F. Olimid')], summary=\"Traditional identity management systems, often centralized, face challenges\\naround privacy, data security, and user control, leaving users vulnerable to\\ndata breaches and misuse. This paper explores the potential of using the\\nArweave network to develop an identity management solution. By harnessing\\nArweave's permanent storage, our solution offers the users a Self-Sovereign\\nIdentity (SSI) framework, that uses Decentralized Identifiers (DIDs) and\\nVerifiable Credentials (VCs) to allow individuals and other entities to create,\\nown, and manage their digital identities. Further, the solution integrates\\nprivacy-preserving technologies, including zero-knowledge proofs and the BBS(+)\\nsignature scheme, enabling selective disclosure. This approach ultimately\\nenhances user privacy and supports compliance with European Union legislation\\nand regulatory standards like the General Data Protection Regulation (GDPR) by\\ndesign.\", comment='37 pages', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.ET'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.13865v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.13865v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.12481v1', updated=datetime.datetime(2024, 12, 17, 2, 35, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 17, 2, 35, 32, tzinfo=datetime.timezone.utc), title='if-ZKP: Intel FPGA-Based Acceleration of Zero Knowledge Proofs', authors=[arxiv.Result.Author('Shahzad Ahmad Butt'), arxiv.Result.Author('Benjamin Reynolds'), arxiv.Result.Author('Veeraraghavan Ramamurthy'), arxiv.Result.Author('Xiao Xiao'), arxiv.Result.Author('Pohrong Chu'), arxiv.Result.Author('Setareh Sharifian'), arxiv.Result.Author('Sergey Gribok'), arxiv.Result.Author('Bogdan Pasca')], summary=\"Zero-Knowledge Proofs (ZKPs) have emerged as an important cryptographic\\ntechnique allowing one party (prover) to prove the correctness of a statement\\nto some other party (verifier) and nothing else. ZKPs give rise to user's\\nprivacy in many applications such as blockchains, digital voting, and machine\\nlearning. Traditionally, ZKPs suffered from poor scalability but recently, a\\nsub-class of ZKPs known as Zero-knowledge Succinct Non-interactive ARgument of\\nKnowledges (zk-SNARKs) have addressed this challenge. They are getting\\nsignificant attention and are being implemented by many public libraries. In\\nthis paper, we present a novel scalable architecture that is suitable for\\naccelerating the zk-SNARK prover compute on FPGAs. We focus on the multi-scalar\\nmultiplication (MSM) that accounts for the majority of computation time spent\\nin zk-SNARK systems. The MSM calculations extensive rely on modular arithmetic\\nso highly optimized Intel IP Libraries for modular arithmetic are used. The\\nproposed architecture exploits the parallelism inherent to MSM and is\\nimplemented using the Intel OneAPI framework for FPGAs. Our implementation runs\\n110x-150x faster compared to reference software library, uses a generic curve\\nform in Jacobian coordinates and is the first to report FPGA hardware\\nacceleration results for BLS12-381 and BN128 family of elliptic curves.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AR', categories=['cs.AR', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.12481v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.12481v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.11471v1', updated=datetime.datetime(2024, 12, 16, 6, 12, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 16, 6, 12, 56, tzinfo=datetime.timezone.utc), title='Red Pill and Blue Pill: Controllable Website Fingerprinting Defense via Dynamic Backdoor Learning', authors=[arxiv.Result.Author('Siyuan Liang'), arxiv.Result.Author('Jiajun Gong'), arxiv.Result.Author('Tianmeng Fang'), arxiv.Result.Author('Aishan Liu'), arxiv.Result.Author('Tao Wang'), arxiv.Result.Author('Xianglong Liu'), arxiv.Result.Author('Xiaochun Cao'), arxiv.Result.Author('Dacheng Tao'), arxiv.Result.Author('Chang Ee-Chien')], summary=\"Website fingerprint (WF) attacks, which covertly monitor user communications\\nto identify the web pages they visit, pose a serious threat to user privacy.\\nExisting WF defenses attempt to reduce the attacker's accuracy by disrupting\\nunique traffic patterns; however, they often suffer from the trade-off between\\noverhead and effectiveness, resulting in less usefulness in practice. To\\novercome this limitation, we introduce Controllable Website Fingerprint Defense\\n(CWFD), a novel defense perspective based on backdoor learning. CWFD exploits\\nbackdoor vulnerabilities in neural networks to directly control the attacker's\\nmodel by designing trigger patterns based on network traffic. Specifically,\\nCWFD injects only incoming packets on the server side into the target web\\npage's traffic, keeping overhead low while effectively poisoning the attacker's\\nmodel during training. During inference, the defender can influence the\\nattacker's model through a 'red pill, blue pill' choice: traces with the\\ntrigger (red pill) lead to misclassification as the target web page, while\\nnormal traces (blue pill) are classified correctly, achieving directed control\\nover the defense outcome. We use the Fast Levenshtein-like distance as the\\noptimization objective to compute trigger patterns that can be effectively\\nassociated with our target page. Experiments show that CWFD significantly\\nreduces RF's accuracy from 99% to 6% with 74% data overhead. In comparison,\\nFRONT reduces accuracy to only 97% at similar overhead, while Palette achieves\\n32% accuracy with 48% more overhead. We further validate the practicality of\\nour method in a real Tor network environment.\", comment='18 pages, 7 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', '68M10', 'C.2.0'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.11471v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.11471v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.09854v1', updated=datetime.datetime(2024, 12, 13, 4, 48, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 13, 4, 48, 33, tzinfo=datetime.timezone.utc), title='User Identity Protection in EEG-based Brain-Computer Interfaces', authors=[arxiv.Result.Author('L. Meng'), arxiv.Result.Author('X. Jiang'), arxiv.Result.Author('J. Huang'), arxiv.Result.Author('W. Li'), arxiv.Result.Author('H. Luo'), arxiv.Result.Author('D. Wu')], summary='A brain-computer interface (BCI) establishes a direct communication pathway\\nbetween the brain and an external device. Electroencephalogram (EEG) is the\\nmost popular input signal in BCIs, due to its convenience and low cost. Most\\nresearch on EEG-based BCIs focuses on the accurate decoding of EEG signals;\\nhowever, EEG signals also contain rich private information, e.g., user\\nidentity, emotion, and so on, which should be protected. This paper first\\nexposes a serious privacy problem in EEG-based BCIs, i.e., the user identity in\\nEEG data can be easily learned so that different sessions of EEG data from the\\nsame user can be associated together to more reliably mine private information.\\nTo address this issue, we further propose two approaches to convert the\\noriginal EEG data into identity-unlearnable EEG data, i.e., removing the user\\nidentity information while maintaining the good performance on the primary BCI\\ntask. Experiments on seven EEG datasets from five different BCI paradigms\\nshowed that on average the generated identity-unlearnable EEG data can reduce\\nthe user identification accuracy from 70.01\\\\% to at most 21.36\\\\%, greatly\\nfacilitating user privacy protection in EEG-based BCIs.', comment=None, journal_ref='IEEE Trans. on Neural Systems and Rehabilitation Engineering,\\n  31:3576-3586, 2023', doi='10.1109/TNSRE.2023.3310883', primary_category='cs.HC', categories=['cs.HC', 'cs.CR', 'eess.SP'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TNSRE.2023.3310883', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2412.09854v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.09854v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.09222v1', updated=datetime.datetime(2024, 12, 12, 12, 24, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 12, 12, 24, 12, tzinfo=datetime.timezone.utc), title='Building a Privacy Web with SPIDEr -- Secure Pipeline for Information De-Identification with End-to-End Encryption', authors=[arxiv.Result.Author('Novoneel Chakraborty'), arxiv.Result.Author('Anshoo Tandon'), arxiv.Result.Author('Kailash Reddy'), arxiv.Result.Author('Kaushal Kirpekar'), arxiv.Result.Author('Bryan Paul Robert'), arxiv.Result.Author('Hari Dilip Kumar'), arxiv.Result.Author('Abhilash Venkatesh'), arxiv.Result.Author('Abhay Sharma')], summary='Data de-identification makes it possible to glean insights from data while\\npreserving user privacy. The use of Trusted Execution Environments (TEEs) allow\\nfor the execution of de-identification applications on the cloud without the\\nneed for a user to trust the third-party application provider. In this paper,\\nwe present \\\\textit{SPIDEr - Secure Pipeline for Information De-Identification\\nwith End-to-End Encryption}, our implementation of an end-to-end encrypted data\\nde-identification pipeline. SPIDEr supports classical anonymisation techniques\\nsuch as suppression, pseudonymisation, generalisation, and aggregation, as well\\nas techniques that offer a formal privacy guarantee such as k-anonymisation and\\ndifferential privacy. To enable scalability and improve performance on\\nconstrained TEE hardware, we enable batch processing of data for differential\\nprivacy computations. We present our design of the control flows for end-to-end\\nsecure execution of de-identification operations within a TEE. As part of the\\ncontrol flow for running SPIDEr within the TEE, we perform attestation, a\\nprocess that verifies that the software binaries were properly instantiated on\\na known, trusted platform.', comment='3 pages, 2 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.IT', 'math.IT'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.09222v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.09222v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.08950v3', updated=datetime.datetime(2025, 2, 26, 16, 23, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 12, 5, 28, 34, tzinfo=datetime.timezone.utc), title='Predicting Quality of Video Gaming Experience Using Global-Scale Telemetry Data and Federated Learning', authors=[arxiv.Result.Author('Zhongyang Zhang'), arxiv.Result.Author('Jinhe Wen'), arxiv.Result.Author('Zixi Chen'), arxiv.Result.Author('Dara Arbab'), arxiv.Result.Author('Sruti Sahani'), arxiv.Result.Author('Kent Giard'), arxiv.Result.Author('Bijan Arbab'), arxiv.Result.Author('Haojian Jin'), arxiv.Result.Author('Tauhidur Rahman')], summary=\"Frames Per Second (FPS) significantly affects the gaming experience.\\nProviding players with accurate FPS estimates prior to purchase benefits both\\nplayers and game developers. However, we have a limited understanding of how to\\npredict a game's FPS performance on a specific device. In this paper, we first\\nconduct a comprehensive analysis of a wide range of factors that may affect\\ngame FPS on a global-scale dataset to identify the determinants of FPS. This\\nincludes player-side and game-side characteristics, as well as country-level\\nsocio-economic statistics. Furthermore, recognizing that accurate FPS\\npredictions require extensive user data, which raises privacy concerns, we\\npropose a federated learning-based model to ensure user privacy. Each player\\nand game is assigned a unique learnable knowledge kernel that gradually\\nextracts latent features for improved accuracy. We also introduce a novel\\ntraining and prediction scheme that allows these kernels to be dynamically\\nplug-and-play, effectively addressing cold start issues. To train this model\\nwith minimal bias, we collected a large telemetry dataset from 224 countries\\nand regions, 100,000 users, and 835 games. Our model achieved a mean\\nWasserstein distance of 0.469 between predicted and ground truth FPS\\ndistributions, outperforming all baseline methods.\", comment='22 pages, 11 figures, 6 tables', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.08950v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.08950v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.08066v2', updated=datetime.datetime(2024, 12, 28, 6, 27, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 11, 3, 22, 4, tzinfo=datetime.timezone.utc), title='Cluster-Enhanced Federated Graph Neural Network for Recommendation', authors=[arxiv.Result.Author('Haiyan Wang'), arxiv.Result.Author('Ye Yuan')], summary='Personal interaction data can be effectively modeled as individual graphs for\\neach user in recommender systems.Graph Neural Networks (GNNs)-based\\nrecommendation techniques have become extremely popular since they can capture\\nhigh-order collaborative signals between users and items by aggregating the\\nindividual graph into a global interactive graph.However, this centralized\\napproach inherently poses a threat to user privacy and security. Recently,\\nfederated GNN-based recommendation techniques have emerged as a promising\\nsolution to mitigate privacy concerns. Nevertheless, current implementations\\neither limit on-device training to an unaccompanied individual graphs or\\nnecessitate reliance on an extra third-party server to touch other individual\\ngraphs, which also increases the risk of privacy leakage. To address this\\nchallenge, we propose a Cluster-enhanced Federated Graph Neural Network\\nframework for Recommendation, named CFedGR, which introduces high-order\\ncollaborative signals to augment individual graphs in a privacy preserving\\nmanner. Specifically, the server clusters the pretrained user representations\\nto identify high-order collaborative signals. In addition, two efficient\\nstrategies are devised to reduce communication between devices and the server.\\nExtensive experiments on three benchmark datasets validate the effectiveness of\\nour proposed methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.08066v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.08066v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.05000v2', updated=datetime.datetime(2025, 2, 13, 13, 53, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 6, 12, 52, 24, tzinfo=datetime.timezone.utc), title='Noise Matters: Diffusion Model-based Urban Mobility Generation with Collaborative Noise Priors', authors=[arxiv.Result.Author('Yuheng Zhang'), arxiv.Result.Author('Yuan Yuan'), arxiv.Result.Author('Jingtao Ding'), arxiv.Result.Author('Jian Yuan'), arxiv.Result.Author('Yong Li')], summary='With global urbanization, the focus on sustainable cities has largely grown,\\ndriving research into equity, resilience, and urban planning, which often\\nrelies on mobility data. The rise of web-based apps and mobile devices has\\nprovided valuable user data for mobility-related research. However, real-world\\nmobility data is costly and raises privacy concerns. To protect privacy while\\nretaining key features of real-world movement, the demand for synthetic data\\nhas steadily increased. Recent advances in diffusion models have shown great\\npotential for mobility trajectory generation due to their ability to model\\nrandomness and uncertainty. However, existing approaches often directly apply\\nidentically distributed (i.i.d.) noise sampling from image generation\\ntechniques, which fail to account for the spatiotemporal correlations and\\nsocial interactions that shape urban mobility patterns. In this paper, we\\npropose CoDiffMob, a diffusion model for urban mobility generation with\\ncollaborative noise priors, we emphasize the critical role of noise in\\ndiffusion models for generating mobility data. By leveraging both individual\\nmovement characteristics and population-wide dynamics, we construct novel\\ncollaborative noise priors that provide richer and more informative guidance\\nthroughout the generation process. Extensive experiments demonstrate the\\nsuperiority of our method, with generated data accurately capturing both\\nindividual preferences and collective patterns, achieving an improvement of\\nover 32%. Furthermore, it can effectively replace web-derived mobility data to\\nbetter support downstream applications, while safeguarding user privacy and\\nfostering a more secure and ethical web. This highlights its tremendous\\npotential for applications in sustainable city-related research. The code and\\ndata are available at https://github.com/tsinghua-fib-lab/CoDiffMob.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.05000v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.05000v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.04518v1', updated=datetime.datetime(2024, 12, 5, 8, 51, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 5, 8, 51, 2, tzinfo=datetime.timezone.utc), title='Privacy-Preserving Gesture Tracking System Utilizing Frequency-Hopping RFID Signals', authors=[arxiv.Result.Author('Bojun Zhang')], summary='Gesture tracking technology provides users with a hands free interactive\\nexperience without the need to hold or touch devices. However, current gesture\\ntracking research has primarily focused on tracking accuracy while neglecting\\nissues of user privacy protection and security. This study aims to develop a\\ngesture tracking system based on frequency hopping RFID signals that\\neffectively protects user privacy without compromising tracking efficiency and\\naccuracy. By introducing frequency hopping technology, we have designed a\\nmechanism that prevents potential eavesdroppers from obtaining raw RFID\\nsignals, thereby enhancing the systems privacy protection capabilities. The\\nsystem architec ture includes the collection of RFID signals, data processing,\\nsignal recovery, and gesture tracking. Experimental results show that our\\nmethod significantly improves privacy protection levels while maintaining real\\ntime and accuracy. This research not only provides a new perspective for the\\nfield of gesture tracking but also offers valuable insights for the use of RFID\\ntechnology in privacy-sensitive applications.', comment='HPCC', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.NI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.04518v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.04518v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.03292v1', updated=datetime.datetime(2024, 12, 4, 13, 10, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 4, 13, 10, 14, tzinfo=datetime.timezone.utc), title='DMP_AI: An AI-Aided K-12 System for Teaching and Learning in Diverse Schools', authors=[arxiv.Result.Author('Zhen-Qun Yang'), arxiv.Result.Author('Jiannong Cao'), arxiv.Result.Author('Xiaoyin Li'), arxiv.Result.Author('Kaile Wang'), arxiv.Result.Author('Xinzhe Zheng'), arxiv.Result.Author('Kai Cheung Franky Poon'), arxiv.Result.Author('Daniel Lai')], summary='The use of Artificial Intelligence (AI) has gained momentum in education.\\nHowever, the use of AI in K-12 education is still in its nascent stages, and\\nfurther research and development is needed to realize its potential. Moreover,\\nthe creation of a comprehensive and cohesive system that effectively harnesses\\nAI to support teaching and learning across a diverse range of primary and\\nsecondary schools presents substantial challenges that need to be addressed. To\\nfill these gaps, especially in countries like China, we designed and\\nimplemented the DMP_AI (Data Management Platform_Artificial Intelligence)\\nsystem, an innovative AI-aided educational system specifically designed for\\nK-12 education. The system utilizes data mining, natural language processing,\\nand machine learning, along with learning analytics, to offer a wide range of\\nfeatures, including student academic performance and behavior prediction, early\\nwarning system, analytics of Individualized Education Plan, talented students\\nprediction and identification, and cross-school personalized electives\\nrecommendation. The development of this system has been meticulously carried\\nout while prioritizing user privacy and addressing the challenges posed by data\\nheterogeneity. We successfully implemented the DMP_AI system in real-world\\nprimary and secondary schools, allowing us to gain valuable insights into the\\npotential and challenges of integrating AI into K-12 education in the real\\nworld. This system will serve as a valuable resource for supporting educators\\nin providing effective and inclusive K-12 education.', comment='15 pages', journal_ref='pp 117-130, LNCS, volume 14797, 2024', doi=None, primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.03292v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.03292v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2412.02987v1', updated=datetime.datetime(2024, 12, 4, 3, 2, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 12, 4, 3, 2, 46, tzinfo=datetime.timezone.utc), title='Advancing Conversational Psychotherapy: Integrating Privacy, Dual-Memory, and Domain Expertise with Large Language Models', authors=[arxiv.Result.Author('XiuYu Zhang'), arxiv.Result.Author('Zening Luo')], summary='Mental health has increasingly become a global issue that reveals the\\nlimitations of traditional conversational psychotherapy, constrained by\\nlocation, time, expense, and privacy concerns. In response to these challenges,\\nwe introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed\\nto democratize access to psychotherapy. SoulSpeak improves upon the\\ncapabilities of standard LLM-enabled chatbots by incorporating a novel\\ndual-memory component that combines short-term and long-term context via\\nRetrieval Augmented Generation (RAG) to offer personalized responses while\\nensuring the preservation of user privacy and intimacy through a dedicated\\nprivacy module. In addition, it leverages a counseling chat dataset of\\ntherapist-client interactions and various prompting techniques to align the\\ngenerated responses with psychotherapeutic methods. We introduce two fine-tuned\\nBERT models to evaluate the system against existing LLMs and human therapists:\\nthe Conversational Psychotherapy Preference Model (CPPM) to simulate human\\npreference among responses and another to assess response relevance to user\\ninput. CPPM is useful for training and evaluating psychotherapy-focused\\nlanguage models independent from SoulSpeak, helping with the constrained\\nresources available for psychotherapy. Furthermore, the effectiveness of the\\ndual-memory component and the robustness of the privacy module are also\\nexamined. Our findings highlight the potential and challenge of enhancing\\nmental health care by offering an alternative that combines the expertise of\\ntraditional therapy with the advantages of LLMs, providing a promising way to\\naddress the accessibility and personalization gap in current mental health\\nservices.', comment='Accepted as a Poster at Statistical Foundations of LLMs and\\n  Foundation Models (NeurIPS 2024 Workshop)', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2412.02987v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2412.02987v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.19841v1', updated=datetime.datetime(2024, 11, 29, 16, 57, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 29, 16, 57, 31, tzinfo=datetime.timezone.utc), title='Parallel Stacked Aggregated Network for Voice Authentication in IoT-Enabled Smart Devices', authors=[arxiv.Result.Author('Awais Khan'), arxiv.Result.Author('Ijaz Ul Haq'), arxiv.Result.Author('Khalid Mahmood Malik')], summary='Voice authentication on IoT-enabled smart devices has gained prominence in\\nrecent years due to increasing concerns over user privacy and security. The\\ncurrent authentication systems are vulnerable to different voice-spoofing\\nattacks (e.g., replay, voice cloning, and audio deepfakes) that mimic\\nlegitimate voices to deceive authentication systems and enable fraudulent\\nactivities (e.g., impersonation, unauthorized access, financial fraud, etc.).\\nExisting solutions are often designed to tackle a single type of attack,\\nleading to compromised performance against unseen attacks. On the other hand,\\nexisting unified voice anti-spoofing solutions, not designed specifically for\\nIoT, possess complex architectures and thus cannot be deployed on IoT-enabled\\nsmart devices. Additionally, most of these unified solutions exhibit\\nsignificant performance issues, including higher equal error rates or lower\\naccuracy for specific attacks. To overcome these issues, we present the\\nparallel stacked aggregation network (PSA-Net), a lightweight framework\\ndesigned as an anti-spoofing defense system for voice-controlled smart IoT\\ndevices. The PSA-Net processes raw audios directly and eliminates the need for\\ndataset-dependent handcrafted features or pre-computed spectrograms.\\nFurthermore, PSA-Net employs a split-transform-aggregate approach, which\\ninvolves the segmentation of utterances, the extraction of intrinsic\\ndifferentiable embeddings through convolutions, and the aggregation of them to\\ndistinguish legitimate from spoofed audios. In contrast to existing deep\\nResnet-oriented solutions, we incorporate cardinality as an additional\\ndimension in our network, which enhances the PSA-Net ability to generalize\\nacross diverse attacks. The results show that the PSA-Net achieves more\\nconsistent performance for different attacks that exist in current\\nanti-spoofing solutions.', comment='arXiv admin note: text overlap with arXiv:2309.10560', journal_ref=None, doi=None, primary_category='cs.SD', categories=['cs.SD', 'cs.CR', 'cs.NE', 'eess.AS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.19841v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.19841v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.19498v1', updated=datetime.datetime(2024, 11, 29, 6, 33, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 29, 6, 33, 31, tzinfo=datetime.timezone.utc), title='Protecting Multiple Types of Privacy Simultaneously in EEG-based Brain-Computer Interfaces', authors=[arxiv.Result.Author('Lubin Meng'), arxiv.Result.Author('Xue Jiang'), arxiv.Result.Author('Tianwang Jia'), arxiv.Result.Author('Dongrui Wu')], summary='A brain-computer interface (BCI) enables direct communication between the\\nbrain and an external device. Electroencephalogram (EEG) is the preferred input\\nsignal in non-invasive BCIs, due to its convenience and low cost. EEG-based\\nBCIs have been successfully used in many applications, such as neurological\\nrehabilitation, text input, games, and so on. However, EEG signals inherently\\ncarry rich personal information, necessitating privacy protection. This paper\\ndemonstrates that multiple types of private information (user identity, gender,\\nand BCI-experience) can be easily inferred from EEG data, imposing a serious\\nprivacy threat to BCIs. To address this issue, we design perturbations to\\nconvert the original EEG data into privacy-protected EEG data, which conceal\\nthe private information while maintaining the primary BCI task performance.\\nExperimental results demonstrated that the privacy-protected EEG data can\\nsignificantly reduce the classification accuracy of user identity, gender and\\nBCI-experience, but almost do not affect at all the classification accuracy of\\nthe primary BCI task, enabling user privacy protection in EEG-based BCIs.', comment=None, journal_ref=\"IEEE Int'l Conf. on Systems, Man and Cybernetics, Sarawak,\\n  Malaysia, October 2024\", doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.19498v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.19498v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.18380v1', updated=datetime.datetime(2024, 11, 27, 14, 27, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 27, 14, 27, 47, tzinfo=datetime.timezone.utc), title='SoK: Privacy Personalised -- Mapping Personal Attributes \\\\& Preferences of Privacy Mechanisms for Shoulder Surfing', authors=[arxiv.Result.Author('Habiba Farzand'), arxiv.Result.Author('Karola Marky'), arxiv.Result.Author('Mohamed Khamis')], summary=\"Shoulder surfing is a byproduct of smartphone use that enables bystanders to\\naccess personal information (such as text and photos) by making screen\\nobservations without consent. To mitigate this, several protection mechanisms\\nhave been proposed to protect user privacy. However, the mechanisms that users\\nprefer remain unexplored. This paper explores correlations between personal\\nattributes and properties of shoulder surfing protection mechanisms. For this,\\nwe first conducted a structured literature review and identified ten protection\\nmechanism categories against content-based shoulder surfing. We then surveyed\\nN=192 users and explored correlations between personal attributes and\\nproperties of shoulder surfing protection mechanisms. Our results show that\\nusers agreed that the presented mechanisms assisted in protecting their\\nprivacy, but they preferred non-digital alternatives. Among the mechanisms,\\nparticipants mainly preferred an icon overlay mechanism followed by a tangible\\nmechanism. We also found that users who prioritized out-of-device privacy and a\\nhigh tendency to interact with technology favoured the personalisation of\\nprotection mechanisms. On the contrary, age and smartphone OS did not impact\\nusers' preference for perceived usefulness and personalisation of mechanisms.\\nBased on the results, we present key takeaways to support the design of future\\nprotection mechanisms.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.18380v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.18380v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.18023v1', updated=datetime.datetime(2024, 11, 27, 3, 41, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 27, 3, 41, 38, tzinfo=datetime.timezone.utc), title='Leveraging A New GAN-based Transformer with ECDH Crypto-system for Enhancing Energy Theft Detection in Smart Grid', authors=[arxiv.Result.Author('Yang Yang'), arxiv.Result.Author('Xun Yuan'), arxiv.Result.Author('Arwa Alromih'), arxiv.Result.Author('Aryan Mohammadi Pasikhani'), arxiv.Result.Author('Prosanta Gope'), arxiv.Result.Author('Biplab Sikdar')], summary=\"Detecting energy theft is vital for effectively managing power grids, as it\\nensures precise billing and prevents financial losses. Split-learning emerges\\nas a promising decentralized machine learning technique for identifying energy\\ntheft while preserving user data confidentiality. Nevertheless, traditional\\nsplit learning approaches are vulnerable to privacy leakage attacks, which\\nsignificantly threaten data confidentiality. To address this challenge, we\\npropose a novel GAN-Transformer-based split learning framework in this paper.\\nThis framework leverages the strengths of the transformer architecture, which\\nis known for its capability to process long-range dependencies in energy\\nconsumption data. Thus, it enhances the accuracy of energy theft detection\\nwithout compromising user privacy. A distinctive feature of our approach is the\\ndeployment of a novel mask-based method, marking a first in its field to\\neffectively combat privacy leakage in split learning scenarios targeted at\\nAI-enabled adversaries. This method protects sensitive information during the\\nmodel's training phase. Our experimental evaluations indicate that the proposed\\nframework not only achieves accuracy levels comparable to conventional methods\\nbut also significantly enhances privacy protection. The results underscore the\\npotential of the GAN-Transformer split learning framework as an effective and\\nsecure tool in the domain of energy theft detection.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.18023v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.18023v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.17758v1', updated=datetime.datetime(2024, 11, 26, 0, 22, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 26, 0, 22, 31, tzinfo=datetime.timezone.utc), title='PP-LEM: Efficient and Privacy-Preserving Clearance Mechanism for Local Energy Markets', authors=[arxiv.Result.Author('Kamil Erdayandi'), arxiv.Result.Author('Mustafa Asan Mustafa')], summary=\"In this paper, we propose a novel Privacy-Preserving clearance mechanism for\\nLocal Energy Markets (PP-LEM), designed for computational efficiency and social\\nwelfare. PP-LEM incorporates a novel competitive game-theoretical clearance\\nmechanism, modelled as a Stackelberg Game. Based on this mechanism, a\\nprivacy-preserving market model is developed using a partially homomorphic\\ncryptosystem, allowing buyers' reaction function calculations to be executed\\nover encrypted data without exposing sensitive information of both buyers and\\nsellers. The comprehensive performance evaluation demonstrates that PP-LEM is\\nhighly effective in delivering an incentive clearance mechanism with\\ncomputational efficiency, enabling it to clear the market for 200 users within\\nthe order of seconds while concurrently protecting user privacy. Compared to\\nthe state of the art, PP-LEM achieves improved computational efficiency without\\ncompromising social welfare while still providing user privacy protection.\", comment=None, journal_ref='Sustainable Energy, Grids and Networks, Volume 39, 2024, 101477,\\n  ISSN 2352-4677', doi='10.1016/j.segan.2024.101477', primary_category='cs.GT', categories=['cs.GT', 'cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.segan.2024.101477', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.17758v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.17758v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.16987v1', updated=datetime.datetime(2024, 11, 25, 23, 28, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 25, 23, 28, 44, tzinfo=datetime.timezone.utc), title='Decentralized Storage And Self-Sovereign Identity For Document-Based Claims', authors=[arxiv.Result.Author('Bruno Gomes'), arxiv.Result.Author('Samih Eisa'), arxiv.Result.Author('David R. Matos'), arxiv.Result.Author('Miguel L. Pardal')], summary='Users increasingly rely on identity providers for accessing online services\\nand resources. However, centralized identity systems often compromise user\\nprivacy due to online activity tracking or data breaches. At the same time,\\nmany online services require digital copies of physical documents for\\nvalidation in claims processes, such as providing proof of residence for\\nopening a bank account or verifying medical images for health insurance claims.\\nWith centralized solutions, privacy depends entirely on the trusted party, but\\nthere are emerging decentralized approaches that offer greater transparency.\\n  This article introduces SoverClaim, a decentralized application prototype\\nthat empowers users to control their identity and also allows them to present\\ndigital documents with privacy. SoverClaim leverages Hyperledger Indy, a\\nblockchain for issuing and presenting self-sovereign digital identities with\\ntransparent audit logs, and Storj, a decentralized peer-to-peer service, for\\nsecure and decentralized document storage and subsequent deletion. The\\nprototype demonstrates the seamless integration of self-sovereign identities\\nand document-based claims, achieving response times of under 750 ms, making it\\nsuitable for timely human interactions.', comment='12 pages, 8 figures', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.ET'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.16987v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.16987v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.11315v1', updated=datetime.datetime(2024, 11, 18, 6, 18, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 18, 6, 18, 13, tzinfo=datetime.timezone.utc), title='A Review on Machine Unlearning', authors=[arxiv.Result.Author('Haibo Zhang'), arxiv.Result.Author('Toru Nakamura'), arxiv.Result.Author('Takamasa Isohara'), arxiv.Result.Author('Kouichi Sakurai')], summary=\"Recently, an increasing number of laws have governed the useability of users'\\nprivacy. For example, Article 17 of the General Data Protection Regulation\\n(GDPR), the right to be forgotten, requires machine learning applications to\\nremove a portion of data from a dataset and retrain it if the user makes such a\\nrequest. Furthermore, from the security perspective, training data for machine\\nlearning models, i.e., data that may contain user privacy, should be\\neffectively protected, including appropriate erasure. Therefore, researchers\\npropose various privacy-preserving methods to deal with such issues as machine\\nunlearning. This paper provides an in-depth review of the security and privacy\\nconcerns in machine learning models. First, we present how machine learning can\\nuse users' private data in daily life and the role that the GDPR plays in this\\nproblem. Then, we introduce the concept of machine unlearning by describing the\\nsecurity threats in machine learning models and how to protect users' privacy\\nfrom being violated using machine learning platforms. As the core content of\\nthe paper, we introduce and analyze current machine unlearning approaches and\\nseveral representative research results and discuss them in the context of the\\ndata lineage. Furthermore, we also discuss the future research challenges in\\nthis field.\", comment=None, journal_ref='SN COMPUT. SCI. 4, 337 (2023)', doi='10.1007/s42979-023-01767-4', primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s42979-023-01767-4', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.11315v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.11315v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.09914v1', updated=datetime.datetime(2024, 11, 15, 3, 22, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 15, 3, 22, 44, tzinfo=datetime.timezone.utc), title='mmSpyVR: Exploiting mmWave Radar for Penetrating Obstacles to Uncover Privacy Vulnerability of Virtual Reality', authors=[arxiv.Result.Author('Luoyu Mei'), arxiv.Result.Author('Ruofeng Liu'), arxiv.Result.Author('Zhimeng Yin'), arxiv.Result.Author('Qingchuan Zhao'), arxiv.Result.Author('Wenchao Jiang'), arxiv.Result.Author('Shuai Wang'), arxiv.Result.Author('Kangjie Lu'), arxiv.Result.Author('Tian He')], summary=\"Virtual reality (VR), while enhancing user experiences, introduces\\nsignificant privacy risks. This paper reveals a novel vulnerability in VR\\nsystems that allows attackers to capture VR privacy through obstacles utilizing\\nmillimeter-wave (mmWave) signals without physical intrusion and virtual\\nconnection with the VR devices. We propose mmSpyVR, a novel attack on VR user's\\nprivacy via mmWave radar. The mmSpyVR framework encompasses two main parts: (i)\\nA transfer learning-based feature extraction model to achieve VR feature\\nextraction from mmWave signal. (ii) An attention-based VR privacy spying module\\nto spy VR privacy information from the extracted feature. The mmSpyVR\\ndemonstrates the capability to extract critical VR privacy from the mmWave\\nsignals that have penetrated through obstacles. We evaluate mmSpyVR through\\nIRB-approved user studies. Across 22 participants engaged in four experimental\\nscenes utilizing VR devices from three different manufacturers, our system\\nachieves an application recognition accuracy of 98.5\\\\% and keystroke\\nrecognition accuracy of 92.6\\\\%. This newly discovered vulnerability has\\nimplications across various domains, such as cybersecurity, privacy protection,\\nand VR technology development. We also engage with VR manufacturer Meta to\\ndiscuss and explore potential mitigation strategies. Data and code are publicly\\navailable for scrutiny and research at https://github.com/luoyumei1-a/mmSpyVR/\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.09914v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.09914v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.09751v1', updated=datetime.datetime(2024, 11, 14, 19, 5, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 14, 19, 5, 47, tzinfo=datetime.timezone.utc), title='Analyzing the AI Nudification Application Ecosystem', authors=[arxiv.Result.Author('Cassidy Gibson'), arxiv.Result.Author('Daniel Olszewski'), arxiv.Result.Author('Natalie Grace Brigham'), arxiv.Result.Author('Anna Crowder'), arxiv.Result.Author('Kevin R. B. Butler'), arxiv.Result.Author('Patrick Traynor'), arxiv.Result.Author('Elissa M. Redmiles'), arxiv.Result.Author('Tadayoshi Kohno')], summary=\"Given a source image of a clothed person (an image subject), AI-based\\nnudification applications can produce nude (undressed) images of that person.\\nMoreover, not only do such applications exist, but there is ample evidence of\\nthe use of such applications in the real world and without the consent of an\\nimage subject. Still, despite the growing awareness of the existence of such\\napplications and their potential to violate the rights of image subjects and\\ncause downstream harms, there has been no systematic study of the nudification\\napplication ecosystem across multiple applications. We conduct such a study\\nhere, focusing on 20 popular and easy-to-find nudification websites. We study\\nthe positioning of these web applications (e.g., finding that most sites\\nexplicitly target the nudification of women, not all people), the features that\\nthey advertise (e.g., ranging from undressing-in-place to the rendering of\\nimage subjects in sexual positions, as well as differing user-privacy options),\\nand their underlying monetization infrastructure (e.g., credit cards and\\ncryptocurrencies). We believe this work will empower future, data-informed\\nconversations -- within the scientific, technical, and policy communities -- on\\nhow to better protect individuals' rights and minimize harm in the face of\\nmodern (and future) AI-based nudification applications. Content warning: This\\npaper includes descriptions of web applications that can be used to create\\nsynthetic non-consensual explicit AI-created imagery (SNEACI). This paper also\\nincludes an artistic rendering of a user interface for such an application.\", comment='22 pages, 5 figures, 2 tables', journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.09751v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.09751v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.10489v1', updated=datetime.datetime(2024, 11, 14, 18, 25, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 14, 18, 25, 20, tzinfo=datetime.timezone.utc), title='Biometrics in Extended Reality: A Review', authors=[arxiv.Result.Author('Ayush Agarwal'), arxiv.Result.Author('Raghavendra Ramachandra'), arxiv.Result.Author('Sushma Venkatesh'), arxiv.Result.Author('S. R. Mahadeva Prasanna')], summary='In the domain of Extended Reality (XR), particularly Virtual Reality (VR),\\nextensive research has been devoted to harnessing this transformative\\ntechnology in various real-world applications. However, a critical challenge\\nthat must be addressed before unleashing the full potential of XR in practical\\nscenarios is to ensure robust security and safeguard user privacy. This paper\\npresents a systematic survey of the utility of biometric characteristics\\napplied in the XR environment. To this end, we present a comprehensive overview\\nof the different types of biometric modalities used for authentication and\\nrepresentation of users in a virtual environment. We discuss different\\nbiometric vulnerability gateways in general XR systems for the first time in\\nthe literature along with taxonomy. A comprehensive discussion on generating\\nand authenticating biometric-based photorealistic avatars in XR environments is\\npresented with a stringent taxonomy. We also discuss the availability of\\ndifferent datasets that are widely employed in evaluating biometric\\nauthentication in XR environments together with performance evaluation metrics.\\nFinally, we discuss the open challenges and potential future work that need to\\nbe addressed in the field of biometrics in XR.', comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.10489v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.10489v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.08443v1', updated=datetime.datetime(2024, 11, 13, 8, 56, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 13, 8, 56, 35, tzinfo=datetime.timezone.utc), title='Machine Unlearning on Pre-trained Models by Residual Feature Alignment Using LoRA', authors=[arxiv.Result.Author('Laiqiao Qin'), arxiv.Result.Author('Tianqing Zhu'), arxiv.Result.Author('Linlin Wang'), arxiv.Result.Author('Wanlei Zhou')], summary=\"Machine unlearning is new emerged technology that removes a subset of the\\ntraining data from a trained model without affecting the model performance on\\nthe remaining data. This topic is becoming increasingly important in protecting\\nuser privacy and eliminating harmful or outdated data. The key challenge lies\\nin effectively and efficiently unlearning specific information without\\ncompromising the model's utility on the retained data. For the pre-trained\\nmodels, fine-tuning is an important way to achieve the unlearning target.\\nPrevious work typically fine-tuned the entire model's parameters, which incurs\\nsignificant computation costs. In addition, the fine-tuning process may cause\\nshifts in the intermediate layer features, affecting the model's overall\\nutility. In this work, we propose a novel and efficient machine unlearning\\nmethod on pre-trained models. We term the method as Residual Feature Alignment\\nUnlearning. Specifically, we leverage LoRA (Low-Rank Adaptation) to decompose\\nthe model's intermediate features into pre-trained features and residual\\nfeatures. By adjusting the residual features, we align the unlearned model with\\nthe pre-trained model at the intermediate feature level to achieve both\\nunlearning and remaining targets. The method aims to learn the zero residuals\\non the retained set and shifted residuals on the unlearning set. Extensive\\nexperiments on numerous datasets validate the effectiveness of our approach.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.08443v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.08443v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.07206v1', updated=datetime.datetime(2024, 11, 11, 18, 28, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 11, 18, 28, 43, tzinfo=datetime.timezone.utc), title='Tasks, Time, and Tools: Quantifying Online Sensemaking Efforts Through a Survey-based Study', authors=[arxiv.Result.Author('Andrew Kuznetsov'), arxiv.Result.Author('Michael Xieyang Liu'), arxiv.Result.Author('Aniket Kittur')], summary='Aiming to help people conduct online research tasks, much research has gone\\ninto tools for searching for, collecting, organizing, and synthesizing online\\ninformation. However, outside of the lab, in-the-wild sensemaking sessions\\n(with data on tasks, users, their tools and challenges) can ground us in the\\nreality of such efforts and the state of tool support. We use a survey-based\\napproach with aided recall focused on segmenting and contextualizing individual\\nexploratory browsing sessions to conduct a mixed method analysis of everyday\\nsensemaking sessions in the traditional desktop browser setting while\\npreserving user privacy. We report data from our survey (n=111) collected in\\nSeptember, 2022, and use these results to update and deepen the rich literature\\non information seeking behavior and exploratory search, contributing new\\nempirical insights into the time spent per week and distribution of that time\\nacross tasks, and the lack of externalization and tool-use despite widespread\\ndesire for support.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.07206v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.07206v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.06995v1', updated=datetime.datetime(2024, 11, 11, 13, 53, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 11, 13, 53, 33, tzinfo=datetime.timezone.utc), title='Which PPML Would a User Choose? A Structured Decision Support Framework for Developers to Rank PPML Techniques Based on User Acceptance Criteria', authors=[arxiv.Result.Author('Sascha Löbner'), arxiv.Result.Author('Sebastian Pape'), arxiv.Result.Author('Vanessa Bracamonte'), arxiv.Result.Author('Kittiphop Phalakarn')], summary=\"Using Privacy-Enhancing Technologies (PETs) for machine learning often\\ninfluences the characteristics of a machine learning approach, e.g., the needed\\ncomputational power, timing of the answers or how the data can be utilized.\\nWhen designing a new service, the developer faces the problem that some\\ndecisions require a trade-off. For example, the use of a PET may cause a delay\\nin the responses or adding noise to the data to improve the users' privacy\\nmight have a negative impact on the accuracy of the machine learning approach.\\nAs of now, there is no structured way how the users' perception of a machine\\nlearning based service can contribute to the selection of Privacy Preserving\\nMachine Learning (PPML) methods. This is especially a challenge since one\\ncannot assume that users have a deep technical understanding of these\\ntechnologies. Therefore, they can only be asked about certain attributes that\\nthey can perceive when using the service and not directly which PPML they\\nprefer.\\n  This study introduces a decision support framework with the aim of supporting\\nthe selection of PPML technologies based on user preferences. Based on prior\\nwork analysing User Acceptance Criteria (UAC), we translate these criteria into\\ndifferentiating characteristics for various PPML techniques. As a final result,\\nwe achieve a technology ranking based on the User Acceptance Criteria while\\nproviding technology insights for the developers. We demonstrate its\\napplication using the use case of classifying privacy-relevant information.\\n  Our contribution consists of the decision support framework which consists of\\na process to connect PPML technologies with UAC, a process for evaluating the\\ncharacteristics that separate PPML techniques, and a ranking method to evaluate\\nthe best PPML technique for the use case.\", comment=None, journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CR', 'cs.LG', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.06995v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.06995v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.06291v3', updated=datetime.datetime(2025, 4, 21, 10, 22, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 9, 21, 26, 59, tzinfo=datetime.timezone.utc), title='TinyML NLP Scheme for Semantic Wireless Sentiment Classification with Privacy Preservation', authors=[arxiv.Result.Author('Ahmed Y. Radwan'), arxiv.Result.Author('Mohammad Shehab'), arxiv.Result.Author('Mohamed-Slim Alouini')], summary='Natural Language Processing (NLP) operations, such as semantic sentiment\\nanalysis and text synthesis, often raise privacy concerns and demand\\nsignificant on-device computational resources. Centralized learning (CL) on the\\nedge provides an energy-efficient alternative but requires collecting raw data,\\ncompromising user privacy. While federated learning (FL) enhances privacy, it\\nimposes high computational energy demands on resource-constrained devices. This\\nstudy provides insights into deploying privacy-preserving, energy-efficient NLP\\nmodels on edge devices. We introduce semantic split learning (SL) as an\\nenergy-efficient, privacy-preserving tiny machine learning (TinyML) framework\\nand compare it to FL and CL in the presence of Rayleigh fading and additive\\nnoise. Our results show that SL significantly reduces computational power and\\nCO2 emissions while enhancing privacy, as evidenced by a fourfold increase in\\nreconstruction error compared to FL and nearly eighteen times that of CL. In\\ncontrast, FL offers a balanced trade-off between privacy and efficiency. Our\\ncode is available for replication at our GitHub repository:\\nhttps://github.com/AhmedRadwan02/TinyEco2AI-NLP.', comment='Accepted at EuCNC & 6G Summit 2025', journal_ref=None, doi='10.1109/EuCNC/6GSummit63408.2025.11037184', primary_category='cs.LG', categories=['cs.LG', 'cs.CR', 'cs.IT', 'math.IT', '68T50, 94A12', 'I.2.7; C.2.1'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/EuCNC/6GSummit63408.2025.11037184', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.06291v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.06291v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.03996v1', updated=datetime.datetime(2024, 11, 6, 15, 38, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 6, 15, 38, 31, tzinfo=datetime.timezone.utc), title='Towards Resource-Efficient Federated Learning in Industrial IoT for Multivariate Time Series Analysis', authors=[arxiv.Result.Author('Alexandros Gkillas'), arxiv.Result.Author('Aris Lalos')], summary='Anomaly and missing data constitute a thorny problem in industrial\\napplications. In recent years, deep learning enabled anomaly detection has\\nemerged as a critical direction, however the improved detection accuracy is\\nachieved with the utilization of large neural networks, increasing their\\nstorage and computational cost. Moreover, the data collected in edge devices\\ncontain user privacy, introducing challenges that can be successfully addressed\\nby the privacy-preserving distributed paradigm, known as federated learning\\n(FL). This framework allows edge devices to train and exchange models\\nincreasing also the communication cost. Thus, to deal with the increased\\ncommunication, processing and storage challenges of the FL based deep anomaly\\ndetection NN pruning is expected to have significant benefits towards reducing\\nthe processing, storage and communication complexity. With this focus, a novel\\ncompression-based optimization problem is proposed at the server-side of a FL\\nparadigm that fusses the received local models broadcast and performs pruning\\ngenerating a more compressed model. Experiments in the context of anomaly\\ndetection and missing value imputation demonstrate that the proposed FL\\nscenario along with the proposed compressed-based method are able to achieve\\nhigh compression rates (more than $99.7\\\\%$) with negligible performance losses\\n(less than $1.18\\\\%$ ) as compared to the centralized solutions.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'eess.SP'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.03996v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.03996v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.01690v1', updated=datetime.datetime(2024, 11, 3, 21, 32, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 3, 21, 32, 7, tzinfo=datetime.timezone.utc), title='Co-clustering for Federated Recommender System', authors=[arxiv.Result.Author('Xinrui He'), arxiv.Result.Author('Shuo Liu'), arxiv.Result.Author('Jackey Keung'), arxiv.Result.Author('Jingrui He')], summary='As data privacy and security attract increasing attention, Federated\\nRecommender System (FRS) offers a solution that strikes a balance between\\nproviding high-quality recommendations and preserving user privacy. However,\\nthe presence of statistical heterogeneity in FRS, commonly observed due to\\npersonalized decision-making patterns, can pose challenges. To address this\\nissue and maximize the benefit of collaborative filtering (CF) in FRS, it is\\nintuitive to consider clustering clients (users) as well as items into\\ndifferent groups and learning group-specific models. Existing methods either\\nresort to client clustering via user representations-risking privacy leakage,\\nor employ classical clustering strategies on item embeddings or gradients,\\nwhich we found are plagued by the curse of dimensionality. In this paper, we\\ndelve into the inefficiencies of the K-Means method in client grouping,\\nattributing failures due to the high dimensionality as well as data sparsity\\noccurring in FRS, and propose CoFedRec, a novel Co-clustering Federated\\nRecommendation mechanism, to address clients heterogeneity and enhance the\\ncollaborative filtering within the federated framework. Specifically, the\\nserver initially formulates an item membership from the client-provided item\\nnetworks. Subsequently, clients are grouped regarding a specific item category\\npicked from the item membership during each communication round, resulting in\\nan intelligently aggregated group model. Meanwhile, to comprehensively capture\\nthe global inter-relationships among items, we incorporate an additional\\nsupervised contrastive learning term based on the server-side generated item\\nmembership into the local training phase for each client. Extensive experiments\\non four datasets are provided, which verify the effectiveness of the proposed\\nCoFedRec.', comment=\"WWW '24: Proceedings of the ACM Web Conference 2024\", journal_ref=None, doi='10.1145/3589334.3645626', primary_category='cs.IR', categories=['cs.IR', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3589334.3645626', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.01690v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.01690v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.01580v2', updated=datetime.datetime(2025, 6, 25, 1, 20, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 3, 14, 13, 38, tzinfo=datetime.timezone.utc), title='Federated Learning Clients Clustering with Adaptation to Data Drifts', authors=[arxiv.Result.Author('Minghao Li'), arxiv.Result.Author('Dmitrii Avdiukhin'), arxiv.Result.Author('Rana Shahout'), arxiv.Result.Author('Nikita Ivkin'), arxiv.Result.Author('Vladimir Braverman'), arxiv.Result.Author('Minlan Yu')], summary='Federated Learning (FL) trains deep models across edge devices without\\ncentralizing raw data, preserving user privacy. However, client heterogeneity\\nslows down convergence and limits global model accuracy. Clustered FL (CFL)\\nmitigates this by grouping clients with similar representations and training a\\nseparate model for each cluster. In practice, client data evolves over time, a\\nphenomenon we refer to as data drift, which breaks cluster homogeneity and\\ndegrades performance. Data drift can take different forms depending on whether\\nchanges occur in the output values, the input features, or the relationship\\nbetween them. We propose FIELDING, a CFL framework for handling diverse types\\nof data drift with low overhead. FIELDING detects drift at individual clients\\nand performs selective re-clustering to balance cluster quality and model\\nperformance, while remaining robust to malicious clients and varying levels of\\nheterogeneity. Experiments show that FIELDING improves final model accuracy by\\n1.9-5.9% and achieves target accuracy 1.16x-2.23x faster than existing\\nstate-of-the-art CFL methods.', comment='24 pages, 16 figures', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.01580v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.01580v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.01471v1', updated=datetime.datetime(2024, 11, 3, 7, 40, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 3, 7, 40, 28, tzinfo=datetime.timezone.utc), title='A Practical and Privacy-Preserving Framework for Real-World Large Language Model Services', authors=[arxiv.Result.Author('Yu Mao'), arxiv.Result.Author('Xueping Liao'), arxiv.Result.Author('Wei Liu'), arxiv.Result.Author('Anjia Yang')], summary=\"Large language models (LLMs) have demonstrated exceptional capabilities in\\ntext understanding and generation, and they are increasingly being utilized\\nacross various domains to enhance productivity. However, due to the high costs\\nof training and maintaining these models, coupled with the fact that some LLMs\\nare proprietary, individuals often rely on online AI as a Service (AIaaS)\\nprovided by LLM companies. This business model poses significant privacy risks,\\nas service providers may exploit users' trace patterns and behavioral data. In\\nthis paper, we propose a practical and privacy-preserving framework that\\nensures user anonymity by preventing service providers from linking requests to\\nthe individuals who submit them. Our framework is built on partially blind\\nsignatures, which guarantee the unlinkability of user requests. Furthermore, we\\nintroduce two strategies tailored to both subscription-based and API-based\\nservice models, ensuring the protection of both users' privacy and service\\nproviders' interests. The framework is designed to integrate seamlessly with\\nexisting LLM systems, as it does not require modifications to the underlying\\narchitectures. Experimental results demonstrate that our framework incurs\\nminimal computation and communication overhead, making it a feasible solution\\nfor real-world applications.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.01471v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.01471v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.01050v1', updated=datetime.datetime(2024, 11, 1, 21, 34, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 1, 21, 34, 43, tzinfo=datetime.timezone.utc), title='BACSA: A Bias-Aware Client Selection Algorithm for Privacy-Preserving Federated Learning in Wireless Healthcare Networks', authors=[arxiv.Result.Author('Sushilkumar Yadav'), arxiv.Result.Author('Irem Bor-Yaliniz')], summary='Federated Learning (FL) has emerged as a transformative approach in\\nhealthcare, enabling collaborative model training across decentralized data\\nsources while preserving user privacy. However, performance of FL rapidly\\ndegrades in practical scenarios due to the inherent bias in non Independent and\\nIdentically distributed (non-IID) data among participating clients, which poses\\nsignificant challenges to model accuracy and generalization. Therefore, we\\npropose the Bias-Aware Client Selection Algorithm (BACSA), which detects user\\nbias and strategically selects clients based on their bias profiles. In\\naddition, the proposed algorithm considers privacy preservation, fairness and\\nconstraints of wireless network environments, making it suitable for sensitive\\nhealthcare applications where Quality of Service (QoS), privacy and security\\nare paramount. Our approach begins with a novel method for detecting user bias\\nby analyzing model parameters and correlating them with the distribution of\\nclass-specific data samples. We then formulate a mixed-integer non-linear\\nclient selection problem leveraging the detected bias, alongside wireless\\nnetwork constraints, to optimize FL performance. We demonstrate that BACSA\\nimproves convergence and accuracy, compared to existing benchmarks, through\\nevaluations on various data distributions, including Dirichlet and\\nclass-constrained scenarios. Additionally, we explore the trade-offs between\\naccuracy, fairness, and network constraints, indicating the adaptability and\\nrobustness of BACSA to address diverse healthcare applications.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.01050v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.01050v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2411.03887v3', updated=datetime.datetime(2025, 6, 3, 4, 28, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 1, 18, 46, 3, tzinfo=datetime.timezone.utc), title='Reclaiming \"Open AI\" -- AI Model Serving Can Be Open Access, Yet Monetizable and Loyal', authors=[arxiv.Result.Author('Zerui Cheng'), arxiv.Result.Author('Edoardo Contente'), arxiv.Result.Author('Ben Finch'), arxiv.Result.Author('Oleg Golev'), arxiv.Result.Author('Jonathan Hayase'), arxiv.Result.Author('Andrew Miller'), arxiv.Result.Author('Niusha Moshrefi'), arxiv.Result.Author('Anshul Nasery'), arxiv.Result.Author('Sandeep Nailwal'), arxiv.Result.Author('Sewoong Oh'), arxiv.Result.Author('Himanshu Tyagi'), arxiv.Result.Author('Pramod Viswanath')], summary='The rapid rise of AI has split model serving between open-weight\\ndistribution, which often lacks owner control and monetization, and opaque\\nAPI-based approaches that risk user privacy and model transparency, forming a\\ndichotomy that hinders an equitable AI ecosystem. This position paper\\nintroduces, rigorously formulates, and champions the Open-access, Monetizable,\\nand Loyal (OML) paradigm for AI model serving: a foundational shift to securely\\ndistribute and serve AI models by synthesizing transparency with granular\\nmonetization and critical safety controls. We survey diverse OML constructions\\nfrom theory and practice, analyze their security, performance, and practical\\ntrade-offs, outline a conceptual OML deployment protocol, and discuss market\\nand policy implications. We assert that OML can foster a democratized,\\nself-sustaining, and innovative AI landscape, mitigating centralized power\\nrisks. Finally, we call on the research community to further explore the broad\\ndesign space of OML, spanning cryptographic, AI-native, and socio-economic\\nmechanisms, to realize its full potential for a collaborative, accountable, and\\nresilient AI future.', comment='54 pages', journal_ref=None, doi=None, primary_category='cs.AI', categories=['cs.AI', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.03887v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.03887v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.21675v1', updated=datetime.datetime(2024, 10, 29, 2, 52, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 29, 2, 52, 49, tzinfo=datetime.timezone.utc), title='BF-Meta: Secure Blockchain-enhanced Privacy-preserving Federated Learning for Metaverse', authors=[arxiv.Result.Author('Wenbo Liu'), arxiv.Result.Author('Handi Chen'), arxiv.Result.Author('Edith C. H. Ngai')], summary=\"The metaverse, emerging as a revolutionary platform for social and economic\\nactivities, provides various virtual services while posing security and privacy\\nchallenges. Wearable devices serve as bridges between the real world and the\\nmetaverse. To provide intelligent services without revealing users' privacy in\\nthe metaverse, leveraging federated learning (FL) to train models on local\\nwearable devices is a promising solution. However, centralized model\\naggregation in traditional FL may suffer from external attacks, resulting in a\\nsingle point of failure. Furthermore, the absence of incentive mechanisms may\\nweaken users' participation during FL training, leading to degraded performance\\nof the trained model and reduced quality of intelligent services. In this\\npaper, we propose BF-Meta, a secure blockchain-empowered FL framework with\\ndecentralized model aggregation, to mitigate the negative influence of\\nmalicious users and provide secure virtual services in the metaverse. In\\naddition, we design an incentive mechanism to give feedback to users based on\\ntheir behaviors. Experiments conducted on five datasets demonstrate the\\neffectiveness and applicability of BF-Meta.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.21675v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.21675v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.22374v1', updated=datetime.datetime(2024, 10, 29, 2, 52, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 29, 2, 52, 26, tzinfo=datetime.timezone.utc), title='Machine Unlearning using Forgetting Neural Networks', authors=[arxiv.Result.Author('Amartya Hatua'), arxiv.Result.Author('Trung T. Nguyen'), arxiv.Result.Author('Filip Cano'), arxiv.Result.Author('Andrew H. Sung')], summary='Modern computer systems store vast amounts of personal data, enabling\\nadvances in AI and ML but risking user privacy and trust. For privacy reasons,\\nit is desired sometimes for an ML model to forget part of the data it was\\ntrained on. This paper presents a new approach to machine unlearning using\\nforgetting neural networks (FNN). FNNs are neural networks with specific\\nforgetting layers, that take inspiration from the processes involved when a\\nhuman brain forgets. While FNNs had been proposed as a theoretical construct,\\nthey have not been previously used as a machine unlearning method. We describe\\nfour different types of forgetting layers and study their properties. In our\\nexperimental evaluation, we report our results on the MNIST handwritten digit\\nrecognition and fashion datasets. The effectiveness of the unlearned models was\\ntested using Membership Inference Attacks (MIA). Successful experimental\\nresults demonstrate the great potential of our proposed method for dealing with\\nthe machine unlearning problem.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'cs.NE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.22374v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.22374v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.20555v2', updated=datetime.datetime(2025, 8, 2, 21, 2, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 27, 19, 11, 33, tzinfo=datetime.timezone.utc), title='Privacy-Enhanced Adaptive Authentication: User Profiling with Privacy Guarantees', authors=[arxiv.Result.Author('Yaser Baseri'), arxiv.Result.Author('Abdelhakim Senhaji Hafid'), arxiv.Result.Author('Dimitrios Makrakis')], summary=\"User profiling is a critical component of adaptive risk-based authentication,\\nyet it raises significant privacy concerns, particularly when handling\\nsensitive data. Profiling involves collecting and aggregating various user\\nfeatures, potentially creating quasi-identifiers that can reveal identities and\\ncompromise privacy. Even anonymized profiling methods remain vulnerable to\\nre-identification attacks through these quasi-identifiers. This paper\\nintroduces a novel privacy-enhanced adaptive authentication protocol that\\nleverages Oblivious Pseudorandom Functions (OPRF), anonymous tokens, and\\nDifferential Privacy (DP) to provide robust privacy guarantees. Our proposed\\napproach dynamically adjusts authentication requirements based on real-time\\nrisk assessments, enhancing security while safeguarding user privacy. By\\nintegrating privacy considerations into the core of adaptive risk-based\\nadaptive authentication, this approach addresses a gap often overlooked in\\ntraditional models. Advanced cryptographic techniques ensure confidentiality,\\nintegrity, and unlinkability of user data, while differential privacy\\nmechanisms minimize the impact of individual data points on overall analysis.\\nFormal security and privacy proofs demonstrate the protocol's resilience\\nagainst various threats and its ability to provide strong privacy guarantees.\\nAdditionally, a comprehensive performance evaluation reveals that the\\ncomputational and communication overheads are manageable, making the protocol\\npractical for real-world deployment. By adhering to data protection regulations\\nsuch as GDPR and CCPA, our protocol not only enhances security but also fosters\\nuser trust and compliance with legal standards.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.20555v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.20555v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.20003v1', updated=datetime.datetime(2024, 10, 25, 23, 0, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 25, 23, 0, 12, tzinfo=datetime.timezone.utc), title='Federated Anomaly Detection for Early-Stage Diagnosis of Autism Spectrum Disorders using Serious Game Data', authors=[arxiv.Result.Author('Nikolaos Pavlidis'), arxiv.Result.Author('Vasileios Perifanis'), arxiv.Result.Author('Eleni Briola'), arxiv.Result.Author('Christos-Chrysanthos Nikolaidis'), arxiv.Result.Author('Eleftheria Katsiri'), arxiv.Result.Author('Pavlos S. Efraimidis'), arxiv.Result.Author('Despina Elisabeth Filippidou')], summary=\"Early identification of Autism Spectrum Disorder (ASD) is considered critical\\nfor effective intervention to mitigate emotional, financial and societal\\nburdens. Although ASD belongs to a group of neurodevelopmental disabilities\\nthat are not curable, researchers agree that targeted interventions during\\nchildhood can drastically improve the overall well-being of individuals.\\nHowever, conventional ASD detection methods such as screening tests, are often\\ncostly and time-consuming. This study presents a novel semi-supervised approach\\nfor ASD detection using AutoEncoder-based Machine Learning (ML) methods due to\\nthe challenge of obtaining ground truth labels for the associated task. Our\\napproach utilizes data collected manually through a serious game specifically\\ndesigned for this purpose. Since the sensitive data collected by the gamified\\napplication are susceptible to privacy leakage, we developed a Federated\\nLearning (FL) framework that can enhance user privacy without compromising the\\noverall performance of the ML models. The framework is further enhanced with\\nFully Homomorphic Encryption (FHE) during model aggregation to minimize the\\npossibility of inference attacks and client selection mechanisms as well as\\nstate-of-the-art aggregators to improve the model's predictive accuracy. Our\\nresults demonstrate that semi-supervised FL can effectively predict an ASD risk\\nindicator for each case while simultaneously addressing privacy concerns.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CY', categories=['cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.20003v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.20003v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.17127v3', updated=datetime.datetime(2025, 3, 25, 20, 20, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 22, 16, 0, 26, tzinfo=datetime.timezone.utc), title='PAPILLON: Privacy Preservation from Internet-based and Local Language Model Ensembles', authors=[arxiv.Result.Author('Li Siyan'), arxiv.Result.Author('Vethavikashini Chithrra Raghuram'), arxiv.Result.Author('Omar Khattab'), arxiv.Result.Author('Julia Hirschberg'), arxiv.Result.Author('Zhou Yu')], summary=\"Users can divulge sensitive information to proprietary LLM providers, raising\\nsignificant privacy concerns. While open-source models, hosted locally on the\\nuser's machine, alleviate some concerns, models that users can host locally are\\noften less capable than proprietary frontier models. Toward preserving user\\nprivacy while retaining the best quality, we propose Privacy-Conscious\\nDelegation, a novel task for chaining API-based and local models. We utilize\\nrecent public collections of user-LLM interactions to construct a natural\\nbenchmark called PUPA, which contains personally identifiable information\\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\\npipeline that uses prompt optimization to address a simpler version of our\\ntask. Our best pipeline maintains high response quality for 85.5% of user\\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\\nmargin to the generation quality of proprietary LLMs for future work. Our data\\nand code is available at https://github.com/siyan-sylvia-li/PAPILLON.\", comment='Accepted to NAACL 2025 Main Conference', journal_ref=None, doi=None, primary_category='cs.CR', categories=['cs.CR', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.17127v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.17127v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.14252v2', updated=datetime.datetime(2025, 8, 11, 8, 16, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 18, 8, 2, 36, tzinfo=datetime.timezone.utc), title='Harmony: A Human-Aware, Responsive, Modular Assistant with a Locally Deployed Large Language Model', authors=[arxiv.Result.Author('Ziqi Yin'), arxiv.Result.Author('Mingxin Zhang'), arxiv.Result.Author('Daisuke Kawahara')], summary='Large Language Models (LLMs) offer powerful capabilities for natural language\\nunderstanding, enabling more intelligent smart home assistants. However,\\nexisting systems often rely on cloud-based LLMs, raising concerns around user\\nprivacy and system dependency on external connectivity. In this work, we\\npresent Harmony, a privacy-preserving and robust smart home assistant powered\\nby the locally deployable Llama3-8B model. Beyond protecting user data, Harmony\\nalso addresses reliability challenges of smaller models, such as hallucination\\nand instruction misinterpretation, through structured prompting and modular\\nagent design. Experimental results in both virtual environments and user\\nstudies show that Harmony achieves performance comparable to GPT-4-based\\nsystems, while enabling offline, proactive, and personalized smart home\\ninteraction.', comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.14252v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.14252v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.13387v3', updated=datetime.datetime(2025, 1, 23, 19, 37, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 17, 9, 39, 10, tzinfo=datetime.timezone.utc), title='CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation for Large Language Model Applications', authors=[arxiv.Result.Author('Chaoran Chen'), arxiv.Result.Author('Daodao Zhou'), arxiv.Result.Author('Yanfang Ye'), arxiv.Result.Author('Toby Jia-jun Li'), arxiv.Result.Author('Yaxing Yao')], summary=\"The rise of end-user applications powered by large language models (LLMs),\\nincluding both conversational interfaces and add-ons to existing graphical user\\ninterfaces (GUIs), introduces new privacy challenges. However, many users\\nremain unaware of the risks. This paper explores methods to increase user\\nawareness of privacy risks associated with LLMs in end-user applications. We\\nconducted five co-design workshops to uncover user privacy concerns and their\\ndemand for contextual privacy information within LLMs. Based on these insights,\\nwe developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk\\nGeneration), a just-in-time contextual assistant designed to help users\\nidentify sensitive information, summarize relevant privacy policies, and\\nhighlight potential risks when sharing information with LLMs. We evaluated the\\nusability and usefulness of CLEAR across two example domains: ChatGPT and the\\nGemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and\\nimproves users' understanding of data practices and privacy risks. We also\\ndiscussed LLM's duality in posing and mitigating privacy risks, offering design\\nand policy implications.\", comment=None, journal_ref=None, doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.13387v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.13387v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.13097v2', updated=datetime.datetime(2025, 6, 1, 0, 41, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 16, 23, 50, 39, tzinfo=datetime.timezone.utc), title='Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models', authors=[arxiv.Result.Author('Sajjad Ghiasvand'), arxiv.Result.Author('Yifan Yang'), arxiv.Result.Author('Zhiyu Xue'), arxiv.Result.Author('Mahnoosh Alizadeh'), arxiv.Result.Author('Zheng Zhang'), arxiv.Result.Author('Ramtin Pedarsani')], summary=\"Parameter-efficient fine-tuning (PEFT) methods typically assume that Large\\nLanguage Models (LLMs) are trained on data from a single device or client.\\nHowever, real-world scenarios often require fine-tuning these models on private\\ndata distributed across multiple devices. Federated Learning (FL) offers an\\nappealing solution by preserving user privacy, as sensitive data remains on\\nlocal devices during training. Nonetheless, integrating PEFT methods into FL\\nintroduces two main challenges: communication overhead and data heterogeneity.\\nIn this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by\\nintegrating tensorized adapters into client-side models' encoder/decoder\\nblocks. FedTT is versatile and can be applied to both cross-silo FL and\\nlarge-scale cross-device FL. FedTT+, an extension of FedTT tailored for\\ncross-silo FL, enhances robustness against data heterogeneity by adaptively\\nfreezing portions of tensor factors, further reducing the number of trainable\\nparameters. Experiments on BERT and LLaMA models demonstrate that our proposed\\nmethods successfully address data heterogeneity challenges and perform on par\\nor even better than existing federated PEFT approaches while achieving up to\\n10$\\\\times$ reduction in communication cost.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.13097v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.13097v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2410.12652v1', updated=datetime.datetime(2024, 10, 16, 15, 16, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 10, 16, 15, 16, 4, tzinfo=datetime.timezone.utc), title='Constrained Posterior Sampling: Time Series Generation with Hard Constraints', authors=[arxiv.Result.Author('Sai Shankar Narasimhan'), arxiv.Result.Author('Shubhankar Agarwal'), arxiv.Result.Author('Litu Rout'), arxiv.Result.Author('Sanjay Shakkottai'), arxiv.Result.Author('Sandeep P. Chinchali')], summary='Generating realistic time series samples is crucial for stress-testing models\\nand protecting user privacy by using synthetic data. In engineering and\\nsafety-critical applications, these samples must meet certain hard constraints\\nthat are domain-specific or naturally imposed by physics or nature. Consider,\\nfor example, generating electricity demand patterns with constraints on peak\\ndemand times. This can be used to stress-test the functioning of power grids\\nduring adverse weather conditions. Existing approaches for generating\\nconstrained time series are either not scalable or degrade sample quality. To\\naddress these challenges, we introduce Constrained Posterior Sampling (CPS), a\\ndiffusion-based sampling algorithm that aims to project the posterior mean\\nestimate into the constraint set after each denoising update. Notably, CPS\\nscales to a large number of constraints (~100) without requiring additional\\ntraining. We provide theoretical justifications highlighting the impact of our\\nprojection step on sampling. Empirically, CPS outperforms state-of-the-art\\nmethods in sample quality and similarity to real time series by around 10% and\\n42%, respectively, on real-world stocks, traffic, and air quality datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.AI', 'eess.SP'], links=[arxiv.Result.Link('http://arxiv.org/abs/2410.12652v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2410.12652v1', title='pdf', rel='related', content_type=None)])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f399f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, List\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------- helpers -------------------------\n",
    "\n",
    "def make_response(status: str, message: str, data: Any) -> Dict[str, Any]:\n",
    "    return {\"status\": status, \"message\": message, \"data\": data}\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ------------------------- Base -------------------------\n",
    "\n",
    "class BaseFetcher(ABC):\n",
    "    SITE: str = \"Base\"\n",
    "    HEADERS: Dict = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/124.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"application/pdf,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "\n",
    "    def fetch(self, year: int) -> Dict[str, Any]:\n",
    "        \"\"\"Public API: validate → scrape → wrap in make_response.\"\"\"\n",
    "        try:\n",
    "            y = self._validate_year(year)\n",
    "            data = self._scrape(y, self.HEADERS)\n",
    "            if not data:\n",
    "                return make_response(\"success\", f\"No papers found for {self.SITE} {y}.\", [])\n",
    "            return make_response(\"success\", f\"Fetched {len(data)} papers from {self.SITE} {y}.\", data)\n",
    "        except Exception as e:\n",
    "            return make_response(\"error\", f\"{self.SITE} error: {e}\", None)\n",
    "\n",
    "    # -------- hooks for subclasses --------\n",
    "    @abstractmethod\n",
    "    def _scrape(self, year: int, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        ...\n",
    "\n",
    "    # -------- shared utilities --------\n",
    "    def _validate_year(self, year: int) -> int:\n",
    "        try:\n",
    "            y = int(year)\n",
    "        except Exception:\n",
    "            raise ValueError(\"year must be an integer.\")\n",
    "        if not (1900 <= y <= 2100):\n",
    "            raise ValueError(f\"year out of expected range: {y}.\")\n",
    "        return y\n",
    "\n",
    "    def _fetch_html(self, url: str, headers: Dict[str, str], label: str, timeout: int = 30) -> str:\n",
    "        try:\n",
    "            import requests\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Missing dependency 'requests': {e}\")\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, allow_redirects=True, timeout=timeout)\n",
    "        except requests.RequestException as e:\n",
    "            raise RuntimeError(f\"Network error fetching {label}: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Unexpected error fetching {label}: {e}\")\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(f\"HTTP {resp.status_code} fetching {url}\")\n",
    "        ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"html\" not in ctype:\n",
    "            logging.warning(\"Expected HTML but got Content-Type=%s for %s\", ctype, url)\n",
    "        return resp.text\n",
    "\n",
    "    def _soup(self, html: str):\n",
    "        try:\n",
    "            from bs4 import BeautifulSoup\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Missing dependency 'beautifulsoup4': {e}\")\n",
    "        try:\n",
    "            return BeautifulSoup(html, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to parse HTML: {e}\")\n",
    "\n",
    "    def _split_authors(self, text: str) -> List[str]:\n",
    "        parts = re.split(r\",|\\band\\b\", text or \"\", flags=re.IGNORECASE)\n",
    "        return [re.sub(r\"\\s+\", \" \", p).strip() for p in parts if p and p.strip()]\n",
    "\n",
    "    # Optional override per site\n",
    "    def _html_to_pdf_link(self, url: str) -> str:\n",
    "        return url or \"\"\n",
    "\n",
    "# ------------------------- USENIX Security -------------------------\n",
    "\n",
    "class UsenixSecurityFetcher(BaseFetcher):\n",
    "    SITE = \"USENIX Security\"\n",
    "    BASE = \"https://www.usenix.org\"\n",
    "\n",
    "    def _sessions_url(self, year: int) -> str:\n",
    "        yy = f\"{year % 100:02d}\"\n",
    "        return f\"{self.BASE}/conference/usenixsecurity{yy}/technical-sessions\"\n",
    "\n",
    "    def _scrape(self, year: int, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        # 1) collect presentation page URLs for the year\n",
    "        sessions_url = self._sessions_url(year)\n",
    "        html = self._fetch_html(sessions_url, headers, f\"{self.SITE} technical sessions {year}\")\n",
    "        soup = self._soup(html)\n",
    "\n",
    "        yy = f\"{year % 100:02d}\"\n",
    "        pres_links = []\n",
    "        for a in soup.select(f'a[href*=\"/conference/usenixsecurity{yy}/presentation/\"]'):\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                pres_links.append(urljoin(sessions_url, href))\n",
    "        pres_links = sorted(set(pres_links))\n",
    "\n",
    "        # 2) parse each presentation page for title, authors, pdf\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for url in tqdm(pres_links, f\"Fetching USENIX {year} papers\"):\n",
    "            try:\n",
    "                phtml = self._fetch_html(url, headers, \"presentation page\")\n",
    "                psoup = self._soup(phtml)\n",
    "\n",
    "                # title\n",
    "                h1 = psoup.find(\"h1\")\n",
    "                title = (h1.get_text(strip=True) if h1 else \"\").strip()\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                # authors (prefer meta tags; fallback to BibTeX)\n",
    "                authors = [m.get(\"content\").strip() for m in psoup.select('meta[name=\"citation_author\"]') if m.get(\"content\")]\n",
    "                if not authors:\n",
    "                    # try BibTeX: author = {A and B and C}\n",
    "                    m = re.search(r\"author\\s*=\\s*\\{([^}]+)\\}\", psoup.get_text(\"\\n\", strip=True), flags=re.IGNORECASE | re.DOTALL)\n",
    "                    if m:\n",
    "                        authors = [a.strip() for a in m.group(1).split(\" and \") if a.strip()]\n",
    "\n",
    "                # pdf url (skip slides if both exist)\n",
    "                pdf_url = \"\"\n",
    "                for a in psoup.select('a[href$=\".pdf\"], a[href*=\".pdf?\"]'):\n",
    "                    href = a.get(\"href\") or \"\"\n",
    "                    if href:\n",
    "                        cand = urljoin(url, href)\n",
    "                        name = cand.lower()\n",
    "                        if \"slides\" in name or \"talk\" in name:\n",
    "                            continue\n",
    "                        pdf_url = cand\n",
    "                        break\n",
    "                if not pdf_url:\n",
    "                    continue\n",
    "\n",
    "                results.append({\"title\": title, \"authors\": authors, \"paper_url\": pdf_url})\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Skipping one presentation (%s): %s\", url, e)\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09a92462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching USENIX 2024 papers:  50%|█████     | 210/418 [01:25<01:24,  2.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m year = \u001b[32m2024\u001b[39m\n\u001b[32m      2\u001b[39m fetcher = UsenixSecurityFetcher()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m resp = \u001b[43mfetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp[\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      5\u001b[39m     out = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33musenix_security_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mBaseFetcher.fetch\u001b[39m\u001b[34m(self, year)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     39\u001b[39m     y = \u001b[38;5;28mself\u001b[39m._validate_year(year)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m make_response(\u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo papers found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.SITE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mUsenixSecurityFetcher._scrape\u001b[39m\u001b[34m(self, year, headers)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m tqdm(pres_links, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching USENIX \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m papers\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         phtml = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresentation page\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m         psoup = \u001b[38;5;28mself\u001b[39m._soup(phtml)\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# title\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mBaseFetcher._fetch_html\u001b[39m\u001b[34m(self, url, headers, label, timeout)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing dependency \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrequests\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNetwork error fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "year = 2024\n",
    "fetcher = UsenixSecurityFetcher()\n",
    "resp = fetcher.fetch(year)\n",
    "if resp[\"status\"] == \"success\":\n",
    "    out = f\"usenix_security_{year}.jsonl\"\n",
    "    write_jsonl(out, resp[\"data\"])\n",
    "    print(resp[\"message\"])\n",
    "    print(f\"Wrote {len(resp)} JSONL to {out}\")\n",
    "else:\n",
    "    print(resp[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcfa7bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching USENIX 2024 papers:  50%|████▉     | 208/418 [01:45<01:46,  1.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[32m    181\u001b[39m out_path = sys.argv[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys.argv) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33musenix_security_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m fetcher = DblpUsenixFetcher()\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m resp = \u001b[43mfetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp[\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    185\u001b[39m     write_jsonl(out_path, resp[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mBaseFetcher.fetch\u001b[39m\u001b[34m(self, year)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     38\u001b[39m     y = \u001b[38;5;28mself\u001b[39m._validate_year(year)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m make_response(\u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo papers found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.SITE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mDblpUsenixFetcher._scrape\u001b[39m\u001b[34m(self, year, headers)\u001b[39m\n\u001b[32m    154\u001b[39m pdf_url = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     page_html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43musenix_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUSENIX presentation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     page_soup = \u001b[38;5;28mself\u001b[39m._soup(page_html)\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# Prefer a paper PDF over slides/preprint if both exist\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mBaseFetcher._fetch_html\u001b[39m\u001b[34m(self, url, headers, label, timeout)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing dependency \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrequests\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNetwork error fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, List\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def make_response(status: str, message: str, data: Any) -> Dict[str, Any]:\n",
    "    return {\"status\": status, \"message\": message, \"data\": data}\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ------------------------- Base -------------------------\n",
    "\n",
    "class BaseFetcher(ABC):\n",
    "    SITE: str = \"Base\"\n",
    "    HEADERS: Dict = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/124.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"application/pdf,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "\n",
    "    def fetch(self, year: int) -> Dict[str, Any]:\n",
    "        \"\"\"Public API: validate → scrape → wrap in make_response.\"\"\"\n",
    "        try:\n",
    "            y = self._validate_year(year)\n",
    "            data = self._scrape(y, self.HEADERS)\n",
    "            if not data:\n",
    "                return make_response(\"success\", f\"No papers found for {self.SITE} {y}.\", [])\n",
    "            return make_response(\"success\", f\"Fetched {len(data)} papers from {self.SITE} {y}.\", data)\n",
    "        except Exception as e:\n",
    "            return make_response(\"error\", f\"{self.SITE} error: {e}\", None)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _scrape(self, year: int, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        ...\n",
    "\n",
    "    def _validate_year(self, year: int) -> int:\n",
    "        try:\n",
    "            y = int(year)\n",
    "        except Exception:\n",
    "            raise ValueError(\"year must be an integer.\")\n",
    "        if not (1900 <= y <= 2100):\n",
    "            raise ValueError(f\"year out of expected range: {y}.\")\n",
    "        return y\n",
    "\n",
    "    def _fetch_html(self, url: str, headers: Dict[str, str], label: str, timeout: int = 30) -> str:\n",
    "        try:\n",
    "            import requests\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Missing dependency 'requests': {e}\")\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, allow_redirects=True, timeout=timeout)\n",
    "        except requests.RequestException as e:\n",
    "            raise RuntimeError(f\"Network error fetching {label}: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Unexpected error fetching {label}: {e}\")\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(f\"HTTP {resp.status_code} fetching {url}\")\n",
    "        return resp.text\n",
    "\n",
    "    def _soup(self, html: str):\n",
    "        try:\n",
    "            from bs4 import BeautifulSoup\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Missing dependency 'beautifulsoup4': {e}\")\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def _split_authors(self, text: str) -> List[str]:\n",
    "        parts = re.split(r\",|\\band\\b\", text or \"\", flags=re.IGNORECASE)\n",
    "        return [re.sub(r\"\\s+\", \" \", p).strip() for p in parts if p and p.strip()]\n",
    "\n",
    "# ------------------------- DBLP → USENIX Security -------------------------\n",
    "\n",
    "class DblpUsenixFetcher(BaseFetcher):\n",
    "    SITE = \"DBLP→USENIX Security\"\n",
    "    DBLP_BASE = \"https://dblp.org/db/conf/uss\"\n",
    "\n",
    "    def _toc_url(self, year: int) -> str:\n",
    "        return f\"{self.DBLP_BASE}/uss{year}.html\"\n",
    "\n",
    "    def _scrape(self, year: int, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        toc_url = self._toc_url(year)\n",
    "        toc_html = self._fetch_html(toc_url, headers, f\"DBLP TOC {year}\")\n",
    "        soup = self._soup(toc_html)\n",
    "\n",
    "        # Each paper row is typically a <li class=\"entry inproceedings\"> under ul.publ-list\n",
    "        items = soup.select(\"ul.publ-list li.entry.inproceedings\")\n",
    "        if not items:\n",
    "            # Fallback (DBLP occasionally tweaks classes)\n",
    "            items = soup.select(\"li.entry\")\n",
    "\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for li in tqdm(items, f\"Fetching USENIX {year} papers\"):\n",
    "            try:\n",
    "                # ---- title\n",
    "                title_el = li.select_one(\"span.title\")\n",
    "                title = (title_el.get_text(strip=True) if title_el else \"\").strip()\n",
    "                if not title:\n",
    "                    # Fallback: sometimes title is within <cite>\n",
    "                    cite = li.find(\"cite\")\n",
    "                    if cite:\n",
    "                        title = (cite.get_text(\" \", strip=True) or \"\").strip()\n",
    "                        # Trim trailing session/venue noise if present\n",
    "                        title = re.split(r\"\\.\\s{2,}|\\s{2,}view\\s\", title, maxsplit=1)[0].strip()\n",
    "\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                # ---- authors (schema.org microdata is stable on DBLP)\n",
    "                author_els = li.select('span[itemprop=\"author\"] span[itemprop=\"name\"]')\n",
    "                authors = [a.get_text(strip=True) for a in author_els if a.get_text(strip=True)]\n",
    "                if not authors:\n",
    "                    # Fallback: collect visible author anchors near the title\n",
    "                    authors = [a.get_text(strip=True) for a in li.select(\"a[href*='/pid/']\")]\n",
    "                    if not authors:\n",
    "                        # Last resort: split the cite text\n",
    "                        authors = self._split_authors(li.get_text(\" \", strip=True))\n",
    "\n",
    "                # ---- find USENIX landing (electronic edition) then extract PDF\n",
    "                usenix_url = \"\"\n",
    "                for a in li.select(\"a[href]\"):\n",
    "                    href = a.get(\"href\") or \"\"\n",
    "                    if \"usenix.org\" in href:\n",
    "                        usenix_url = href\n",
    "                        break\n",
    "\n",
    "                if not usenix_url:\n",
    "                    # Try via DBLP record page\n",
    "                    rec = next((a.get(\"href\") for a in li.select(\"a[href^='https://dblp.org/rec/']\") if a.get(\"href\")), \"\")\n",
    "                    if rec:\n",
    "                        rec_html = self._fetch_html(rec, headers, \"DBLP record\")\n",
    "                        rec_soup = self._soup(rec_html)\n",
    "                        link = rec_soup.select_one(\"a[href*='usenix.org']\")\n",
    "                        usenix_url = link.get(\"href\") if link else \"\"\n",
    "\n",
    "                if not usenix_url:\n",
    "                    # If we can’t reach a landing page, skip (DBLP usually has one)\n",
    "                    continue\n",
    "\n",
    "                # On the USENIX presentation page, grab a direct PDF link\n",
    "                pdf_url = \"\"\n",
    "                try:\n",
    "                    page_html = self._fetch_html(usenix_url, headers, \"USENIX presentation\")\n",
    "                    page_soup = self._soup(page_html)\n",
    "                    # Prefer a paper PDF over slides/preprint if both exist\n",
    "                    pdf_as = page_soup.select(\"a[href$='.pdf'], a[href*='.pdf?']\")\n",
    "                    # Heuristic: prefer text containing 'Paper'; otherwise first PDF\n",
    "                    cand = next((a for a in pdf_as if re.search(r\"paper\", a.get_text(\"\", strip=True), re.I)), None)\n",
    "                    pdf_url = (cand or (pdf_as[0] if pdf_as else None)).get(\"href\") if (cand or pdf_as) else \"\"\n",
    "                    if pdf_url and not pdf_url.startswith(\"http\"):\n",
    "                        pdf_url = urljoin(usenix_url, pdf_url)\n",
    "                except Exception:\n",
    "                    pdf_url = \"\"\n",
    "\n",
    "                # Fallback to the USENIX landing if no direct PDF discovered\n",
    "                paper_url = pdf_url or usenix_url\n",
    "\n",
    "                results.append({\"title\": title, \"authors\": authors, \"paper_url\": paper_url})\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Skipping an entry: %s\", e)\n",
    "\n",
    "        return results\n",
    "\n",
    "# ------------------------- CLI -------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    year = 2024\n",
    "    out_path = sys.argv[2] if len(sys.argv) > 2 else f\"usenix_security_{year}.jsonl\"\n",
    "    fetcher = DblpUsenixFetcher()\n",
    "    resp = fetcher.fetch(year)\n",
    "    if resp[\"status\"] == \"success\":\n",
    "        write_jsonl(out_path, resp[\"data\"])\n",
    "        print(resp[\"message\"])\n",
    "        print(f\"Wrote JSONL to {out_path}\")\n",
    "    else:\n",
    "        print(resp[\"message\"])\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DblpAAAIFetcher(BaseFetcher):\n",
    "    SITE = \"DBLP→AAAI\"\n",
    "    DBLP_BASE = \"https://dblp.org/db/conf/aaai\"\n",
    "\n",
    "    def _toc_url(self, year: int) -> str:\n",
    "        # DBLP yearly TOC, e.g., https://dblp.org/db/conf/aaai/aaai2024.html\n",
    "        return f\"{self.DBLP_BASE}/aaai{year}.html\"\n",
    "\n",
    "    def _scrape(self, year: int, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        toc = self._toc_url(year)\n",
    "        html = self._fetch_html(toc, headers, f\"DBLP AAAI {year} TOC\")\n",
    "        soup = self._soup(html)\n",
    "\n",
    "        items = soup.select(\"ul.publ-list li.entry.inproceedings\") or soup.select(\"li.entry\")\n",
    "        results: List[Dict[str, Any]] = []\n",
    "\n",
    "        for li in tqdm(items, f\"Fetching AAAI {year} papers\"):\n",
    "            try:\n",
    "                # -------- title\n",
    "                t_el = li.select_one(\"span.title\")\n",
    "                title = (t_el.get_text(strip=True) if t_el else \"\").strip()\n",
    "                if not title:\n",
    "                    c = li.find(\"cite\")\n",
    "                    if c:\n",
    "                        title = (c.get_text(\" \", strip=True) or \"\").strip()\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                # -------- authors (DBLP microdata)\n",
    "                a_els = li.select('span[itemprop=\"author\"] span[itemprop=\"name\"]')\n",
    "                authors = [a.get_text(strip=True) for a in a_els if a.get_text(strip=True)]\n",
    "                if not authors:\n",
    "                    # last resort: split nearby text\n",
    "                    text = li.get_text(\" \", strip=True)\n",
    "                    parts = re.split(r\"\\s{2,}|\\s-\\s\", text)\n",
    "                    head = parts[0] if parts else text\n",
    "                    authors = [re.sub(r\"\\s+\", \" \", s).strip() for s in re.split(r\",|\\band\\b\", head) if s.strip()]\n",
    "\n",
    "                # -------- landing link (prefer AAAI OJS)\n",
    "                hrefs = [a.get(\"href\") or \"\" for a in li.select(\"a[href]\")]\n",
    "                landing = next((h for h in hrefs if \"ojs.aaai.org/index.php/AAAI/article/view\" in h), \"\")\n",
    "                if not landing:\n",
    "                    # open DBLP record to find OJS/DOI if missing on TOC row\n",
    "                    rec = next((h for h in hrefs if h.startswith(\"https://dblp.org/rec/\")), \"\")\n",
    "                    if rec:\n",
    "                        rec_html = self._fetch_html(rec, headers, \"DBLP record\")\n",
    "                        rec_soup = self._soup(rec_html)\n",
    "                        link = rec_soup.select_one(\"a[href*='ojs.aaai.org/index.php/AAAI/article/view'], a[href*='doi.org/10.1609/']\")\n",
    "                        landing = link.get(\"href\") if link else \"\"\n",
    "                if not landing:\n",
    "                    # fall back to the first “electronic edition” of any kind\n",
    "                    landing = next((h for h in hrefs if h.startswith(\"http\")), \"\")\n",
    "\n",
    "                # -------- get a direct PDF if available on the landing page\n",
    "                paper_url = landing\n",
    "                try:\n",
    "                    page_html = self._fetch_html(landing, headers, \"AAAI landing\")\n",
    "                    page_soup = self._soup(page_html)\n",
    "                    # OJS typically exposes direct PDFs like .../article/view/<id>/<pdfid>\n",
    "                    a_pdf = page_soup.select_one(\"a[href$='.pdf'], a[href*='/article/view/'][href*='/']\")\n",
    "                    if a_pdf:\n",
    "                        paper_url = urljoin(landing, a_pdf.get(\"href\") or \"\")\n",
    "                except Exception:\n",
    "                    pass  # keep landing URL\n",
    "\n",
    "                results.append({\"title\": title, \"authors\": authors, \"paper_url\": paper_url})\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Skipping one AAAI entry: %s\", e)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41153268",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    year = 2023\n",
    "    out_path = sys.argv[2] if len(sys.argv) > 2 else f\"aaai_{year}.jsonl\"\n",
    "    fetcher = DblpAAAIFetcher()\n",
    "    resp = fetcher.fetch(year)\n",
    "    if resp[\"status\"] == \"success\":\n",
    "        write_jsonl(out_path, resp[\"data\"])\n",
    "        print(resp[\"message\"])\n",
    "        print(f\"Wrote JSONL to {out_path}\")\n",
    "    else:\n",
    "        print(resp[\"message\"])\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832755c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
