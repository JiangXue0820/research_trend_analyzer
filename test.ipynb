{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a45e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import ToolException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349484c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def save_paper_info(paper_info: Dict[str, Any], conference: str, year: int) -> str:\n",
    "    \"\"\"\n",
    "    Save a paper's information as a single line in a JSONL file for a given conference and year.\n",
    "\n",
    "    Args:\n",
    "        paper_info (Dict[str, Any]): Metadata of the paper (title, authors, abstract, urls).\n",
    "        conference (str): The conference name (e.g., \"neurips\").\n",
    "        year (int): The year of the conference.\n",
    "\n",
    "    Returns:\n",
    "        str: The file path where the paper was saved.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(\n",
    "        \"paper_list\", conference.lower(), f\"{conference.lower()}{year}.jsonl\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(paper_info, ensure_ascii=False) + \"\\n\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_paper_list(filepath: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a list of paper metadata from a JSONL file. Each line in the file should be a JSON object.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of paper metadata dictionaries. Returns empty list if file does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "def paper_matches_topic(paper: Dict[str, Any], topic_keywords: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if the paper's title or abstract contains any of the topic keywords (case-insensitive).\n",
    "\n",
    "    Args:\n",
    "        paper (Dict[str, Any]): Paper metadata. Must include 'title'; may include 'abstract'.\n",
    "        topic_keywords (List[str]): List of keywords or phrases to match against title/abstract.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if any keyword appears in title or abstract, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If 'title' is missing or empty.\n",
    "    \"\"\"\n",
    "    title = paper.get(\"title\")\n",
    "    if not title or not title.strip():\n",
    "        raise ValueError(\"Paper dictionary must include a non-empty 'title' field.\")\n",
    "    abstract = (paper.get(\"abstract\") or \"\").lower()\n",
    "    title_lower = title.lower()\n",
    "    for kw in topic_keywords:\n",
    "        kw_lower = kw.lower()\n",
    "        if kw_lower in title_lower or kw_lower in abstract:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_neurips_abstract_links(year: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve a list of NeurIPS paper abstract URLs for a specified year.\n",
    "\n",
    "    Args:\n",
    "        year (int): The year of the NeurIPS conference (e.g., 2023).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of full URLs, each pointing to a paper's abstract page.\n",
    "\n",
    "    Raises:\n",
    "        requests.RequestException: If the HTTP request fails.\n",
    "    \"\"\"\n",
    "    url = f\"https://papers.nips.cc/paper/{year}/\"  # trailing slash required\n",
    "    response = requests.get(url, timeout=20)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    return [\n",
    "        \"https://papers.nips.cc\" + a[\"href\"]\n",
    "        for a in links\n",
    "        if a[\"href\"].endswith(\"-Abstract.html\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_neurips_paper_from_html(url: str, html: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse metadata from a single NeurIPS paper abstract page HTML.\n",
    "\n",
    "    Args:\n",
    "        url (str): The full URL of the paper abstract page.\n",
    "        html (str): The HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing:\n",
    "            - title (str)\n",
    "            - authors (str)\n",
    "            - abstract (str)\n",
    "            - url_web (str)\n",
    "            - url_pdf (str)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If expected HTML elements are missing or multiple PDF links found.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    try:\n",
    "        title = soup.find_all(\"h4\")[0].text\n",
    "        authors = soup.find_all(\"i\")[-1].text\n",
    "        abstract = soup.find_all(\"p\")[2].text\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing metadata from {url}: {e}\")\n",
    "\n",
    "    info: Dict[str, Any] = {\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"abstract\": abstract,\n",
    "        \"url_web\": url\n",
    "    }\n",
    "\n",
    "    pdf_links = [\n",
    "        tag[\"href\"]\n",
    "        for tag in soup.find_all(\"a\", href=True)\n",
    "        if tag[\"href\"].lower().endswith(\"paper.pdf\")\n",
    "    ]\n",
    "    if len(pdf_links) != 1:\n",
    "        raise ValueError(f\"Found incorrect pdf url for {url}: {pdf_links}\")\n",
    "\n",
    "    info[\"url_pdf\"] = \"https://papers.nips.cc\" + pdf_links[0]\n",
    "    return info\n",
    "\n",
    "\n",
    "def fetch_single_paper_sync(\n",
    "    client: requests.Session,\n",
    "    url: str,\n",
    "    keywords: Optional[List[str]]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch and parse a single paper synchronously using an existing HTTP session.\n",
    "\n",
    "    Args:\n",
    "        client (requests.Session): A configured HTTP session.\n",
    "        url (str): The paper abstract URL to fetch.\n",
    "        keywords (Optional[List[str]]): If provided, only return metadata if it matches any keyword.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: Paper metadata dict on success and matching filter, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.get(url, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        meta = parse_neurips_paper_from_html(url, response.text)\n",
    "        if not keywords or paper_matches_topic(meta, keywords):\n",
    "            return meta\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_neurips_sync(\n",
    "    year: int,\n",
    "    max_papers: Optional[int] = None,\n",
    "    keywords: Optional[List[str]] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetch NeurIPS papers' metadata for a year synchronously and save new entries to JSONL.\n",
    "\n",
    "    Steps:\n",
    "        1. Load existing papers to dedupe.\n",
    "        2. Scrape the list of abstract URLs.\n",
    "        3. Sequentially fetch & parse each paper with a progress bar.\n",
    "        4. Append only new papers to the JSONL file.\n",
    "\n",
    "    Args:\n",
    "        year (int): Year of the NeurIPS conference.\n",
    "        max_papers (Optional[int]): Maximum number of papers to process. Processes all if None.\n",
    "        keywords (Optional[List[str]]): If provided, only save papers containing these keywords.\n",
    "\n",
    "    Returns:\n",
    "        str: File path to the JSONL file with saved paper info.\n",
    "    \"\"\"\n",
    "    conference = \"neurips\"\n",
    "    filepath = os.path.join(\n",
    "        \"paper_list\", conference, f\"{conference}{year}.jsonl\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    # 1) Load existing papers and dedupe by URL\n",
    "    existing = load_paper_list(filepath)\n",
    "    seen_urls = {p[\"url_web\"] for p in existing if \"url_web\" in p}\n",
    "    print(f\">>> Already have {len(seen_urls)} papers saved.\")\n",
    "\n",
    "    # 2) Get all abstract URLs\n",
    "    paper_urls = get_neurips_abstract_links(year)\n",
    "    if max_papers:\n",
    "        paper_urls = paper_urls[:max_papers]\n",
    "    print(f\">>> Found {len(paper_urls)} candidate URLs.\")\n",
    "\n",
    "    # 3) Fetch & filter\n",
    "    new_papers: List[Dict[str, Any]] = []\n",
    "    with requests.Session() as client:\n",
    "        for url in tqdm(paper_urls, desc=f\"Parsing NeurIPS {year} abstracts\"):\n",
    "            meta = fetch_single_paper_sync(client, url, keywords)\n",
    "            if meta and meta[\"url_web\"] not in seen_urls:\n",
    "                new_papers.append(meta)\n",
    "                seen_urls.add(meta[\"url_web\"])\n",
    "\n",
    "    print(f\">>> {len(new_papers)} new papers to save.\")\n",
    "\n",
    "    # 4) Append new ones\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        for paper in new_papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\">>> Appended {len(new_papers)} papers to {filepath}\")\n",
    "\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a9af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Found 2334 URLs for NeurIPS 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing abstracts:   0%|          | 0/2334 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, List, Dict, Any\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "def save_paper_info(paper_info: Dict[str, Any], conference: str, year: int) -> str:\n",
    "    filepath = os.path.join(\n",
    "        \"paper_list\", conference.lower(), f\"{conference.lower()}{year}.jsonl\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(paper_info, ensure_ascii=False) + \"\\n\")\n",
    "    return filepath\n",
    "\n",
    "def load_paper_list(filepath: str) -> List[Dict[str, Any]]:\n",
    "    if not os.path.exists(filepath):\n",
    "        return []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def paper_matches_topic(paper: Dict[str, Any], topic_keywords: List[str]) -> bool:\n",
    "    title = paper.get(\"title\")\n",
    "    if not title or not title.strip():\n",
    "        raise ValueError(\"Paper dictionary must include a non-empty 'title' field.\")\n",
    "    abstract = (paper.get(\"abstract\") or \"\").lower()\n",
    "    title = title.lower()\n",
    "    keywords_lower = [kw.lower() for kw in topic_keywords]\n",
    "    return any(kw in title or kw in abstract for kw in keywords_lower)\n",
    "\n",
    "PRIVACY_TOPICS = [\n",
    "    \"privacy\", \"private\", \"confidential\", \"anonymity\", \"anonymization\",\n",
    "    \"data protection\", \"secure\", \"secrecy\", \"obfuscation\", \"de-identification\"\n",
    "]\n",
    "\n",
    "async def get_neurips_abstract_links(year: int) -> List[str]:\n",
    "    url = f\"https://papers.nips.cc/paper/{year}\"\n",
    "    async with httpx.AsyncClient(timeout=20) as client:\n",
    "        response = await client.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "        paper_urls = [\n",
    "            \"https://papers.nips.cc\" + link[\"href\"]\n",
    "            for link in links if \"-Abstract.html\" in link.get(\"href\", \"\")\n",
    "        ]\n",
    "        return paper_urls\n",
    "\n",
    "def parse_neurips_paper_from_html(url: str, html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    try:\n",
    "        title = soup.find_all(\"h4\")[0].text\n",
    "        authors = soup.find_all(\"i\")[-1].text\n",
    "        abstract = soup.find_all(\"p\")[2].text\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing metadata from {url}: {e}\")\n",
    "    info = {\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"abstract\": abstract,\n",
    "        \"url_web\": url\n",
    "    }\n",
    "    pdf_url = [\n",
    "        tag['href'] for tag in soup.find_all('a', href=True)\n",
    "        if tag['href'].lower().endswith('paper.pdf')\n",
    "    ]\n",
    "    if len(pdf_url) != 1:\n",
    "        raise ValueError(f\"Found incorrect pdf url for {url}: {pdf_url}\")\n",
    "    info[\"url_pdf\"] = \"https://papers.nips.cc\" + pdf_url[0]\n",
    "    return info\n",
    "\n",
    "async def fetch_single_paper(\n",
    "    client: httpx.AsyncClient,\n",
    "    url: str,\n",
    "    keywords: Optional[List[str]]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch and parse a single paper using the given AsyncClient.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = await client.get(url)\n",
    "        r.raise_for_status()\n",
    "        meta = parse_neurips_paper_from_html(url, r.text)\n",
    "        if not keywords or paper_matches_topic(meta, keywords):\n",
    "            return meta\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "async def fetch_neurips(\n",
    "    year: int,\n",
    "    max_papers: Optional[int] = None,\n",
    "    keywords: Optional[List[str]] = None\n",
    ") -> str:\n",
    "    conference = \"neurips\"\n",
    "    filepath = os.path.join(\"paper_list\", conference, f\"{conference}{year}.jsonl\")\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    # 1) Load existing papers and build a set of seen URLs\n",
    "    existing = load_paper_list(filepath)\n",
    "    seen_urls = {paper.get(\"url_web\") for paper in existing if \"url_web\" in paper}\n",
    "    print(f\">>> Already have {len(seen_urls)} papers saved.\")\n",
    "\n",
    "    # 2) Scrape all paper URLs\n",
    "    paper_urls = await get_neurips_abstract_links(year)\n",
    "    if max_papers:\n",
    "        paper_urls = paper_urls[:max_papers]\n",
    "    print(f\">>> Found {len(paper_urls)} candidate URLs.\")\n",
    "\n",
    "    # 3) Fetch and filter only new papers\n",
    "    new_papers = []\n",
    "    async with httpx.AsyncClient(timeout=20) as client:\n",
    "        tasks = [fetch_single_paper(client, url, keywords) for url in paper_urls]\n",
    "        for coro in tqdm_asyncio.as_completed(tasks, total=len(tasks)):\n",
    "            meta = await coro\n",
    "            if meta and meta[\"url_web\"] not in seen_urls:\n",
    "                new_papers.append(meta)\n",
    "                seen_urls.add(meta[\"url_web\"])\n",
    "\n",
    "    print(f\">>> {len(new_papers)} new papers to save.\")\n",
    "\n",
    "    # 4) Save only the new ones\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        for paper in new_papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\">>> Appended {len(new_papers)} papers to {filepath}\")\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "await fetch_neurips(2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_paper_info(paper_info: Dict[str, Any], conference: str, year: int) -> str:\n",
    "    \"\"\"\n",
    "    Save a paper's information as a single line in a JSONL file for a given conference and year.\n",
    "    \n",
    "    Args:\n",
    "        paper_info (Dict[str, Any]): Metadata of the paper (title, authors, etc.).\n",
    "        conference (str): The conference name (e.g., \"neurips\").\n",
    "        year (int): The year of the conference.\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path where the paper was saved.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(\n",
    "        \"paper_list\", conference.lower(), f\"{conference.lower()}{year}.jsonl\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(paper_info, ensure_ascii=False) + \"\\n\")\n",
    "    return filepath\n",
    "\n",
    "def load_paper_list(filepath: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a list of paper metadata from a JSONL file. Each line in the file should be a JSON object.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the JSONL file.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of paper metadata dictionaries. Returns empty list if file does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "# def get_paper_titles(paper_list: List[Dict[str, Any]]) -> List[str]:\n",
    "\n",
    "    \n",
    "def paper_matches_topic(paper: Dict[str, Any], topic_keywords: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if the paper's title or (optional) abstract contains any of the topic keywords (case-insensitive).\n",
    "    Title is required; raises ValueError if missing or empty.\n",
    "\n",
    "    Args:\n",
    "        paper (Dict[str, Any]): Paper metadata. Must have 'title'. May have 'abstract'.\n",
    "        topic_keywords (List[str]): Keywords or phrases for topic matching.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the paper matches any keyword, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If 'title' is missing or empty.\n",
    "    \"\"\"\n",
    "    title = paper.get(\"title\")\n",
    "    if not title or not title.strip():\n",
    "        raise ValueError(\"Paper dictionary must include a non-empty 'title' field.\")\n",
    "    abstract = (paper.get(\"abstract\") or \"\").lower()\n",
    "    title = title.lower()\n",
    "    keywords_lower = [kw.lower() for kw in topic_keywords]\n",
    "    return any(kw in title or kw in abstract for kw in keywords_lower)\n",
    "\n",
    "PRIVACY_TOPICS = [\n",
    "    \"privacy\", \"private\", \"confidential\", \"anonymity\", \"anonymization\",\n",
    "    \"data protection\", \"secure\", \"secrecy\", \"obfuscation\", \"de-identification\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab7a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "class SavePaperInfoTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that saves a paper's information to a JSONL file for a given conference and year.\n",
    "    \"\"\"\n",
    "    name: str = \"save_paper_info\"\n",
    "    description: str = (\n",
    "        \"Save paper metadata (title, authors, abstract, urls) to a JSONL file under paper_list/<conference>/<conference><year>.jsonl.\"\n",
    "    )\n",
    "\n",
    "    def _run(\n",
    "        self, paper_info: Dict[str, Any], conference: str, year: int\n",
    "    ) -> str:\n",
    "        filepath = os.path.join(\n",
    "            \"paper_list\", conference.lower(), f\"{conference.lower()}{year}.jsonl\"\n",
    "        )\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(paper_info, ensure_ascii=False) + \"\\n\")\n",
    "        return filepath\n",
    "\n",
    "    async def _arun(\n",
    "        self, paper_info: Dict[str, Any], conference: str, year: int\n",
    "    ) -> str:\n",
    "        return self._run(paper_info, conference, year)\n",
    "\n",
    "\n",
    "class LoadPaperListTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that loads paper metadata list for a given conference and year.\n",
    "    \"\"\"\n",
    "    name: str = \"load_paper_list\"\n",
    "    description: str = (\n",
    "        \"Load paper metadata from the JSONL file saved by save_paper_info for a given conference and year.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, conference: str, year: int) -> List[Dict[str, Any]]:\n",
    "        filepath = os.path.join(\n",
    "            \"paper_list\", conference.lower(), f\"{conference.lower()}{year}.jsonl\"\n",
    "        )\n",
    "        if not os.path.exists(filepath):\n",
    "            return []\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "\n",
    "    async def _arun(self, conference: str, year: int) -> List[Dict[str, Any]]:\n",
    "        return self._run(conference, year)\n",
    "\n",
    "\n",
    "class PaperMatchesTopicTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that checks if a paper's title or abstract contains any given topic keywords.\n",
    "    \"\"\"\n",
    "    name: str = \"paper_matches_topic\"\n",
    "    description: str = (\n",
    "        \"Return True if any keyword appears in the paper's title or abstract (case-insensitive).\"\n",
    "    )\n",
    "\n",
    "    def _run(self, paper: Dict[str, Any], topic_keywords: List[str]) -> bool:\n",
    "        title = paper.get(\"title\")\n",
    "        if not title or not title.strip():\n",
    "            raise ValueError(\"Paper dictionary must include a non-empty 'title' field.\")\n",
    "        abstract = (paper.get(\"abstract\") or \"\").lower()\n",
    "        title_lower = title.lower()\n",
    "        for kw in topic_keywords:\n",
    "            if kw.lower() in title_lower or kw.lower() in abstract:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    async def _arun(self, paper: Dict[str, Any], topic_keywords: List[str]) -> bool:\n",
    "        return self._run(paper, topic_keywords)\n",
    "\n",
    "\n",
    "class GetNeuripsAbstractLinksTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that retrieves NeurIPS paper abstract URLs for a specified year.\n",
    "    \"\"\"\n",
    "    name: str = \"get_neurips_abstract_links\"\n",
    "    description: str = (\n",
    "        \"Fetch the list of NeurIPS paper abstract URLs for a given conference year.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, year: int) -> List[str]:\n",
    "        url = f\"https://papers.nips.cc/paper/{year}/\"\n",
    "        response = requests.get(url, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        return [\n",
    "            \"https://papers.nips.cc\" + a[\"href\"]\n",
    "            for a in links\n",
    "            if a[\"href\"].endswith(\"-Abstract.html\")\n",
    "        ]\n",
    "\n",
    "    async def _arun(self, year: int) -> List[str]:\n",
    "        return self._run(year)\n",
    "\n",
    "\n",
    "class ParseNeuripsPaperTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that parses metadata from a NeurIPS paper abstract page HTML.\n",
    "    \"\"\"\n",
    "    name: str = \"parse_neurips_paper\"\n",
    "    description: str = (\n",
    "        \"Parse a NeurIPS abstract page HTML and extract title, authors, abstract, web URL, and PDF URL.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, url: str, html: str) -> Dict[str, Any]:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        try:\n",
    "            title = soup.find_all(\"h4\")[0].text\n",
    "            authors = soup.find_all(\"i\")[-1].text\n",
    "            abstract = soup.find_all(\"p\")[2].text\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing metadata from {url}: {e}\")\n",
    "\n",
    "        info: Dict[str, Any] = {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"abstract\": abstract,\n",
    "            \"url_web\": url\n",
    "        }\n",
    "        pdf_links = [\n",
    "            tag[\"href\"] for tag in soup.find_all(\"a\", href=True)\n",
    "            if tag[\"href\"].lower().endsWith(\"paper.pdf\")\n",
    "        ]\n",
    "        if len(pdf_links) != 1:\n",
    "            raise ValueError(f\"Found incorrect pdf url for {url}: {pdf_links}\")\n",
    "        info[\"url_pdf\"] = \"https://papers.nips.cc\" + pdf_links[0]\n",
    "        return info\n",
    "\n",
    "    async def _arun(self, url: str, html: str) -> Dict[str, Any]:\n",
    "        return self._run(url, html)\n",
    "\n",
    "\n",
    "class FetchSinglePaperSyncTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that fetches and parses a single NeurIPS paper using an HTTP session, filtering by keywords.\n",
    "    \"\"\"\n",
    "    name: str = \"fetch_single_paper_sync\"\n",
    "    description: str = (\n",
    "        \"Fetch a NeurIPS paper abstract page synchronously and return metadata if it matches keywords.\"\n",
    "    )\n",
    "\n",
    "    def _run(\n",
    "        self, client: requests.Session, url: str, keywords: Optional[List[str]] = None\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        try:\n",
    "            response = client.get(url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            meta = ParseNeuripsPaperTool().run(url, response.text)\n",
    "            if not keywords or PaperMatchesTopicTool().run(meta, keywords):\n",
    "                return meta\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    async def _arun(\n",
    "        self, client: requests.Session, url: str, keywords: Optional[List[str]] = None\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        return self._run(client, url, keywords)\n",
    "\n",
    "\n",
    "class FetchNeuripsSyncTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that fetches and saves NeurIPS papers for a given year, with optional filtering and deduplication.\n",
    "    \"\"\"\n",
    "    name: str = \"fetch_neurips_sync\"\n",
    "    description: str = (\n",
    "        \"Fetch all NeurIPS abstracts for a year, filter by keywords, dedupe existing, and save new to JSONL.\"\n",
    "    )\n",
    "\n",
    "    def _run(\n",
    "        self, year: int, max_papers: Optional[int] = None, keywords: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        conference = \"neurips\"\n",
    "        filepath = os.path.join(\n",
    "            \"paper_list\", conference, f\"{conference}{year}.jsonl\"\n",
    "        )\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "        existing = LoadPaperListTool().run(conference, year)\n",
    "        seen_urls = {p[\"url_web\"] for p in existing if \"url_web\" in p}\n",
    "\n",
    "        paper_urls = GetNeuripsAbstractLinksTool().run(year)\n",
    "        if max_papers:\n",
    "            paper_urls = paper_urls[:max_papers]\n",
    "\n",
    "        new_papers: List[Dict[str, Any]] = []\n",
    "        with requests.Session() as client:\n",
    "            for url in tqdm(paper_urls, desc=f\"Parsing NeurIPS {year} abstracts\"):\n",
    "                meta = FetchSinglePaperSyncTool().run(client, url, keywords)\n",
    "                if meta and meta[\"url_web\"] not in seen_urls:\n",
    "                    new_papers.append(meta)\n",
    "                    seen_urls.add(meta[\"url_web\"])\n",
    "\n",
    "        with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "            for paper in new_papers:\n",
    "                f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        return filepath\n",
    "\n",
    "    async def _arun(\n",
    "        self, year: int, max_papers: Optional[int] = None, keywords: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        return self._run(year, max_papers, keywords)\n",
    "\n",
    "# Instantiate the tools list for use\n",
    "langchain_tools = [\n",
    "    SavePaperInfoTool(),\n",
    "    LoadPaperListTool(),\n",
    "    PaperMatchesTopicTool(),\n",
    "    GetNeuripsAbstractLinksTool(),\n",
    "    ParseNeuripsPaperTool(),\n",
    "    FetchSinglePaperSyncTool(),\n",
    "    FetchNeuripsSyncTool(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f81b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Authentication error: Langfuse client initialized without public_key. Client will be disabled. Provide a public_key parameter or set LANGFUSE_PUBLIC_KEY environment variable. \n",
      "C:\\Users\\Xue Jiang\\AppData\\Local\\Temp\\ipykernel_1600\\2044495122.py:11: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  paper_fetch_agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from configs import config\n",
    "from configs.llm_provider import get_llm\n",
    "from research_trend_analyzer.tools.paper_fetch_tools import toolkit\n",
    "\n",
    "# Initialize the LLM (adjust parameters as needed)\n",
    "config.LLM_PROVIDER='gemini'\n",
    "llm = get_llm(config)\n",
    "\n",
    "# Create the agent with the tools\n",
    "paper_fetch_agent = initialize_agent(\n",
    "    tools=toolkit,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94a83aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Jiang\\AppData\\Local\\Temp\\ipykernel_1600\\1022231200.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = paper_fetch_agent.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"generate_keyword_list_given_topic\",\n",
      "  \"action_input\": {\n",
      "    \"topic\": \"privacy\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'error': 'Could not parse LLM output: invalid syntax (<unknown>, line 1)', 'raw_response': '```python\\n{\"topic\": \"privacy\", \"keywords\": [\"privacy\", \"private\", \"anonymity\", \"anonymization\", \"data protection\", \"GDPR\", \"differential privacy\", \"federated learning\", \"privacy-preserving machine learning\", \"secure multi-party computation\", \"homomorphic encryption\", \"data minimization\", \"consent\", \"confidentiality\", \"personal data\", \"privacy-enhancing technologies\", \"re-identification\", \"surveillance\", \"digital rights\"]}\\n```', 'topic': 'privacy'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"filter_paper_by_topic\",\n",
      "  \"action_input\": {\n",
      "    \"conference\": \"NeurIPS\",\n",
      "    \"year\": 2020,\n",
      "    \"topic_keywords\": {\n",
      "      \"topic\": \"privacy\",\n",
      "      \"keywords\": [\n",
      "        \"privacy\",\n",
      "        \"private\",\n",
      "        \"anonymity\",\n",
      "        \"anonymization\",\n",
      "        \"data protection\",\n",
      "        \"GDPR\",\n",
      "        \"differential privacy\",\n",
      "        \"federated learning\",\n",
      "        \"privacy-preserving machine learning\",\n",
      "        \"secure multi-party computation\",\n",
      "        \"homomorphic encryption\",\n",
      "        \"data minimization\",\n",
      "        \"consent\",\n",
      "        \"confidentiality\",\n",
      "        \"personal data\",\n",
      "        \"privacy-enhancing technologies\",\n",
      "        \"re-identification\",\n",
      "        \"surveillance\",\n",
      "        \"digital rights\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'filepath': 'paper_list\\\\neurips\\\\neurips2020_privacy.jsonl', 'total_count': 42, 'new_count': 0}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"I found 42 papers related to privacy from NeurIPS 2020. You can find the list in `paper_list\\\\neurips\\\\neurips2020_privacy.jsonl`.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I found 42 papers related to privacy from NeurIPS 2020. You can find the list in `paper_list\\neurips\\neurips2020_privacy.jsonl`.\n"
     ]
    }
   ],
   "source": [
    "result = paper_fetch_agent.run(\n",
    "    \"find privacy related papers from NeurIPS 2020.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b0b543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year=2024\n",
    "url = f\"https://dblp.org/db/conf/uss/uss2024.html\"\n",
    "response = requests.get(url, timeout=20)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548494ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743befae",
   "metadata": {},
   "source": [
    "## Build Paper Summarizer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_content = \"\"\"\n",
    "# Paper Summary Instruction\n",
    "\n",
    "You are a scientific assistant that reads academic papers and provides structured, in-depth summaries.  \n",
    "Given the text of the following research paper, summarize it in clear, concise language, focusing on the following aspects (use bullet points or numbered lists for clarity):\n",
    "\n",
    "### 0. Key Words\n",
    "- Provide 5 key technology terms that best describe the paper\n",
    "- Keywords should be nouns, from field of AI / NLP / trustworthiness (privacy, safety, fairness, hallucination, etc.) / etc. \n",
    "\n",
    "### 1. Motivation (Research Background)\n",
    "- Briefly describe the background and motivation for the research.\n",
    "- What is the main problem or challenge being addressed?\n",
    "\n",
    "### 2. State-of-the-Art Methods and Their Limitations\n",
    "- Summarize the current state-of-the-art approaches related to this problem.\n",
    "- What are the key limitations or shortcomings of existing methods that the paper aims to overcome?\n",
    "\n",
    "### 3. Proposed Method (Main Contribution, Main Idea, Highlights, and Novelty)\n",
    "- Clearly state the main contribution(s) of the paper.\n",
    "- Describe the core idea and highlights of the proposed method.\n",
    "- Emphasize what is novel or unique about the approach.\n",
    "\n",
    "### 4. Experiment Results\n",
    "- Summarize the experimental setup, including datasets, metrics, and baselines.\n",
    "- What were the main results and findings? How does the proposed method compare to baselines?\n",
    "\n",
    "### 5. Limitation and Future Work\n",
    "- Point out any limitations or open questions discussed in the paper.\n",
    "- Summarize suggested directions for future research.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "- Present the summary in well-organized sections corresponding to the points above.\n",
    "- Avoid copying text directly from the paper; paraphrase and synthesize the information.\n",
    "- Keep the language accessible to someone with a technical background but who may not be an expert in the specific subfield.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Output Structure\n",
    "\n",
    "```text\n",
    "0. Key word (5 technology terms best describe the paper)\n",
    "   - ...\n",
    "\n",
    "1. Motivation (Research Background):\n",
    "   - ...\n",
    "\n",
    "2. State-of-the-Art Methods and Their Limitations:\n",
    "   - ...\n",
    "\n",
    "3. Proposed Method (Main Contribution, Main Idea, Highlights, and Novelty):\n",
    "   - ...\n",
    "\n",
    "4. Experiment Results:\n",
    "   - ...\n",
    "\n",
    "5. Limitation and Future Work:\n",
    "   - ...\n",
    "\n",
    "## Given Paper\n",
    "{context}\n",
    "\n",
    "## Summary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415ee98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "extending_context_window_llama_3 = \"https://arxiv.org/pdf/2404.19553\"\n",
    "attention_is_all_you_need = \"https://arxiv.org/pdf/1706.03762\"\n",
    "\n",
    "docs = PyMuPDFLoader(extending_context_window_llama_3).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ec8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''\n",
    "for doc in docs:\n",
    "    content += doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45ba31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Authentication error: Langfuse client initialized without public_key. Client will be disabled. Provide a public_key parameter or set LANGFUSE_PUBLIC_KEY environment variable. \n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from configs import config\n",
    "from configs.llm_provider import get_llm\n",
    "from tools.paper_fetch_tools import paper_fetch_toolkit\n",
    "\n",
    "# Initialize the LLM (adjust parameters as needed)\n",
    "config.LLM_PROVIDER='gemini'\n",
    "llm = get_llm(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d1c39f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prompt = ChatPromptTemplate.from_template(\u001b[43mprompt_content\u001b[49m)\n\u001b[32m      4\u001b[39m output_parser = StrOutputParser()\n\u001b[32m      5\u001b[39m chain = prompt | llm | output_parser\n",
      "\u001b[31mNameError\u001b[39m: name 'prompt_content' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(prompt_content)\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6141fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"paper_content\": content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5d4fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the provided research paper:\n",
      "\n",
      "### 1. Motivation (Research Background)\n",
      "The rapid advancements in large language models (LLMs) have led to increased interest in extending their context windows to handle longer inputs. While various methods have been proposed to achieve long-context capabilities, a significant challenge remains: most existing approaches demand substantial computational resources and time, making them inaccessible or inefficient for many researchers and practitioners. The paper aims to address this by developing an efficient and resource-friendly solution for extending LLM context lengths.\n",
      "\n",
      "### 2. State-of-the-Art Methods and Their Limitations\n",
      "The paper acknowledges that several approaches have been developed to enable long-context capabilities in LLMs. However, it highlights a critical limitation: these existing methods generally \"require significant compute and resources to accomplish.\" This implies that they are often computationally expensive, time-consuming, or necessitate large-scale infrastructure, thus posing a barrier to efficient and widespread adoption for context extension.\n",
      "\n",
      "### 3. Proposed Method (Main Contribution, Main Idea, Highlights, and Novelty)\n",
      "The main contribution of this paper is the successful and highly efficient extension of the Llama-3-8B-Instruct model's context length from its original 8K tokens to 80K tokens. The authors also publicly release all associated resources, including the model, training data, data generation pipeline, and training code, to foster further community research.\n",
      "\n",
      "The core idea of the proposed method involves:\n",
      "*   **QLoRA Fine-tuning:** Utilizing the QLoRA technique for efficient fine-tuning of the quantized LLM. LoRA is applied to all Q, K, V, O projections, and the embedding layer is additionally trained.\n",
      "*   **Synthetic Data Generation:** The key novelty lies in the use of a small, high-quality dataset of merely 3.5K synthetic long-context training samples, generated by GPT-4. This small dataset size, combined with the significant context extension achieved, suggests an inherent, underestimated potential within LLMs for context extension.\n",
      "    *   **Data Tasks:** The synthetic data covers three types of long-context tasks:\n",
      "        1.  **Single-Detail QA:** Questions targeting specific details within a long context (e.g., a segment from a book).\n",
      "        2.  **Multi-Detail QA:** Questions requiring information aggregation and reasoning across multiple details in homogeneous (coherent text) or heterogeneous (clustered independent texts) long contexts.\n",
      "        3.  **Biography Summarization:** Generating biographies for main characters from a given book.\n",
      "    *   The synthetic contexts range from 64K to 80K tokens in length.\n",
      "*   **RoPE Base Expansion:** The RoPE (Rotary Positional Embedding) base is expanded significantly from 500K to 200M during training, which is crucial for enabling the model to handle longer sequences.\n",
      "*   **Mitigation of Forgetting:** To prevent the model from forgetting its original short-context capabilities, the training dataset is mixed with 5K instances from RedPajama and 12K instances from LongAlpaca (max 16K length).\n",
      "\n",
      "The entire training cycle is remarkably efficient, taking only 8 hours to complete on a single machine with 8xA800 (80G) GPUs. This efficiency and the demonstrated performance make the approach highly noteworthy.\n",
      "\n",
      "### 4. Experiment Results\n",
      "The proposed model, Llama-3-8B-Instruct-80K-QLoRA, was evaluated against the original Llama-3-8B-Instruct (8K context) and a community-trained long-context model, Llama-3-8B-Instruct-262K.\n",
      "\n",
      "*   **Needle-In-A-Haystack:** The model achieved 100% accuracy across its training context length (up to 80K) and demonstrated strong generalization to unseen positions up to 128K, indicating excellent long-context retrieval capabilities.\n",
      "*   **Topic Retrieval:** It maintained 100% accuracy across all tested context lengths, significantly outperforming the original Llama-3-8B-Instruct, which failed beyond 9K tokens.\n",
      "*   **LongBench:** On this benchmark of real-world long-context tasks (mostly <32K tokens), the proposed model consistently and significantly outperformed both baselines across most categories (Single-Doc, Multi-Doc, Summarization, Few-Shot, Synthetic). It showed slightly lower performance only on the code completion task.\n",
      "*   **InfiniteBench:** For extremely long contexts (>100K, truncated to 80K for evaluation), the model excelled in Long-Book QA and achieved competitive performance against GPT-4 in Long-Book Summarization.\n",
      "*   **MMLU (Short-context preservation):** While the original Llama-3-8B-Instruct performed best on this short-context benchmark, both long-context models (including the proposed one) showed a slight drop in performance, suggesting a minor compromise in original short-context capabilities, consistent with previous research. However, the proposed model still maintained superior performance compared to other open-source models (Llama-2, Mistral) at the same scale.\n",
      "\n",
      "### 5. Limitation and Future Work\n",
      "**Limitations:**\n",
      "*   **Short-Context Capability Compromise:** As observed on the MMLU benchmark, extending context length can slightly compromise the model's original short-context performance.\n",
      "*   **Code Completion Performance:** The model underperformed on the code completion task in LongBench, suggesting a potential gap in the training data for this specific domain.\n",
      "*   **Metric-Oriented Issues:** The paper notes that metrics like ROUGE-F1 for summarization might not fully capture quality due to valid paraphrases not overlapping with ground truth.\n",
      "\n",
      "**Future Work:**\n",
      "*   **Further Context Extension:** The authors suggest that the context length could be extended \"far beyond 80K with more computation resources,\" indicating a clear path for future research.\n",
      "*   **Improving the Approach:** General future research can be made to further enhance the proposed method.\n",
      "*   **Addressing Code Performance:** Mixing more code-specific data into the training set is suggested to mitigate the observed underperformance on code completion tasks.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 50,\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f604e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1 Beijing Academy of Artificial Intelligence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='1 Beijing Academy of Artificial Intelligence\\n2 Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com\\nzhengliu1026@gmail.com\\nAbstract'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='context language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='to merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='context length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='entire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding.\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='FlagOpen/FlagEmbedding.\\n1\\nIntroduction\\nRecently, considerable attention has been directed towards long-context large language models,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='where different approaches are adopted to establish long-context capabilities for large language\\nmodels [4, 14, 5, 8, 9, 16, 2]. However, most of them require significant compute and resources to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='accomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='LLMs, with which we extend the context length of Llama-3-8B-Instruct3 from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='1. Single-Detail QA: the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='from a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='question-answer pairs based on this segment.\\n2. Multi-Detail QA: the inquiry requires information aggregation and reasoning over multiple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='details in a long context. We define two types of long context. The homogeneous\\ncontext contains a coherent text, such as a book or a long paper. We prompt GPT-4 to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='generate multiple question-answer pairs that require aggregating and analyzing information\\nfrom different locations in the context. The heterogeneous context consists of multiple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='independent texts. Notably, we perform clustering over a large corpus then extract texts from\\n∗Corresponding author.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='∗Corresponding author.\\n2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 0}, page_content='However, users could apply the model for even longer contexts via extrapolation.\\n3https://llama.meta.com/llama3/\\nPreprint. Under review.\\narXiv:2404.19553v1  [cs.CL]  30 Apr 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='8000\\n14315\\n20631\\n26947\\n33263\\n39578\\n45894\\n52210\\n58526\\n64842\\n71157\\n77473\\n83789\\n90105\\n96421\\n102736\\n109052\\n115368\\n121684\\n128000\\nContext Length\\n0\\n11\\n22\\n33\\n44\\n55\\n66\\n77\\n88\\n100\\nDepth Percent\\n1.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='0\\n11\\n22\\n33\\n44\\n55\\n66\\n77\\n88\\n100\\nDepth Percent\\n1.0\\nNeedle In A HayStack\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nAccuracy Score from GPT3.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nAccuracy Score from GPT3.5\\nFigure 1: The accuracy score of Llama-3-8B-Instruct-80K-QLoRA on Needle-In-A-HayStack task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='The blue vertical line indicates the training length, i.e. 80K.\\nthe same cluster to form each heterogeneous context. Therefore, the grouped texts share'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='some semantic similarity. We then prompt GPT-4 to ask about the similarities/dissimilarities\\nacross these texts.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='across these texts.\\n3. Biography Summarization: we prompt GPT-4 to write a biography for each main character\\nin a given book.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='in a given book.\\nFor all three tasks, the length of context is between 64K to 80K. Note that longer data can also be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='synthesized following the same methodology. When training, we organize the question-answer pairs\\nfor the same context in one multi-turn conversation then fine-tune the LLM to correctly answer the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='questions given the entire long context as input. Following previous work4, we mix 5K instances\\nrandomly chosen from RedPajama [6] to mitigate forgetting. We also mix LongAlpaca [5] in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='training set, which contains 12K instruction tuning instances with 16K length at maximum. Therefore,\\nthe entire training dataset contains 20K instances.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='We use QLoRA [7] to efficiently fine-tune the model. We apply LoRA on all Q,K,V,O projections\\nand additionally train the embedding layer. We set LoRA rank to 32 and alpha to 16. The learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='rate is 5e-5 with linear decay and no warmups. The batch size is 8. Gradient checkpointing is enabled.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='No parallel strategy is required thanks to the efficient implementation from Unsloth [1]. We train the\\nmodel for 1 epoch, which takes 8 hours to complete on a 8xA800 (80G) machine. Importantly, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='expand the RoPE base from 500K to 200M in training.\\nOur contributions are highlighted as follows:\\n• We release Llama-3-8B-Instruct-80K-QLoRA, which extends the context length of Llama-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='3-8B-Instruct from 8K to 80K. The entire resources including the model, training data, and\\ncode are all publicly available, which may advance the field of training long-context LLMs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='• Our training recipe is simple and efficient, while the resulted model demonstrates remark-\\nable performance on downstream long-context tasks. Further research can be made to\\nimprove our approach.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='improve our approach.\\n2\\nExperiments\\nWe evaluate our model on popular long-context benchmarks, then compare it with the original'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 1}, page_content='Llama-3-8B-Instruct model and the long-context Llama-3-8B-Instruct-262K from the community5.\\n4https://www.together.ai/blog/llama-2-7b-32k\\n5https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='3K\\n6K\\n9K\\n11K\\n14K\\n16K\\n21K\\n26K\\n31K\\n36K\\nContext Length\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAccuracy\\nLlama-3-8B-Instruct\\nLlama-3-8B-Instruct-262k\\nLlama-3-8B-Instruct-80K-QLoRA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='Llama-3-8B-Instruct-80K-QLoRA\\nFigure 2: The accuracy of Topic Retrieval task.\\nModel\\nSingle-Doc\\nMulti-Doc\\nSumm.\\nFew-Shot\\nSynthetic\\nCode\\nAvg\\nLlama-3-8B-Instruct\\n37.33\\n36.04\\n26.83\\n69.56\\n37.75\\n53.24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='37.33\\n36.04\\n26.83\\n69.56\\n37.75\\n53.24\\n43.20\\nLlama-3-8B-Instruct-262K\\n37.29\\n31.20\\n26.18\\n67.25\\n44.25\\n62.71\\n43.73\\nLlama-3-8B-Instruct-80K-QLoRA\\n43.57\\n43.07\\n28.93\\n69.15\\n48.50\\n51.95\\n47.19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='43.57\\n43.07\\n28.93\\n69.15\\n48.50\\n51.95\\n47.19\\nTable 1: Evaluation results on LongBench. For Llama-3-8B-Instruct, we use 8K context length.\\nModel\\nLongBookQA Eng\\nLongBookSum Eng\\nGPT-4\\n22.22\\n14.73'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='LongBookQA Eng\\nLongBookSum Eng\\nGPT-4\\n22.22\\n14.73\\nLlama-3-8B-Instruct\\n7.00\\n16.40\\nLlama-3-8B-Instruct-262K\\n20.30\\n10.34\\nLlama-3-8B-Instruct-80K-QLoRA\\n30.92\\n14.73'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='10.34\\nLlama-3-8B-Instruct-80K-QLoRA\\n30.92\\n14.73\\nTable 2: Evaluation results on InfBench. For Llama-3-8B-Instruct, we use 8K context length. The\\nresults of GPT-4 is copied from the paper [17].\\nModel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='Model\\nSTEM\\nSocial\\nHumanities\\nOthers\\nAvg\\nLlama-2-7B-Chat\\n35.92\\n54.37\\n51.74\\n51.42\\n47.22\\nMistral-7B-v0.2-Instruct\\n48.79\\n69.95\\n64.99\\n61.64\\n60.10\\nLlama-3-8B-Instruct\\n53.87\\n75.66\\n69.44\\n69.75\\n65.91'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='Llama-3-8B-Instruct\\n53.87\\n75.66\\n69.44\\n69.75\\n65.91\\nLlama-3-8B-Instruct-262K\\n52.10\\n73.26\\n67.15\\n69.80\\n64.34\\nLlama-3-8B-Instruct-80K-QLoRA\\n53.10\\n73.24\\n67.32\\n68.79\\n64.44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='53.10\\n73.24\\n67.32\\n68.79\\n64.44\\nTable 3: Zero-shot performance on MMLU.\\nFirstly, we leverage the Needle-In-A-Haystack task, which aims to recall an irrelevant piece of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='information (a.k.a. needle) inserted into a lengthy context (a.k.a. haystack). The accuracy is evaluated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='with GPT3.5. We use the same needle and haystack as in the official repository6. Our model achieves'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='100% accuracy over all its training context length. Besides, the model generalizes well to the unseen\\npositions (80K∼128K).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='positions (80K∼128K).\\nSecondly, we report the Topic Retrieval [12] accuracy in Figure 2. This task synthesizes a long'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='conversation with multiple independent discussions of a certain topic between the user and the\\nassistant. Then the LLM is required to repeat the first topic as is in the conversation. We use the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='conversations made up of [5,10,15,20,25,30,40,50,60,70] topics for evaluation. It can be observed\\nthat Llama-3-8B-Instruct fails to remember the topic when the context is longer than 9K. However,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 2}, page_content='the accuracy of our model remains 100% throughout all context lengths.\\n6https://github.com/gkamradt/LLMTest_NeedleInAHaystack\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='Thirdly, we evaluate our model on LongBench [3], which contains a variety of real-world long-context\\ntasks. Most context on this benchmark is shorter than 32K. Thus, we use 32K context length by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='default and 8K for Llama-3-8B-Instruct. The results are shown in Table 1. Our model significantly\\nand consistently outperforms all baselines except on the code completion task. Mixing more code'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='data in training may mitigate this problem.\\nForthly, we employ the English Long-Book QA and the Long-Book Summarization task from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='InfiniteBench [17] to assess the model’s performance on really long context. The testing instances are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='usually longer than 100K. We truncate them to 80K. According to Table 2, Llama-3-8B-Instruct-80K-\\nQLoRA excels on answering the questions based on the long context. It also achieves competitive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='performance against GPT-4 in terms of summarization. Interestingly, Llama-3-8B-Instruct with\\n8K context outperforms GPT-4 with 128K context on summarization. This is likely to be a metric-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='oriented issue (currently rouge-f1 is used) since the summary may have different paraphrases, which\\nmay not necessarily overlap with the ground truth.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='Lastly, in Table 3, we compare the zero-shot performance of our model and the baselines on\\nMMLU [10] benchmark. We also include Llama-2-7B-Chat [15] and Mistral-7B-Instruct-v0.2 [11]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='for comparison. It can be observed that both long-context models underperform the original Llama-3-\\n8B-Instruct, indicating that context extension may compromise the model’s short-context capability.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='This observation is in line with previous research [14]. However, our model’s performance is still\\nsuperior to other open-source models at the same scale.\\nReferences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='References\\n[1] Unsloth.ai. https://github.com/unslothai/unsloth, 2023.\\n[2] S. An, Z. Ma, Z. Lin, N. Zheng, and J.-G. Lou. Make your llm fully utilize the context, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='[3] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, Y. Dong,\\nJ. Tang, and J. Li. Longbench: A bilingual, multitask benchmark for long context understanding,\\n2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='2023.\\n[4] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models\\nvia positional interpolation, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='via positional interpolation, 2023.\\n[5] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia. Longlora: Efficient fine-tuning of\\nlong-context large language models, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='long-context large language models, 2024.\\n[6] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='[7] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of\\nquantized llms, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='quantized llms, 2023.\\n[8] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope:\\nExtending llm context window beyond 2 million tokens, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='[9] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for\\nscaling language models to 128k context, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='scaling language models to 128k context, 2024.\\n[10] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='massive multitask language understanding, 2021.\\n[11] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='[12] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, , and H. Zhang.\\nHow long can open-source llms truly promise on context length?, June 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='[13] OpenAI. Gpt-4 technical report, 2024.\\n[14] B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of\\nlarge language models, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='large language models, 2023.\\n[15] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 3}, page_content='R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 4}, page_content='I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 4}, page_content='I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 4}, page_content='[16] P. Zhang, Z. Liu, S. Xiao, N. Shao, Q. Ye, and Z. Dou. Soaring from 4k to 400k: Extending\\nllm’s context with activation beacon, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 4}, page_content='llm’s context with activation beacon, 2024.\\n[17] X. Zhang, Y. Chen, S. Hu, Z. Xu, J. Chen, M. K. Hao, X. Han, Z. L. Thai, S. Wang, Z. Liu, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-01T00:39:31+00:00', 'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-05-01T00:39:31+00:00', 'trapped': '', 'modDate': 'D:20240501003931Z', 'creationDate': 'D:20240501003931Z', 'page': 4}, page_content='M. Sun. ∞bench: Extending long context evaluation beyond 100k tokens, 2024.\\n5')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "embedding_fn  = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad3213",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# documents = [Document(page_content=chunk, metadata={\"chunk_id\": idx}) for idx, chunk in enumerate(split_chunks)]\n",
    "# Save to local Chroma DB directory (e.g., ./chroma_db)\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=docs[:2],\n",
    "    embedding=embedding_fn,\n",
    "    # persist_directory=\"./chroma_db\"\n",
    ")\n",
    "vectorstore.persist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5232bf1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(docs))]\n",
    "vs.add_documents(documents=docs, ids=uuids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a376dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\AI_Learning\\AI_agent\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Xue Jiang\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={\"device\": \"cpu\",\n",
    "                              \"trust_remote_code\": True}, # 可改为 \"cuda\" 用GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
